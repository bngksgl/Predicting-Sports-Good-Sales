{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "predict_0=pd.read_csv('ARIMA_dataset0_pred_units.csv',delimiter=';')\n",
    "predict_1=pd.read_csv('ARIMA_dataset1_pred_units.csv',delimiter=';')\n",
    "predict_2=pd.read_csv('ARIMA_dataset2_pred_units.csv',delimiter=';')\n",
    "predict_3=pd.read_csv('ARIMA_dataset3_pred_units.csv',delimiter=';')\n",
    "predict_4=pd.read_csv('ARIMA_dataset4_pred_units.csv',delimiter=';')\n",
    "predict_5=pd.read_csv('ARIMA_dataset5_pred_units.csv',delimiter=';')\n",
    "predict_6=pd.read_csv('ARIMA_dataset6_pred_units.csv',delimiter=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. ARIMA ALL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Arima model has been run on Rapid Miner and the results have been taken as an excel output. The excel has been imported into python and tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# performance Measurement\n",
    "import math\n",
    "import datetime as dt\n",
    "def performance_measurement(test_data,result,date_ass):\n",
    "    test_data['pid']=test_data[\"pid\"].astype(int)\n",
    "    test_data['key']=test_data[\"pid\"].astype(str) +\" \"+ test_data[\"size\"].astype(str)\n",
    "    test_data=test_data[['key','stock','sold_out_date']]\n",
    "    d1 = date(2018, 1, 1)\n",
    "    d2 = date(2018, 1, 31)\n",
    "    delta = d2 - d1         \n",
    "    pre_day=[]\n",
    "    for i in range(delta.days + 1):\n",
    "        add=str(d1 + timedelta(days=i))\n",
    "        pre_day.append(add)\n",
    "        \n",
    "    all_=test_data.key.unique()\n",
    "    n1=len(pre_day)\n",
    "    n=len(all_)\n",
    "    days=[]\n",
    "    ids=[]\n",
    "    i=0\n",
    "    k=0\n",
    "    while(i<n):\n",
    "        b=all_[i]\n",
    "        k=0\n",
    "        while(k<n1):\n",
    "            a=pre_day[k]\n",
    "            ids.append(b)\n",
    "            days.append(a)\n",
    "            k=k+1\n",
    "        i=i+1\n",
    "    \n",
    "    test_days = pd.DataFrame({'date':days,'key':ids})\n",
    "    test_comb=test_days.merge(test_data,on=['key'],how='outer')\n",
    "    test_comb=test_comb.fillna(0)\n",
    "    \n",
    "    i=0\n",
    "    sold_out=[]\n",
    "    n=len(all_)\n",
    "    while(i<n):\n",
    "        subset_w=test_comb.loc[test_comb['key']==all_[i]]\n",
    "        stock=subset_w['stock'].values[0]\n",
    "        n1=result.shape[0]\n",
    "        k=0\n",
    "        while(k<n1): #day based\n",
    "            day=pre_day[k]\n",
    "            that_day_sale=result.loc[k][0]\n",
    "            stock=stock-that_day_sale\n",
    "            if((stock==0) | (stock<0) ):\n",
    "                sold_out.append(day)\n",
    "                break\n",
    "            if(k==(n1-1)):\n",
    "                sold_out.append(date_ass)\n",
    "            k=k+1\n",
    "        i=i+1\n",
    "        \n",
    "    test_data['sol_out_pred']=sold_out\n",
    "    test_data[\"date_sold_out_date\"] = pd.to_datetime(test_data[\"sold_out_date\"],format=\"%Y/%m/%d\")\n",
    "    test_data[\"date_sold_out_pred\"] = pd.to_datetime(test_data[\"sol_out_pred\"],format=\"%Y/%m/%d\")\n",
    "    days_differ=test_data[\"date_sold_out_date\"]-test_data[\"date_sold_out_pred\"]\n",
    "    \n",
    "    test_data['days_differ']=days_differ\n",
    "    test_data['correct_differ']=abs(test_data['days_differ'].astype(dt.timedelta).map(lambda x: np.nan if pd.isnull(x) else x.days))\n",
    "    summ=sum(test_data['correct_differ'])\n",
    "    print(math.sqrt(summ))\n",
    "    \n",
    "    cluster_time=pd.read_csv('clusters_key_7')\n",
    "    window_clust=test_data.merge(cluster_time, on='key', how='left')\n",
    "    window_clust['clust_7']=window_clust['clust_7'].fillna(8)\n",
    "    sub=window_clust[['clust_7','correct_differ']]\n",
    "    cluster_size=sub.clust_7.value_counts()\n",
    "    by_group=sub.groupby(['clust_7']).sum()\n",
    "    by_group['clust_size']=cluster_size\n",
    "    by_group['mean']=by_group['correct_differ']/by_group['clust_size']\n",
    "    print(by_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result=pd.read_table('Arima_results_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result['forecast_of_units']=result['forecast of units;'].str.replace(';','')\n",
    "result=result.drop('forecast of units;',axis=1)\n",
    "result['forecast_of_units']=result['forecast_of_units'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data=pd.read_csv('test_data.csv')\n",
    "performance_measurement(test_data,result,'2018-01-22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data=pd.read_csv('test_1.csv')\n",
    "performance_measurement(test_data,result,'2018-01-22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data=pd.read_csv('test_2.csv')\n",
    "performance_measurement(test_data,result,'2018-01-22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data=pd.read_csv('test_3.csv')\n",
    "performance_measurement(test_data,result,'2018-01-22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data=pd.read_csv('test_4.csv')\n",
    "performance_measurement(test_data,result,'2018-01-22')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. ARIMA CLUSTERING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arima with time series clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_0=predict_0.drop(['Unnamed: 1'],axis=1)\n",
    "predict_1=predict_1.drop(['Unnamed: 1'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "key_0=pd.read_csv('cluster0_bg.csv')\n",
    "key_1=pd.read_csv('cluster1_bg.csv')\n",
    "key_2=pd.read_csv('cluster2_bg.csv')\n",
    "key_3=pd.read_csv('cluster3_bg.csv')\n",
    "key_4=pd.read_csv('cluster4_bg.csv')\n",
    "key_5=pd.read_csv('cluster5_bg.csv')\n",
    "key_6=pd.read_csv('cluster6_bg.csv')\n",
    "key_7=pd.read_csv('cluster7_bg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "key_0=key_0[['key','day','units']]\n",
    "key_1=key_1[['key','day','units']]\n",
    "key_2=key_2[['key','day','units']]\n",
    "key_3=key_3[['key','day','units']]\n",
    "key_4=key_4[['key','day','units']]\n",
    "key_5=key_5[['key','day','units']]\n",
    "key_6=key_6[['key','day','units']]\n",
    "key_7=key_7[['key','day','units']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_0_full=predict_0.append([predict_0]*8857, ignore_index=True)\n",
    "predict_1_full=predict_1.append([predict_1]*1719, ignore_index=True)\n",
    "predict_2_full=predict_2.append([predict_2]*283, ignore_index=True)\n",
    "predict_3_full=predict_3.append([predict_3]*10, ignore_index=True)\n",
    "predict_4_full=predict_4.append([predict_4]*93, ignore_index=True)\n",
    "predict_5_full=predict_5.append([predict_5]*39, ignore_index=True)\n",
    "predict_6_full=predict_6.append([predict_6]*3, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Integrate Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge(key,predict_full):\n",
    "    pre_cluster=pd.merge(key,predict_full, left_index=True, right_index=True)\n",
    "    pre_cluster=pre_cluster.rename(index=str, columns={'forecast of units': \"pre_units\"})\n",
    "    return pre_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pre_cluster0=merge(key_0,predict_0_full)\n",
    "pre_cluster1=merge(key_1,predict_1_full)\n",
    "pre_cluster2=merge(key_2,predict_2_full)\n",
    "pre_cluster3=merge(key_3,predict_3_full)\n",
    "pre_cluster4=merge(key_4,predict_4_full)\n",
    "pre_cluster5=merge(key_5,predict_5_full)\n",
    "pre_cluster6=merge(key_6,predict_6_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pre_cluster7=key_7\n",
    "pre_cluster7['pre_units']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pre_total = pd.concat([pre_cluster0,pre_cluster1,pre_cluster2,pre_cluster3,pre_cluster4,pre_cluster5,pre_cluster6,pre_cluster7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def predict_date(pre_total):\n",
    "    date=[]\n",
    "    i=0\n",
    "    n=len(pre_total)\n",
    "    while i<n:\n",
    "        day_int=pre_total.iloc[i]['day']\n",
    "        date_time=datetime.datetime(2018, 1, day_int, 0,0).date()\n",
    "        date_string=str(date_time)\n",
    "        date.append(date_string)\n",
    "        i+=1\n",
    "    pre_total['date']=date\n",
    "\n",
    "    pre_total_pvt = pre_total.pivot_table('pre_units', ['key'], 'date')\n",
    "    pre_total_pvt['key'] = pre_total_pvt.index\n",
    "    pre_total_pvt=pre_total_pvt.reset_index(drop=True)\n",
    "\n",
    "    return pre_total_pvt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pre_total_pvt    = predict_date(pre_total)\n",
    "pre_cluster0_pvt = predict_date(pre_cluster0)\n",
    "pre_cluster1_pvt = predict_date(pre_cluster1)\n",
    "pre_cluster2_pvt = predict_date(pre_cluster2)\n",
    "pre_cluster3_pvt = predict_date(pre_cluster3)\n",
    "pre_cluster4_pvt = predict_date(pre_cluster4)\n",
    "pre_cluster5_pvt = predict_date(pre_cluster5)\n",
    "pre_cluster6_pvt = predict_date(pre_cluster6)\n",
    "pre_cluster7_pvt = predict_date(pre_cluster7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# measure sold_out_day deviation with actual\n",
    "from datetime import date, timedelta\n",
    "import math\n",
    "\n",
    "def measurement(prediction, test_data):\n",
    "\n",
    "    d1 = date(2018, 1, 1)  \n",
    "    d2 = date(2018, 1, 31)  \n",
    "    delta = d2 - d1         \n",
    "    pre_day=[]\n",
    "    for i in range(delta.days + 1):\n",
    "        add=str(d1 + timedelta(days=i))\n",
    "        pre_day.append(add)\n",
    "\n",
    "    sold_out_day=[]\n",
    "    key=[]\n",
    "    #lets innet join with testdataset\n",
    "    window_inner=test_data.merge(prediction,left_on='key', right_on='key', how='inner')\n",
    "    window_inner['sol_out_pred']=0\n",
    "\n",
    "    all_=window_inner.key.unique()\n",
    "    n=len(all_) \n",
    "    i=0\n",
    "\n",
    "    while(i<n): #dataset\n",
    "        subset_w=prediction.loc[prediction['key']==all_[i]]\n",
    "        stock=test_data.loc[test_data['key']==all_[i]]['stock'].values[0]\n",
    "        n1=len(pre_day)\n",
    "        k=0\n",
    "        while(k<n1): #day calculation\n",
    "            day=pre_day[k]\n",
    "            that_day_sale=subset_w[day].values[0]\n",
    "            stock=stock-that_day_sale\n",
    "            if((stock==0) | (stock<0) ):\n",
    "                window_inner.iloc[i,37]=day\n",
    "                break\n",
    "            if(k==(n1-1)): \n",
    "                window_inner.iloc[i,37]='2018-01-22'\n",
    "            k=k+1\n",
    "        i=i+1\n",
    "\n",
    "    test_data['key']=test_data[\"pid\"].astype(str) +\" \"+ test_data[\"size\"].astype(str)\n",
    "    window_inner[\"date_sold_out_date\"] = pd.to_datetime(window_inner[\"sold_out_date\"],format=\"%Y/%m/%d\")\n",
    "    window_inner[\"date_sold_out_pred\"] = pd.to_datetime(window_inner[\"sol_out_pred\"],format=\"%Y/%m/%d\")\n",
    "\n",
    "    days_differ=window_inner[\"date_sold_out_date\"]-window_inner[\"date_sold_out_pred\"]\n",
    "    days=pd.DataFrame(days_differ,columns=['days_differ'])\n",
    "    days['correct_differ']=abs(days['days_differ'].astype(datetime.timedelta).map(lambda x: np.nan if pd.isnull(x) else x.days))\n",
    "\n",
    "    error=math.sqrt(sum(days['correct_differ']))\n",
    "\n",
    "    return error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error(test_table,total,cluster0,cluster1,cluster2,cluster3,cluster4,cluster5,cluster6,cluster7):\n",
    "    \n",
    "    test_data_0=pd.read_csv(test_table)\n",
    "    test_data_0['pid']=test_data_0['pid'].astype(int)\n",
    "    test_data_0['key']=test_data_0[\"pid\"].astype(str) +\" \"+ test_data_0[\"size\"].astype(str)\n",
    "\n",
    "    error_0=measurement(cluster0, test_data_0)\n",
    "    error_1=measurement(cluster1, test_data_0)\n",
    "    error_2=measurement(cluster2, test_data_0)\n",
    "    error_3=measurement(cluster3, test_data_0)\n",
    "    error_4=measurement(cluster4, test_data_0)\n",
    "    error_5=measurement(cluster5, test_data_0)\n",
    "    error_6=measurement(cluster6, test_data_0)\n",
    "    error_7=measurement(cluster7, test_data_0)\n",
    "    error_total=measurement(total, test_data_0)\n",
    "\n",
    "    print('cluster0',error_0,'\\ncluster1',error_1, '\\ncluster2',error_2,'\\ncluster3',error_3, '\\ncluster4',error_4,\n",
    "          '\\ncluster5',error_5, '\\ncluster6',error_6,'\\ncluster7',error_7,'\\ntotoal',error_total)\n",
    "    \n",
    "    return error_0,error_1,error_2,error_3,error_4,error_5,error_6,error_7,error_total\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster0 191.4654015742792 \n",
      "cluster1 109.99090871522064 \n",
      "cluster2 42.941821107167776 \n",
      "cluster3 7.615773105863909 \n",
      "cluster4 23.15167380558045 \n",
      "cluster5 15.491933384829668 \n",
      "cluster6 2.6457513110645907 \n",
      "cluster7 101.59724405711013 \n",
      "totoal 248.52364072659165\n"
     ]
    }
   ],
   "source": [
    "error_0=error('test_0.csv',pre_total_pvt,pre_cluster0_pvt,pre_cluster1_pvt,pre_cluster2_pvt,pre_cluster3_pvt,pre_cluster4_pvt,pre_cluster5_pvt,pre_cluster6_pvt,pre_cluster7_pvt,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster0 192.3018460649819 \n",
      "cluster1 110.6752004741803 \n",
      "cluster2 42.5205832509386 \n",
      "cluster3 7.874007874011811 \n",
      "cluster4 21.633307652783937 \n",
      "cluster5 14.52583904633395 \n",
      "cluster6 4.0 \n",
      "cluster7 101.48398888494677 \n",
      "totoal 249.18467047553307\n"
     ]
    }
   ],
   "source": [
    "error_1=error('test_1.csv',pre_total_pvt,pre_cluster0_pvt,pre_cluster1_pvt,pre_cluster2_pvt,pre_cluster3_pvt,pre_cluster4_pvt,pre_cluster5_pvt,pre_cluster6_pvt,pre_cluster7_pvt,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster0 192.13536894595956 \n",
      "cluster1 110.61645447219867 \n",
      "cluster2 43.46262762420146 \n",
      "cluster3 7.745966692414834 \n",
      "cluster4 22.38302928559939 \n",
      "cluster5 14.142135623730951 \n",
      "cluster6 4.242640687119285 \n",
      "cluster7 101.35580891098448 \n",
      "totoal 249.18467047553307\n"
     ]
    }
   ],
   "source": [
    "error_2=error('test_2.csv',pre_total_pvt,pre_cluster0_pvt,pre_cluster1_pvt,pre_cluster2_pvt,pre_cluster3_pvt,pre_cluster4_pvt,pre_cluster5_pvt,pre_cluster6_pvt,pre_cluster7_pvt,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster0 191.57505056765612 \n",
      "cluster1 110.95945205344158 \n",
      "cluster2 42.61455150532503 \n",
      "cluster3 8.12403840463596 \n",
      "cluster4 21.330729007701542 \n",
      "cluster5 15.0 \n",
      "cluster6 3.4641016151377544 \n",
      "cluster7 101.06433594498111 \n",
      "totoal 248.5980691799516\n"
     ]
    }
   ],
   "source": [
    "error_3=error('test_3.csv',pre_total_pvt,pre_cluster0_pvt,pre_cluster1_pvt,pre_cluster2_pvt,pre_cluster3_pvt,pre_cluster4_pvt,pre_cluster5_pvt,pre_cluster6_pvt,pre_cluster7_pvt,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster0 192.17179813906097 \n",
      "cluster1 111.18902823570319 \n",
      "cluster2 42.43819034784589 \n",
      "cluster3 7.745966692414834 \n",
      "cluster4 22.781571499789035 \n",
      "cluster5 14.560219778561036 \n",
      "cluster6 4.242640687119285 \n",
      "cluster7 100.93066927351666 \n",
      "totoal 249.17865077088769\n"
     ]
    }
   ],
   "source": [
    "error_4=error('test_4.csv',pre_total_pvt,pre_cluster0_pvt,pre_cluster1_pvt,pre_cluster2_pvt,pre_cluster3_pvt,pre_cluster4_pvt,pre_cluster5_pvt,pre_cluster6_pvt,pre_cluster7_pvt,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster0 191.929893058 \n",
      "cluster1 110.68620879 \n",
      "cluster2 42.7955547671 \n",
      "cluster3 7.82115055387 \n",
      "cluster4 22.2560622503 \n",
      "cluster5 14.7440255667 \n",
      "cluster6 3.71902686009 \n",
      "cluster7 101.286409414 \n",
      "totoal 248.933940326\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "c0=[error_0[0],error_1[0],error_2[0],error_3[0],error_4[0]]\n",
    "c1=[error_0[1],error_1[1],error_2[1],error_3[1],error_4[1]]\n",
    "c2=[error_0[2],error_1[2],error_2[2],error_3[2],error_4[2]]\n",
    "c3=[error_0[3],error_1[3],error_2[3],error_3[3],error_4[3]]\n",
    "c4=[error_0[4],error_1[4],error_2[4],error_3[4],error_4[4]]\n",
    "c5=[error_0[5],error_1[5],error_2[5],error_3[5],error_4[5]]\n",
    "c6=[error_0[6],error_1[6],error_2[6],error_3[6],error_4[6]]\n",
    "c7=[error_0[7],error_1[7],error_2[7],error_3[7],error_4[7]]\n",
    "total=[error_0[8],error_1[8],error_2[8],error_3[8],error_4[8]]\n",
    "\n",
    "print('cluster0',numpy.mean(c0),'\\ncluster1',numpy.mean(c1), '\\ncluster2',numpy.mean(c2),'\\ncluster3',numpy.mean(c3), '\\ncluster4',numpy.mean(c4),\n",
    "           '\\ncluster5',numpy.mean(c5), '\\ncluster6',numpy.mean(c6),'\\ncluster7',numpy.mean(c7),'\\ntotoal',numpy.mean(total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
