{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '/Users/bengikoseoglu/anaconda2/lib/python27.zip',\n",
       " '/Users/bengikoseoglu/anaconda2/lib/python2.7',\n",
       " '/Users/bengikoseoglu/anaconda2/lib/python2.7/plat-darwin',\n",
       " '/Users/bengikoseoglu/anaconda2/lib/python2.7/plat-mac',\n",
       " '/Users/bengikoseoglu/anaconda2/lib/python2.7/plat-mac/lib-scriptpackages',\n",
       " '/Users/bengikoseoglu/anaconda2/lib/python2.7/lib-tk',\n",
       " '/Users/bengikoseoglu/anaconda2/lib/python2.7/lib-old',\n",
       " '/Users/bengikoseoglu/anaconda2/lib/python2.7/lib-dynload',\n",
       " '/Users/bengikoseoglu/anaconda2/lib/python2.7/site-packages',\n",
       " '/Users/bengikoseoglu/anaconda2/lib/python2.7/site-packages/aeosa',\n",
       " '/Users/bengikoseoglu/anaconda2/lib/python2.7/site-packages/IPython/extensions',\n",
       " '/Users/bengikoseoglu/.ipython',\n",
       " '/Users/bengikoseoglu/anaconda2/pkgs',\n",
       " '/Library/Python/2.7/site-packages']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/bengikoseoglu/anaconda2/pkgs')\n",
    "sys.path.append('/Library/Python/2.7/site-packages')\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bengikoseoglu/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/bengikoseoglu/anaconda2/lib/python2.7/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from datetime import date, timedelta\n",
    "import datetime as dt\n",
    "import xgboost\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import cross_validation, metrics   #Additional scklearn functions\n",
    "from sklearn.grid_search import GridSearchCV  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv('integrated_data.csv') #new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df=data[data.month !=1] \n",
    "test_df=data[data.month ==1 ] \n",
    "test_df=test_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Technique- Bining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bin_unit=pd.read_csv('discretized_item_by_sales.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bin_unit=bin_unit.drop('Unnamed: 0',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12824, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_unit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>total_sales</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000 XL ( 158-170 )</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    key  total_sales  category\n",
       "0  10000 XL ( 158-170 )            1         0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_unit.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_unit.category.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Technique 7 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster7=pd.read_csv('clust_7.csv') #cluster numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_cluster=pd.read_csv('data_clustering_R')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_cluster['clust_7']=cluster7['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_cluster=data_cluster[['key','clust_7']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train_df[['date','key','units']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\t\"\"\"\n",
    "\tFrame a time series as a supervised learning dataset.\n",
    "\tArguments:\n",
    "\t\tdata: Sequence of observations as a list or NumPy array.\n",
    "\t\tn_in: Number of lag observations as input (X).\n",
    "\t\tn_out: Number of observations as output (y).\n",
    "\t\tdropnan: Boolean whether or not to drop rows with NaN values.\n",
    "\tReturns:\n",
    "\t\tPandas DataFrame of series framed for supervised learning.\n",
    "\t\"\"\"\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = date(2017, 10, 1)  # start date\n",
    "d2 = date(2017, 12, 31)  # end date\n",
    "d3=date(2018, 1, 31)\n",
    "delta = d2 - d1         # timedelta\n",
    "pre_day=[]\n",
    "for i in range(delta.days + 1):\n",
    "    add=str(d1 + timedelta(days=i))\n",
    "    pre_day.append(add)\n",
    "post_day=[] \n",
    "delta_2 = d3 - d2\n",
    "for i in range(delta_2.days + 1):\n",
    "    add=str(d2 + timedelta(days=i))\n",
    "    post_day.append(add)\n",
    "df_days_pre = pd.DataFrame({'date':pre_day})\n",
    "df_days_post = pd.DataFrame({'date':post_day})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster 30 Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col=post_day[1:]\n",
    "col.append('key')\n",
    "all_=train.key.unique()\n",
    "empty_matrix=np.zeros((len(all_),len(col)))\n",
    "data_frame=pd.DataFrame(data=empty_matrix, columns=col)\n",
    "data_frame['key']=all_\n",
    "days=range(1,32)\n",
    "n1=len(days)\n",
    "empty_matrix_2=np.zeros((all_.shape[0],1))\n",
    "data_frame_2=pd.DataFrame(data=empty_matrix_2, columns=['key'])\n",
    "data_frame_2['key']=all_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales_prev=train.loc[train['date']=='2017-12-31'].sort_values('date')[['key','units']]\n",
    "sales_prev1=train.loc[train['date']=='2017-12-30'].sort_values('date')[['key','units']]\n",
    "sales_prev2=train.loc[train['date']=='2017-12-29'].sort_values('date')[['key','units']]\n",
    "sales_prev3=train.loc[train['date']=='2017-12-28'].sort_values('date')[['key','units']]\n",
    "sales_prev4=train.loc[train['date']=='2017-12-27'].sort_values('date')[['key','units']]\n",
    "sales_prev5=train.loc[train['date']=='2017-12-26'].sort_values('date')[['key','units']]\n",
    "sales_prev6=train.loc[train['date']=='2017-12-25'].sort_values('date')[['key','units']]\n",
    "sales_prev7=train.loc[train['date']=='2017-12-24'].sort_values('date')[['key','units']]\n",
    "sales_prev8=train.loc[train['date']=='2017-12-23'].sort_values('date')[['key','units']] \n",
    "sales_prev9=train.loc[train['date']=='2017-12-22'].sort_values('date')[['key','units']]\n",
    "sales_prev10=train.loc[train['date']=='2017-12-21'].sort_values('date')[['key','units']]\n",
    "sales_prev11=train.loc[train['date']=='2017-12-20'].sort_values('date')[['key','units']]\n",
    "sales_prev12=train.loc[train['date']=='2017-12-19'].sort_values('date')[['key','units']]\n",
    "sales_prev13=train.loc[train['date']=='2017-12-18'].sort_values('date')[['key','units']]\n",
    "sales_prev14=train.loc[train['date']=='2017-12-17'].sort_values('date')[['key','units']]\n",
    "sales_prev15=train.loc[train['date']=='2017-12-16'].sort_values('date')[['key','units']]\n",
    "sales_prev16=train.loc[train['date']=='2017-12-15'].sort_values('date')[['key','units']]\n",
    "sales_prev17=train.loc[train['date']=='2017-12-14'].sort_values('date')[['key','units']]\n",
    "sales_prev18=train.loc[train['date']=='2017-12-13'].sort_values('date')[['key','units']]\n",
    "sales_prev19=train.loc[train['date']=='2017-12-12'].sort_values('date')[['key','units']]\n",
    "sales_prev20=train.loc[train['date']=='2017-12-11'].sort_values('date')[['key','units']]\n",
    "sales_prev21=train.loc[train['date']=='2017-12-10'].sort_values('date')[['key','units']]\n",
    "sales_prev22=train.loc[train['date']=='2017-12-09'].sort_values('date')[['key','units']]\n",
    "sales_prev23=train.loc[train['date']=='2017-12-08'].sort_values('date')[['key','units']]\n",
    "sales_prev24=train.loc[train['date']=='2017-12-07'].sort_values('date')[['key','units']]\n",
    "sales_prev25=train.loc[train['date']=='2017-12-06'].sort_values('date')[['key','units']]\n",
    "sales_prev26=train.loc[train['date']=='2017-12-05'].sort_values('date')[['key','units']]\n",
    "sales_prev27=train.loc[train['date']=='2017-12-04'].sort_values('date')[['key','units']]\n",
    "sales_prev28=train.loc[train['date']=='2017-12-03'].sort_values('date')[['key','units']]\n",
    "sales_prev29=train.loc[train['date']=='2017-12-02'].sort_values('date')[['key','units']]\n",
    "sales_prev30=train.loc[train['date']=='2017-12-01'].sort_values('date')[['key','units']]\n",
    "data_frame_2=data_frame_2.merge(sales_prev, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-31'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev1, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-30'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev2, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-29'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev3, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-28'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev4, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-27'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev5, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-26'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev6, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-25'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev7, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-24'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev8, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-23'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev9, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-22'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev10, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-21'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev11, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-20'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev12, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-19'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev13, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-18'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev14, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-17'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev15, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-16'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev16, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-15'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev17, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-14'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev18, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-13'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev19, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-12'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev20, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-11'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev21, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-10'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev22, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-09'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev23, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-08'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev24, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-07'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev25, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-06'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev26, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-05'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev27, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-04'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev28, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-03'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev29, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-02'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev30, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-01'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_frame_2=data_frame_2.fillna(0)\n",
    "data_frame=pd.merge(data_frame,data_frame_2,on='key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Time Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c_train=train.merge(data_cluster,on='key',how='inner')\n",
    "c_dataframe=data_frame.merge(data_cluster,on='key',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2018-01-01</th>\n",
       "      <th>2018-01-02</th>\n",
       "      <th>2018-01-03</th>\n",
       "      <th>2018-01-04</th>\n",
       "      <th>2018-01-05</th>\n",
       "      <th>2018-01-06</th>\n",
       "      <th>2018-01-07</th>\n",
       "      <th>2018-01-08</th>\n",
       "      <th>2018-01-09</th>\n",
       "      <th>2018-01-10</th>\n",
       "      <th>...</th>\n",
       "      <th>2017-12-09</th>\n",
       "      <th>2017-12-08</th>\n",
       "      <th>2017-12-07</th>\n",
       "      <th>2017-12-06</th>\n",
       "      <th>2017-12-05</th>\n",
       "      <th>2017-12-04</th>\n",
       "      <th>2017-12-03</th>\n",
       "      <th>2017-12-02</th>\n",
       "      <th>2017-12-01</th>\n",
       "      <th>clust_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   2018-01-01  2018-01-02  2018-01-03  2018-01-04  2018-01-05  2018-01-06  \\\n",
       "0         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "1         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "2         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "3         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "4         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "\n",
       "   2018-01-07  2018-01-08  2018-01-09  2018-01-10   ...     2017-12-09  \\\n",
       "0         0.0         0.0         0.0         0.0   ...            0.0   \n",
       "1         0.0         0.0         0.0         0.0   ...            0.0   \n",
       "2         0.0         0.0         0.0         0.0   ...            0.0   \n",
       "3         0.0         0.0         0.0         0.0   ...            0.0   \n",
       "4         0.0         0.0         0.0         0.0   ...            0.0   \n",
       "\n",
       "   2017-12-08  2017-12-07  2017-12-06  2017-12-05  2017-12-04  2017-12-03  \\\n",
       "0         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "1         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "2         2.0         1.0         0.0         0.0         1.0         0.0   \n",
       "3         1.0         0.0         0.0         0.0         0.0         0.0   \n",
       "4         1.0         0.0         0.0         0.0         0.0         0.0   \n",
       "\n",
       "   2017-12-02  2017-12-01  clust_7  \n",
       "0         0.0         0.0        1  \n",
       "1         0.0         0.0        1  \n",
       "2         0.0         0.0        1  \n",
       "3         0.0         0.0        1  \n",
       "4         0.0         0.0        2  \n",
       "\n",
       "[5 rows x 64 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For DataFrame Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_unit=bin_unit.rename(columns={'category':'clust_7'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c_train_bin=train.merge(bin_unit,on='key',how='inner')\n",
    "c_dataframe_bin=data_frame.merge(bin_unit,on='key',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>key</th>\n",
       "      <th>units</th>\n",
       "      <th>total_sales</th>\n",
       "      <th>clust_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-10-01</td>\n",
       "      <td>19671 39 1/3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-10-02</td>\n",
       "      <td>19671 39 1/3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-10-03</td>\n",
       "      <td>19671 39 1/3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-10-04</td>\n",
       "      <td>19671 39 1/3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-10-05</td>\n",
       "      <td>19671 39 1/3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date           key  units  total_sales  clust_7\n",
       "0  2017-10-01  19671 39 1/3    0.0            1        0\n",
       "1  2017-10-02  19671 39 1/3    0.0            1        0\n",
       "2  2017-10-03  19671 39 1/3    0.0            1        0\n",
       "3  2017-10-04  19671 39 1/3    0.0            1        0\n",
       "4  2017-10-05  19671 39 1/3    1.0            1        0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_train_bin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_train_bin.clust_7.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2018-01-01</th>\n",
       "      <th>2018-01-02</th>\n",
       "      <th>2018-01-03</th>\n",
       "      <th>2018-01-04</th>\n",
       "      <th>2018-01-05</th>\n",
       "      <th>2018-01-06</th>\n",
       "      <th>2018-01-07</th>\n",
       "      <th>2018-01-08</th>\n",
       "      <th>2018-01-09</th>\n",
       "      <th>2018-01-10</th>\n",
       "      <th>...</th>\n",
       "      <th>2017-12-08</th>\n",
       "      <th>2017-12-07</th>\n",
       "      <th>2017-12-06</th>\n",
       "      <th>2017-12-05</th>\n",
       "      <th>2017-12-04</th>\n",
       "      <th>2017-12-03</th>\n",
       "      <th>2017-12-02</th>\n",
       "      <th>2017-12-01</th>\n",
       "      <th>total_sales</th>\n",
       "      <th>clust_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   2018-01-01  2018-01-02  2018-01-03  2018-01-04  2018-01-05  2018-01-06  \\\n",
       "0         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "1         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "2         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "3         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "4         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "\n",
       "   2018-01-07  2018-01-08  2018-01-09  2018-01-10   ...     2017-12-08  \\\n",
       "0         0.0         0.0         0.0         0.0   ...            0.0   \n",
       "1         0.0         0.0         0.0         0.0   ...            0.0   \n",
       "2         0.0         0.0         0.0         0.0   ...            2.0   \n",
       "3         0.0         0.0         0.0         0.0   ...            1.0   \n",
       "4         0.0         0.0         0.0         0.0   ...            1.0   \n",
       "\n",
       "   2017-12-07  2017-12-06  2017-12-05  2017-12-04  2017-12-03  2017-12-02  \\\n",
       "0         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "1         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "2         1.0         0.0         0.0         1.0         0.0         0.0   \n",
       "3         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "4         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "\n",
       "   2017-12-01  total_sales  clust_7  \n",
       "0         0.0            1        0  \n",
       "1         0.0            4        1  \n",
       "2         0.0           16        2  \n",
       "3         0.0            8        1  \n",
       "4         0.0           26        3  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_dataframe_bin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12824, 65)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_dataframe_bin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Running the clusters\n",
    "def days_30_cluster(c_dataframe,c_train,clusterno,model):\n",
    "    c_train_c1=c_train[c_train['clust_7']==clusterno].drop('clust_7',axis=1)\n",
    "    print(c_train_c1.shape)\n",
    "    print(c_train_c1.key.nunique())\n",
    "    train_comb_pivot=c_train_c1.pivot(index='key', columns='date', values='units')\n",
    "    train_comb_pivot=train_comb_pivot.mean(axis=0) #assign zero sales\n",
    "    train_comb_pivot=pd.DataFrame(train_comb_pivot,columns=['mean_sales'])\n",
    "    train_cluster1=series_to_supervised(train_comb_pivot, n_in=30, n_out=1, dropnan=True)\n",
    "    cluster1=c_dataframe[c_dataframe['clust_7']==clusterno]\n",
    "    cluster1_n=cluster1.drop('clust_7',axis=1).set_index('key').transpose()\n",
    "    cluster1.mean=cluster1_n.mean(axis=1)\n",
    "    cluster1_frame=pd.DataFrame(cluster1.mean, columns=['mean_sales'])\n",
    "    cluster1_frame=cluster1_frame.transpose()\n",
    "    model.fit(train_cluster1[['var1(t-30)', 'var1(t-29)', 'var1(t-28)', 'var1(t-27)',\n",
    "       'var1(t-26)', 'var1(t-25)', 'var1(t-24)', 'var1(t-23)',\n",
    "       'var1(t-22)', 'var1(t-21)', 'var1(t-20)', 'var1(t-19)',\n",
    "       'var1(t-18)', 'var1(t-17)', 'var1(t-16)', 'var1(t-15)',\n",
    "       'var1(t-14)', 'var1(t-13)', 'var1(t-12)', 'var1(t-11)',\n",
    "       'var1(t-10)', 'var1(t-9)', 'var1(t-8)', 'var1(t-7)', 'var1(t-6)',\n",
    "       'var1(t-5)', 'var1(t-4)', 'var1(t-3)', 'var1(t-2)', 'var1(t-1)']],train_cluster1[['var1(t)']])\n",
    "    \n",
    "    days=range(1,32)\n",
    "    n1=len(days)\n",
    "    k=0\n",
    "    day_n=0\n",
    "    while(k<n1):\n",
    "        day=days[day_n]\n",
    "        predicted_date=datetime.date(year=2018,day=day,month=1)\n",
    "        pred_date_before=predicted_date- timedelta(days=30)\n",
    "        delta = predicted_date - pred_date_before\n",
    "        bet_day=[]\n",
    "        for i in range(delta.days):\n",
    "            add=str(pred_date_before + timedelta(days=i))\n",
    "            bet_day.append(add)\n",
    "        pred_date_before=str(pred_date_before)\n",
    "        predicted_date=str(predicted_date)\n",
    "        value_predict=cluster1_frame[bet_day]\n",
    "        y_pred=model.predict(value_predict)\n",
    "        cluster1_frame[predicted_date]=y_pred\n",
    "        k=k+1\n",
    "        day_n=day_n+1\n",
    "    return(cluster1_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# performance Measurement\n",
    "import math\n",
    "def cluster_performance_measurement(test_data,cluster1_frame,cluster2_frame,cluster3_frame,cluster4_frame,cluster5_frame,cluster6_frame,cluster7_frame,data_cluster,date_ass):\n",
    "    test_data['pid']=test_data[\"pid\"].astype(int)\n",
    "    test_data['key']=test_data[\"pid\"].astype(str) +\" \"+ test_data[\"size\"].astype(str) \n",
    "    test_data=test_data[['key','stock','sold_out_date']]\n",
    "    d1 = date(2018, 1, 1)  \n",
    "    d2 = date(2018, 1, 31)  \n",
    "    delta = d2 - d1         \n",
    "    pre_day=[]\n",
    "    for i in range(delta.days + 1):\n",
    "        add=str(d1 + timedelta(days=i))\n",
    "        pre_day.append(add)\n",
    "\n",
    "    all_=test_data.key.unique()\n",
    "    n=len(test_data.key.unique())\n",
    "    i=0\n",
    "    k=0\n",
    "    n1=len(pre_day)\n",
    "    days=[]\n",
    "    ids=[]\n",
    "    while(i<n):\n",
    "        b=all_[i]\n",
    "        k=0\n",
    "        while(k<n1):\n",
    "            a=pre_day[k]\n",
    "            ids.append(b)\n",
    "            days.append(a)\n",
    "            k=k+1\n",
    "        i=i+1\n",
    "    test_days = pd.DataFrame({'date':days,'key':ids})\n",
    "    test_comb=test_days.merge(test_data,on=['key'],how='outer')\n",
    "    test_comb=test_comb.fillna(0)\n",
    "    c_test=test_comb.merge(data_cluster,on='key',how='left')\n",
    "    c_test['clust_7']=c_test['clust_7'].fillna(8)\n",
    "    check=c_test[['key','sold_out_date','clust_7']]\n",
    "    check=check.drop_duplicates()\n",
    "    check['sol_out_pred']=0\n",
    "    i=0\n",
    "    all_=c_test.key.unique()\n",
    "    sold_out=[]\n",
    "    n=len(all_)\n",
    "    while(i<n):\n",
    "        subset_w=c_test.loc[c_test['key']==all_[i]]\n",
    "        check_w=check.loc[check['key']==all_[i]]\n",
    "        stock=subset_w['stock'].values[0]\n",
    "        clust=subset_w['clust_7'].values[0]\n",
    "        n1=len(pre_day)\n",
    "        k=0\n",
    "        while(k<n1): #day based\n",
    "            day=pre_day[k]\n",
    "            that_day_sale=0\n",
    "            if(clust==1):\n",
    "                that_day_sale=cluster1_frame[day].values[0]\n",
    "            if(clust==2):\n",
    "                that_day_sale=cluster2_frame[day].values[0]  \n",
    "            if(clust==3):\n",
    "                that_day_sale=cluster3_frame[day].values[0]\n",
    "            if(clust==4):\n",
    "                that_day_sale=cluster4_frame[day].values[0]\n",
    "            if(clust==5):\n",
    "                that_day_sale=cluster5_frame[day].values[0]\n",
    "            if(clust==6):\n",
    "                that_day_sale=cluster6_frame[day].values[0]\n",
    "            if(clust==7):\n",
    "                that_day_sale=cluster7_frame[day].values[0]\n",
    "            if(clust==8):\n",
    "                sold_out.append(date_ass)\n",
    "                break\n",
    "            stock=stock-that_day_sale\n",
    "            if((stock==0) | (stock<0) ):\n",
    "                sold_out.append(day)\n",
    "                break\n",
    "            if(k==(n1-1)):\n",
    "                sold_out.append(date_ass)\n",
    "            k=k+1\n",
    "        i=i+1\n",
    "        \n",
    "    check['sol_out_pred']=sold_out\n",
    "    check[\"date_sold_out_date\"] = pd.to_datetime(check[\"sold_out_date\"],format=\"%Y/%m/%d\")\n",
    "    check[\"date_sold_out_pred\"] = pd.to_datetime(check[\"sol_out_pred\"],format=\"%Y/%m/%d\")\n",
    "    days_differ=check[\"date_sold_out_date\"]-check[\"date_sold_out_pred\"]\n",
    "    \n",
    "    check['days_differ']=days_differ\n",
    "    check['correct_differ']=abs(check['days_differ'].astype(dt.timedelta).map(lambda x: np.nan if pd.isnull(x) else x.days))\n",
    "\n",
    "    cluster_size=check.clust_7.value_counts()\n",
    "    by_group=check.groupby(['clust_7']).sum()\n",
    "    by_group['clust_size']=cluster_size\n",
    "    by_group['mean']=by_group['correct_differ']/by_group['clust_size']\n",
    "    print(by_group)\n",
    "    \n",
    "    \n",
    "    days=pd.DataFrame(days_differ,columns=['days_differ'])\n",
    "    days['correct_differ']=abs(days['days_differ'].astype(dt.timedelta).map(lambda x: np.nan if pd.isnull(x) else x.days))\n",
    "    summ=sum(days['correct_differ'])\n",
    "    return(math.sqrt(summ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# performance Measurement\n",
    "import math\n",
    "def cluster_performance_measurement_bin(test_data,cluster1_frame,cluster2_frame,cluster3_frame,cluster4_frame,cluster5_frame,data_cluster,date_ass):\n",
    "    test_data['pid']=test_data[\"pid\"].astype(int)\n",
    "    test_data['key']=test_data[\"pid\"].astype(str) +\" \"+ test_data[\"size\"].astype(str) \n",
    "    test_data=test_data[['key','stock','sold_out_date']]\n",
    "    d1 = date(2018, 1, 1)  \n",
    "    d2 = date(2018, 1, 31)  \n",
    "    delta = d2 - d1         \n",
    "    pre_day=[]\n",
    "    for i in range(delta.days + 1):\n",
    "        add=str(d1 + timedelta(days=i))\n",
    "        pre_day.append(add)\n",
    "\n",
    "    all_=test_data.key.unique()\n",
    "    n=len(test_data.key.unique())\n",
    "    i=0\n",
    "    k=0\n",
    "    n1=len(pre_day)\n",
    "    days=[]\n",
    "    ids=[]\n",
    "    while(i<n):\n",
    "        b=all_[i]\n",
    "        k=0\n",
    "        while(k<n1):\n",
    "            a=pre_day[k]\n",
    "            ids.append(b)\n",
    "            days.append(a)\n",
    "            k=k+1\n",
    "        i=i+1\n",
    "    test_days = pd.DataFrame({'date':days,'key':ids})\n",
    "    test_comb=test_days.merge(test_data,on=['key'],how='outer')\n",
    "    test_comb=test_comb.fillna(0)\n",
    "    c_test=test_comb.merge(data_cluster,on='key',how='left')\n",
    "    check=c_test[['key','sold_out_date','clust_7']]\n",
    "    check=check.drop_duplicates()\n",
    "    check['sol_out_pred']=0\n",
    "    i=0\n",
    "    all_=c_test.key.unique()\n",
    "    sold_out=[]\n",
    "    n=len(all_)\n",
    "    while(i<n):\n",
    "        subset_w=c_test.loc[c_test['key']==all_[i]]\n",
    "        check_w=check.loc[check['key']==all_[i]]\n",
    "        stock=subset_w['stock'].values[0]\n",
    "        clust=subset_w['clust_7'].values[0]\n",
    "        n1=len(pre_day)\n",
    "        k=0\n",
    "        while(k<n1): #day based\n",
    "            day=pre_day[k]\n",
    "            that_day_sale=0\n",
    "            if(clust==0):\n",
    "                that_day_sale=cluster1_frame[day].values[0]\n",
    "            if(clust==1):\n",
    "                that_day_sale=cluster2_frame[day].values[0]  \n",
    "            if(clust==2):\n",
    "                that_day_sale=cluster3_frame[day].values[0]\n",
    "            if(clust==3):\n",
    "                that_day_sale=cluster4_frame[day].values[0]\n",
    "            if(clust==4):\n",
    "                that_day_sale=cluster5_frame[day].values[0]\n",
    "            if(clust=='nan'):\n",
    "                sold_out.append(date_ass)\n",
    "                break\n",
    "            stock=stock-that_day_sale\n",
    "            if((stock==0) | (stock<0) ):\n",
    "                sold_out.append(day)\n",
    "                break\n",
    "            if(k==(n1-1)):\n",
    "                sold_out.append(date_ass)\n",
    "            k=k+1\n",
    "        i=i+1\n",
    "        \n",
    "    check['sol_out_pred']=sold_out\n",
    "    check[\"date_sold_out_date\"] = pd.to_datetime(check[\"sold_out_date\"],format=\"%Y/%m/%d\")\n",
    "    check[\"date_sold_out_pred\"] = pd.to_datetime(check[\"sol_out_pred\"],format=\"%Y/%m/%d\")\n",
    "    days_differ=check[\"date_sold_out_date\"]-check[\"date_sold_out_pred\"]\n",
    "    \n",
    "    check['days_differ']=days_differ\n",
    "    check['correct_differ']=abs(check['days_differ'].astype(dt.timedelta).map(lambda x: np.nan if pd.isnull(x) else x.days))\n",
    "\n",
    "    cluster_size=data_cluster.clust_7.value_counts()\n",
    "    by_group=check.groupby(['clust_7']).sum()\n",
    "    by_group['clust_size']=cluster_size\n",
    "    by_group['mean']=by_group['correct_differ']/by_group['clust_size']\n",
    "    print(by_group)\n",
    "    \n",
    "    \n",
    "    days=pd.DataFrame(days_differ,columns=['days_differ'])\n",
    "    days['correct_differ']=abs(days['days_differ'].astype(dt.timedelta).map(lambda x: np.nan if pd.isnull(x) else x.days))\n",
    "    summ=sum(days['correct_differ'])\n",
    "    return(math.sqrt(summ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original Clusters (Bining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(313586, 4)\n",
      "3446\n"
     ]
    }
   ],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "cluster1_frame=days_30_cluster(c_dataframe_bin,c_train_bin,0,regr) #0, 1, 2, 3, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(384748, 4)\n",
      "4228\n"
     ]
    }
   ],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "cluster2_frame=days_30_cluster(c_dataframe_bin,c_train_bin,1,regr) #0, 1, 2, 3, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(154791, 4)\n",
      "1701\n"
     ]
    }
   ],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "cluster3_frame=days_30_cluster(c_dataframe_bin,c_train_bin,2,regr) #0, 1, 2, 3, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(146510, 4)\n",
      "1610\n"
     ]
    }
   ],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "cluster4_frame=days_30_cluster(c_dataframe_bin,c_train_bin,3,regr) #0, 1, 2, 3, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(167349, 4)\n",
      "1839\n"
     ]
    }
   ],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "cluster5_frame=days_30_cluster(c_dataframe_bin,c_train_bin,4,regr) #0, 1, 2, 3, 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tested on 0 test dataset and saw that, it resulted worse results than clustering 7. Therefore, we didn't tested on other clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         correct_differ  clust_size      mean\n",
      "clust_7                                      \n",
      "0                 11176        3446  3.243180\n",
      "1                 20610        4228  4.874645\n",
      "2                 10152        1701  5.968254\n",
      "3                 12867        1610  7.991925\n",
      "4                 10395        1839  5.652529\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "255.3429066960741"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data=pd.read_csv('test_0.csv') \n",
    "cluster_performance_measurement_bin(test_data,cluster1_frame,cluster2_frame,cluster3_frame,cluster4_frame,cluster5_frame,bin_unit,'2018-01-22')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Windowing Lag(30) Regreesion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far this our best cluster and model. So here we are, running again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(806078, 3)\n",
      "8858\n"
     ]
    }
   ],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "cluster1_frame=days_30_cluster(c_dataframe,c_train,1,regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156520, 3)\n",
      "1720\n"
     ]
    }
   ],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "cluster2_frame=days_30_cluster(c_dataframe,c_train,2,regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25844, 3)\n",
      "284\n"
     ]
    }
   ],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "cluster3_frame=days_30_cluster(c_dataframe,c_train,3,regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1001, 3)\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "cluster4_frame=days_30_cluster(c_dataframe,c_train,4,regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8463, 3)\n",
      "93\n"
     ]
    }
   ],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "cluster5_frame=days_30_cluster(c_dataframe,c_train,5,regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3640, 3)\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "cluster6_frame=days_30_cluster(c_dataframe,c_train,6,regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(364, 3)\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "cluster7_frame=days_30_cluster(c_dataframe,c_train,7,regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pre_total = pd.concat([cluster1_frame,cluster2_frame,cluster3_frame,cluster4_frame,cluster5_frame,cluster6_frame,cluster7_frame])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_total=pre_total.reset_index().drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_total=pre_total.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pre_total.to_csv('Windowing_lag30_best_cluster.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         correct_differ  clust_size      mean\n",
      "clust_7                                      \n",
      "1.0               39175        4668  8.392245\n",
      "2.0               11513        1575  7.309841\n",
      "3.0                1797         281  6.395018\n",
      "4.0                  67          11  6.090909\n",
      "5.0                 571          93  6.139785\n",
      "6.0                 234          40  5.850000\n",
      "7.0                   9           4  2.250000\n",
      "8.0               10322        1470  7.021769\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "252.36481529722008"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data=pd.read_csv('test_0.csv') \n",
    "cluster_performance_measurement(test_data,cluster1_frame,cluster2_frame,cluster3_frame,cluster4_frame,cluster5_frame,cluster6_frame,cluster7_frame,data_cluster,'2018-01-22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data=pd.read_csv('test_1.csv') \n",
    "cluster_performance_measurement(test_data,cluster1_frame,cluster2_frame,cluster3_frame,cluster4_frame,cluster5_frame,cluster6_frame,cluster7_frame,data_cluster,'2018-01-22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data=pd.read_csv('test_2.csv') \n",
    "cluster_performance_measurement(test_data,cluster1_frame,cluster2_frame,cluster3_frame,cluster4_frame,cluster5_frame,cluster6_frame,cluster7_frame,data_cluster,'2018-01-22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data=pd.read_csv('test_3.csv') \n",
    "cluster_performance_measurement(test_data,cluster1_frame,cluster2_frame,cluster3_frame,cluster4_frame,cluster5_frame,cluster6_frame,cluster7_frame,data_cluster,'2018-01-22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data=pd.read_csv('test_4.csv') \n",
    "cluster_performance_measurement(test_data,cluster1_frame,cluster2_frame,cluster3_frame,cluster4_frame,cluster5_frame,cluster6_frame,cluster7_frame,data_cluster,'2018-01-22')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Windowing Lag(30) Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(806078, 3)\n",
      "8858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bengikoseoglu/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:21: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    }
   ],
   "source": [
    "clf=RandomForestRegressor(max_features='sqrt', min_samples_split=2, n_estimators=10, min_samples_leaf= 4)\n",
    "cluster1_frame=days_30_cluster(c_dataframe,c_train,1,clf)#c_dataframe,c_train,6,regr,True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156520, 3)\n",
      "1720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bengikoseoglu/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:21: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    }
   ],
   "source": [
    "clf=RandomForestRegressor(max_features='sqrt', min_samples_split=2, n_estimators=10, min_samples_leaf= 4)\n",
    "cluster2_frame=days_30_cluster(c_dataframe,c_train,2,clf)#c_dataframe,c_train,6,regr,True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25844, 3)\n",
      "284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bengikoseoglu/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:21: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    }
   ],
   "source": [
    "clf=RandomForestRegressor(max_features='sqrt', min_samples_split=2, n_estimators=10, min_samples_leaf= 4)\n",
    "cluster3_frame=days_30_cluster(c_dataframe,c_train,3,clf)#c_dataframe,c_train,6,regr,True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1001, 3)\n",
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bengikoseoglu/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:21: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    }
   ],
   "source": [
    "clf=RandomForestRegressor(max_features='sqrt', min_samples_split=2, n_estimators=10, min_samples_leaf= 4)\n",
    "cluster4_frame=days_30_cluster(c_dataframe,c_train,4,clf)#c_dataframe,c_train,6,regr,True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8463, 3)\n",
      "93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bengikoseoglu/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:21: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    }
   ],
   "source": [
    "clf=RandomForestRegressor(max_features='sqrt', min_samples_split=2, n_estimators=10, min_samples_leaf= 4)\n",
    "cluster5_frame=days_30_cluster(c_dataframe,c_train,5,clf)#c_dataframe,c_train,6,regr,True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3640, 3)\n",
      "40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bengikoseoglu/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:21: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    }
   ],
   "source": [
    "clf=RandomForestRegressor(max_features='sqrt', min_samples_split=2, n_estimators=10, min_samples_leaf= 4)\n",
    "cluster6_frame=days_30_cluster(c_dataframe,c_train,6,clf)#c_dataframe,c_train,6,regr,True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(364, 3)\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bengikoseoglu/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:21: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    }
   ],
   "source": [
    "clf=RandomForestRegressor(max_features='sqrt', min_samples_split=2, n_estimators=10, min_samples_leaf= 4)\n",
    "cluster7_frame=days_30_cluster(c_dataframe,c_train,7,clf)#c_dataframe,c_train,6,regr,True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         correct_differ  clust_size      mean\n",
      "clust_7                                      \n",
      "1.0               40188        4668  8.609254\n",
      "2.0               12880        1575  8.177778\n",
      "3.0                1888         281  6.718861\n",
      "4.0                  74          11  6.727273\n",
      "5.0                 555          93  5.967742\n",
      "6.0                 240          40  6.000000\n",
      "7.0                   9           4  2.250000\n",
      "8.0               10322        1470  7.021769\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "257.2080869646209"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data=pd.read_csv('test_0.csv') \n",
    "cluster_performance_measurement(test_data,cluster1_frame,cluster2_frame,cluster3_frame,cluster4_frame,cluster5_frame,cluster6_frame,cluster7_frame,data_cluster,'2018-01-22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         correct_differ  clust_size      mean\n",
      "clust_7                                      \n",
      "1.0               40568        4668  8.690660\n",
      "2.0               13053        1575  8.287619\n",
      "3.0                1817         281  6.466192\n",
      "4.0                  78          11  7.090909\n",
      "5.0                 493          93  5.301075\n",
      "6.0                 212          40  5.300000\n",
      "7.0                  17           4  4.250000\n",
      "8.0               10299        1470  7.006122\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "257.9476691113917"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data=pd.read_csv('test_1.csv') \n",
    "cluster_performance_measurement(test_data,cluster1_frame,cluster2_frame,cluster3_frame,cluster4_frame,cluster5_frame,cluster6_frame,cluster7_frame,data_cluster,'2018-01-22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         correct_differ  clust_size      mean\n",
      "clust_7                                      \n",
      "1.0               40414        4668  8.657669\n",
      "2.0               13049        1575  8.285079\n",
      "3.0                1912         281  6.804270\n",
      "4.0                  73          11  6.636364\n",
      "5.0                 527          93  5.666667\n",
      "6.0                 200          40  5.000000\n",
      "7.0                  18           4  4.500000\n",
      "8.0               10273        1470  6.988435\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "257.8100075637096"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data=pd.read_csv('test_2.csv') \n",
    "cluster_performance_measurement(test_data,cluster1_frame,cluster2_frame,cluster3_frame,cluster4_frame,cluster5_frame,cluster6_frame,cluster7_frame,data_cluster,'2018-01-22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         correct_differ  clust_size      mean\n",
      "clust_7                                      \n",
      "1.0               40366        4668  8.647386\n",
      "2.0               13131        1575  8.337143\n",
      "3.0                1850         281  6.583630\n",
      "4.0                  78          11  7.090909\n",
      "5.0                 486          93  5.225806\n",
      "6.0                 229          40  5.725000\n",
      "7.0                  12           4  3.000000\n",
      "8.0               10214        1470  6.948299\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "257.615993292342"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data=pd.read_csv('test_3.csv') \n",
    "cluster_performance_measurement(test_data,cluster1_frame,cluster2_frame,cluster3_frame,cluster4_frame,cluster5_frame,cluster6_frame,cluster7_frame,data_cluster,'2018-01-22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         correct_differ  clust_size      mean\n",
      "clust_7                                      \n",
      "1.0               40557        4668  8.688303\n",
      "2.0               13177        1575  8.366349\n",
      "3.0                1781         281  6.338078\n",
      "4.0                  76          11  6.909091\n",
      "5.0                 551          93  5.924731\n",
      "6.0                 214          40  5.350000\n",
      "7.0                  17           4  4.250000\n",
      "8.0               10187        1470  6.929932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "257.9922479455536"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data=pd.read_csv('test_4.csv') \n",
    "cluster_performance_measurement(test_data,cluster1_frame,cluster2_frame,cluster3_frame,cluster4_frame,cluster5_frame,cluster6_frame,cluster7_frame,data_cluster,'2018-01-22')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Windowing Lag(30) ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(806078, 3)\n",
      "8858\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPRegressor()\n",
    "cluster1_frame=days_30_cluster(c_dataframe,c_train,1,mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156520, 3)\n",
      "1720\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPRegressor()\n",
    "cluster2_frame=days_30_cluster(c_dataframe,c_train,2,mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25844, 3)\n",
      "284\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPRegressor()\n",
    "cluster3_frame=days_30_cluster(c_dataframe,c_train,3,mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1001, 3)\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPRegressor()\n",
    "cluster4_frame=days_30_cluster(c_dataframe,c_train,4,mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8463, 3)\n",
      "93\n",
      "91\n",
      "8463\n"
     ]
    }
   ],
   "source": [
    "cluster5_frame=days_30_cluster(c_dataframe,5,mlp,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3640, 3)\n",
      "40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bengikoseoglu/anaconda2/lib/python2.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPRegressor()\n",
    "cluster6_frame=days_30_cluster(c_dataframe,c_train,6,mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         correct_differ  clust_size      mean\n",
      "clust_7                                      \n",
      "1.0               44095        4668  9.446230\n",
      "2.0               13576        1575  8.619683\n",
      "3.0                1810         281  6.441281\n",
      "4.0                  54          11  4.909091\n",
      "5.0                 530          93  5.698925\n",
      "6.0                 281          40  7.025000\n",
      "7.0                  20           4  5.000000\n",
      "8.0               10322        1470  7.021769\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "265.87214972614186"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data=pd.read_csv('test_0.csv') \n",
    "cluster_performance_measurement(test_data,cluster1_frame,cluster2_frame,cluster3_frame,cluster4_frame,cluster5_frame,cluster6_frame,cluster7_frame,data_cluster,'2018-01-22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         correct_differ  clust_size      mean\n",
      "clust_7                                      \n",
      "1.0               44438        4668  9.519709\n",
      "2.0               13758        1575  8.735238\n",
      "3.0                1762         281  6.270463\n",
      "4.0                  48          11  4.363636\n",
      "5.0                 465          93  5.000000\n",
      "6.0                 262          40  6.550000\n",
      "7.0                  31           4  7.750000\n",
      "8.0               10299        1470  7.006122\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "266.5764430702758"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data=pd.read_csv('test_1.csv') \n",
    "cluster_performance_measurement(test_data,cluster1_frame,cluster2_frame,cluster3_frame,cluster4_frame,cluster5_frame,cluster6_frame,cluster7_frame,data_cluster,'2018-01-22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         correct_differ  clust_size      mean\n",
      "clust_7                                      \n",
      "1.0               44224        4668  9.473865\n",
      "2.0               13738        1575  8.722540\n",
      "3.0                1831         281  6.516014\n",
      "4.0                  51          11  4.636364\n",
      "5.0                 499          93  5.365591\n",
      "6.0                 261          40  6.525000\n",
      "7.0                  32           4  8.000000\n",
      "8.0               10273        1470  6.988435\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "266.2874386823382"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data=pd.read_csv('test_2.csv') \n",
    "cluster_performance_measurement(test_data,cluster1_frame,cluster2_frame,cluster3_frame,cluster4_frame,cluster5_frame,cluster6_frame,cluster7_frame,data_cluster,'2018-01-22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         correct_differ  clust_size      mean\n",
      "clust_7                                      \n",
      "1.0               44301        4668  9.490360\n",
      "2.0               13794        1575  8.758095\n",
      "3.0                1783         281  6.345196\n",
      "4.0                  55          11  5.000000\n",
      "5.0                 459          93  4.935484\n",
      "6.0                 283          40  7.075000\n",
      "7.0                  27           4  6.750000\n",
      "8.0               10214        1470  6.948299\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "266.3005820496831"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data=pd.read_csv('test_3.csv') \n",
    "cluster_performance_measurement(test_data,cluster1_frame,cluster2_frame,cluster3_frame,cluster4_frame,cluster5_frame,cluster6_frame,cluster7_frame,data_cluster,'2018-01-22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         correct_differ  clust_size      mean\n",
      "clust_7                                      \n",
      "1.0               44526        4668  9.538560\n",
      "2.0               13873        1575  8.808254\n",
      "3.0                1760         281  6.263345\n",
      "4.0                  52          11  4.727273\n",
      "5.0                 529          93  5.688172\n",
      "6.0                 246          40  6.150000\n",
      "7.0                  30           4  7.500000\n",
      "8.0               10187        1470  6.929932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "266.8389027109803"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data=pd.read_csv('test_4.csv') \n",
    "cluster_performance_measurement(test_data,cluster1_frame,cluster2_frame,cluster3_frame,cluster4_frame,cluster5_frame,cluster6_frame,cluster7_frame,data_cluster,'2018-01-22')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Windowing Lag(30) Gradient Boosting Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "params = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 2,\n",
    "              'learning_rate': 0.01, 'loss': 'ls'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(806078, 3)\n",
      "8858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bengikoseoglu/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "clf = ensemble.GradientBoostingRegressor(**params)\n",
    "cluster1_frame=days_30_cluster(c_dataframe,c_train,1,clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156520, 3)\n",
      "1720\n"
     ]
    }
   ],
   "source": [
    "clf = ensemble.GradientBoostingRegressor(**params)\n",
    "cluster2_frame=days_30_cluster(c_dataframe,c_train,2,clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25844, 3)\n",
      "284\n"
     ]
    }
   ],
   "source": [
    "clf = ensemble.GradientBoostingRegressor(**params)\n",
    "cluster3_frame=days_30_cluster(c_dataframe,c_train,3,clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1001, 3)\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "clf = ensemble.GradientBoostingRegressor(**params)\n",
    "cluster4_frame=days_30_cluster(c_dataframe,c_train,4,clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8463, 3)\n",
      "93\n"
     ]
    }
   ],
   "source": [
    "clf = ensemble.GradientBoostingRegressor(**params)\n",
    "cluster5_frame=days_30_cluster(c_dataframe,c_train,5,clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3640, 3)\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "clf = ensemble.GradientBoostingRegressor(**params)\n",
    "cluster6_frame=days_30_cluster(c_dataframe,c_train,6,clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(364, 3)\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "clf = ensemble.GradientBoostingRegressor(**params)\n",
    "cluster7_frame=days_30_cluster(c_dataframe,c_train,7,clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         correct_differ  clust_size      mean\n",
      "clust_7                                      \n",
      "1.0               41325        4668  8.852828\n",
      "2.0               12524        1575  7.951746\n",
      "3.0                1879         281  6.686833\n",
      "4.0                  67          11  6.090909\n",
      "5.0                 569          93  6.118280\n",
      "6.0                 238          40  5.950000\n",
      "7.0                  11           4  2.750000\n",
      "8.0               10322        1470  7.021769\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "258.7179931894958"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data=pd.read_csv('test_0.csv') \n",
    "cluster_performance_measurement(test_data,cluster1_frame,cluster2_frame,cluster3_frame,cluster4_frame,cluster5_frame,cluster6_frame,cluster7_frame,data_cluster,'2018-01-22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         correct_differ  clust_size      mean\n",
      "clust_7                                      \n",
      "1.0               41702        4668  8.933590\n",
      "2.0               12647        1575  8.029841\n",
      "3.0                1822         281  6.483986\n",
      "4.0                  66          11  6.000000\n",
      "5.0                 496          93  5.333333\n",
      "6.0                 226          40  5.650000\n",
      "7.0                  20           4  5.000000\n",
      "8.0               10299        1470  7.006122\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "259.3800300717077"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data=pd.read_csv('test_1.csv') \n",
    "cluster_performance_measurement(test_data,cluster1_frame,cluster2_frame,cluster3_frame,cluster4_frame,cluster5_frame,cluster6_frame,cluster7_frame,data_cluster,'2018-01-22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         correct_differ  clust_size      mean\n",
      "clust_7                                      \n",
      "1.0               41522        4668  8.895030\n",
      "2.0               12614        1575  8.008889\n",
      "3.0                1908         281  6.790036\n",
      "4.0                  66          11  6.000000\n",
      "5.0                 517          93  5.559140\n",
      "6.0                 227          40  5.675000\n",
      "7.0                  20           4  5.000000\n",
      "8.0               10273        1470  6.988435\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "259.12738180285"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data=pd.read_csv('test_2.csv') \n",
    "cluster_performance_measurement(test_data,cluster1_frame,cluster2_frame,cluster3_frame,cluster4_frame,cluster5_frame,cluster6_frame,cluster7_frame,data_cluster,'2018-01-22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         correct_differ  clust_size      mean\n",
      "clust_7                                      \n",
      "1.0               41519        4668  8.894387\n",
      "2.0               12709        1575  8.069206\n",
      "3.0                1862         281  6.626335\n",
      "4.0                  70          11  6.363636\n",
      "5.0                 484          93  5.204301\n",
      "6.0                 232          40  5.800000\n",
      "7.0                  17           4  4.250000\n",
      "8.0               10214        1470  6.948299\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "259.05018818754024"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data=pd.read_csv('test_3.csv') \n",
    "cluster_performance_measurement(test_data,cluster1_frame,cluster2_frame,cluster3_frame,cluster4_frame,cluster5_frame,cluster6_frame,cluster7_frame,data_cluster,'2018-01-22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         correct_differ  clust_size      mean\n",
      "clust_7                                      \n",
      "1.0               41714        4668  8.936161\n",
      "2.0               12746        1575  8.092698\n",
      "3.0                1824         281  6.491103\n",
      "4.0                  62          11  5.636364\n",
      "5.0                 556          93  5.978495\n",
      "6.0                 241          40  6.025000\n",
      "7.0                  23           4  5.750000\n",
      "8.0               10187        1470  6.929932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "259.52456531126296"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data=pd.read_csv('test_4.csv') \n",
    "cluster_performance_measurement(test_data,cluster1_frame,cluster2_frame,cluster3_frame,cluster4_frame,cluster5_frame,cluster6_frame,cluster7_frame,data_cluster,'2018-01-22')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
