{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '/Users/bengikoseoglu/anaconda2/lib/python27.zip',\n",
       " '/Users/bengikoseoglu/anaconda2/lib/python2.7',\n",
       " '/Users/bengikoseoglu/anaconda2/lib/python2.7/plat-darwin',\n",
       " '/Users/bengikoseoglu/anaconda2/lib/python2.7/plat-mac',\n",
       " '/Users/bengikoseoglu/anaconda2/lib/python2.7/plat-mac/lib-scriptpackages',\n",
       " '/Users/bengikoseoglu/anaconda2/lib/python2.7/lib-tk',\n",
       " '/Users/bengikoseoglu/anaconda2/lib/python2.7/lib-old',\n",
       " '/Users/bengikoseoglu/anaconda2/lib/python2.7/lib-dynload',\n",
       " '/Users/bengikoseoglu/anaconda2/lib/python2.7/site-packages',\n",
       " '/Users/bengikoseoglu/anaconda2/lib/python2.7/site-packages/aeosa',\n",
       " '/Users/bengikoseoglu/anaconda2/lib/python2.7/site-packages/IPython/extensions',\n",
       " '/Users/bengikoseoglu/.ipython',\n",
       " '/Users/bengikoseoglu/anaconda2/pkgs',\n",
       " '/Library/Python/2.7/site-packages']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/bengikoseoglu/anaconda2/pkgs')\n",
    "sys.path.append('/Library/Python/2.7/site-packages')\n",
    "sys.path2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bengikoseoglu/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/bengikoseoglu/anaconda2/lib/python2.7/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from datetime import date, timedelta\n",
    "import datetime as dt\n",
    "import xgboost\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import cross_validation, metrics   #Additional scklearn functions\n",
    "from sklearn.grid_search import GridSearchCV  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv('integrated_data.csv') #new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df=data[data.month !=1] \n",
    "test_df=data[data.month ==1 ] \n",
    "test_df=test_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Technique 7 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster7=pd.read_csv('clust_7.csv') #cluster numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_cluster=pd.read_csv('data_clustering_R')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_cluster['clust_7']=cluster7['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_cluster=data_cluster[['key','clust_7']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train=train_df[['date','key','units']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\t\"\"\"\n",
    "\tFrame a time series as a supervised learning dataset.\n",
    "\tArguments:\n",
    "\t\tdata: Sequence of observations as a list or NumPy array.\n",
    "\t\tn_in: Number of lag observations as input (X).\n",
    "\t\tn_out: Number of observations as output (y).\n",
    "\t\tdropnan: Boolean whether or not to drop rows with NaN values.\n",
    "\tReturns:\n",
    "\t\tPandas DataFrame of series framed for supervised learning.\n",
    "\t\"\"\"\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d1 = date(2017, 10, 1)  # start date\n",
    "d2 = date(2017, 12, 31)  # end date\n",
    "d3=date(2018, 1, 31)\n",
    "delta = d2 - d1         # timedelta\n",
    "pre_day=[]\n",
    "for i in range(delta.days + 1):\n",
    "    add=str(d1 + timedelta(days=i))\n",
    "    pre_day.append(add)\n",
    "post_day=[] \n",
    "delta_2 = d3 - d2\n",
    "for i in range(delta_2.days + 1):\n",
    "    add=str(d2 + timedelta(days=i))\n",
    "    post_day.append(add)\n",
    "df_days_pre = pd.DataFrame({'date':pre_day})\n",
    "df_days_post = pd.DataFrame({'date':post_day})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster 20 Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col=post_day[1:]\n",
    "col.append('key')\n",
    "all_=train.key.unique()\n",
    "empty_matrix=np.zeros((len(all_),len(col)))\n",
    "data_frame=pd.DataFrame(data=empty_matrix, columns=col)\n",
    "data_frame['key']=all_\n",
    "days=range(1,32)\n",
    "n1=len(days)\n",
    "empty_matrix_2=np.zeros((all_.shape[0],1))\n",
    "data_frame_2=pd.DataFrame(data=empty_matrix_2, columns=['key'])\n",
    "data_frame_2['key']=all_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales_prev=train.loc[train['date']=='2017-12-31'].sort_values('date')[['key','units']]\n",
    "sales_prev1=train.loc[train['date']=='2017-12-30'].sort_values('date')[['key','units']]\n",
    "sales_prev2=train.loc[train['date']=='2017-12-29'].sort_values('date')[['key','units']]\n",
    "sales_prev3=train.loc[train['date']=='2017-12-28'].sort_values('date')[['key','units']]\n",
    "sales_prev4=train.loc[train['date']=='2017-12-27'].sort_values('date')[['key','units']]\n",
    "sales_prev5=train.loc[train['date']=='2017-12-26'].sort_values('date')[['key','units']]\n",
    "sales_prev6=train.loc[train['date']=='2017-12-25'].sort_values('date')[['key','units']]\n",
    "sales_prev7=train.loc[train['date']=='2017-12-24'].sort_values('date')[['key','units']]\n",
    "sales_prev8=train.loc[train['date']=='2017-12-23'].sort_values('date')[['key','units']] \n",
    "sales_prev9=train.loc[train['date']=='2017-12-22'].sort_values('date')[['key','units']]\n",
    "sales_prev10=train.loc[train['date']=='2017-12-21'].sort_values('date')[['key','units']]\n",
    "sales_prev11=train.loc[train['date']=='2017-12-20'].sort_values('date')[['key','units']]\n",
    "sales_prev12=train.loc[train['date']=='2017-12-19'].sort_values('date')[['key','units']]\n",
    "sales_prev13=train.loc[train['date']=='2017-12-18'].sort_values('date')[['key','units']]\n",
    "sales_prev14=train.loc[train['date']=='2017-12-17'].sort_values('date')[['key','units']]\n",
    "sales_prev15=train.loc[train['date']=='2017-12-16'].sort_values('date')[['key','units']]\n",
    "sales_prev16=train.loc[train['date']=='2017-12-15'].sort_values('date')[['key','units']]\n",
    "sales_prev17=train.loc[train['date']=='2017-12-14'].sort_values('date')[['key','units']]\n",
    "sales_prev18=train.loc[train['date']=='2017-12-13'].sort_values('date')[['key','units']]\n",
    "sales_prev19=train.loc[train['date']=='2017-12-12'].sort_values('date')[['key','units']]\n",
    "sales_prev20=train.loc[train['date']=='2017-12-11'].sort_values('date')[['key','units']]\n",
    "sales_prev21=train.loc[train['date']=='2017-12-10'].sort_values('date')[['key','units']]\n",
    "sales_prev22=train.loc[train['date']=='2017-12-09'].sort_values('date')[['key','units']]\n",
    "data_frame_2=data_frame_2.merge(sales_prev, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-31'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev1, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-30'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev2, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-29'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev3, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-28'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev4, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-27'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev5, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-26'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev6, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-25'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev7, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-24'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev8, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-23'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev9, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-22'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev10, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-21'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev11, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-20'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev12, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-19'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev13, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-18'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev14, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-17'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev15, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-16'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev16, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-15'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev17, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-14'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev18, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-13'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev19, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-12'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev20, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-11'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev21, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-10'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev22, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-09'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_frame_2=data_frame_2.fillna(0)\n",
    "data_frame=pd.merge(data_frame,data_frame_2,on='key')\n",
    "c_train=train.merge(data_cluster,on='key',how='inner')\n",
    "c_dataframe=data_frame.merge(data_cluster,on='key',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Running the clusters\n",
    "def days_20_cluster(c_dataframe,c_train,clusterno,model):\n",
    "    c_train_c1=c_train[c_train['clust_7']==clusterno].drop('clust_7',axis=1)\n",
    "    print(c_train_c1.shape)\n",
    "    print(c_train_c1.key.nunique())\n",
    "    train_comb_pivot=c_train_c1.pivot(index='key', columns='date', values='units')\n",
    "    train_comb_pivot=train_comb_pivot.mean(axis=0) #assign zero sales\n",
    "    train_comb_pivot=pd.DataFrame(train_comb_pivot,columns=['mean_sales'])\n",
    "    train_cluster1=series_to_supervised(train_comb_pivot, n_in=20, n_out=1, dropnan=True)\n",
    "    cluster1=c_dataframe[c_dataframe['clust_7']==clusterno]\n",
    "    cluster1_n=cluster1.drop('clust_7',axis=1).set_index('key').transpose()\n",
    "    cluster1.mean=cluster1_n.mean(axis=1)\n",
    "    cluster1_frame=pd.DataFrame(cluster1.mean, columns=['mean_sales'])\n",
    "    cluster1_frame=cluster1_frame.transpose()\n",
    "    model.fit(train_cluster1[['var1(t-20)', 'var1(t-19)', 'var1(t-18)', 'var1(t-17)',\n",
    "       'var1(t-16)', 'var1(t-15)', 'var1(t-14)', 'var1(t-13)',\n",
    "       'var1(t-12)', 'var1(t-11)', 'var1(t-10)', 'var1(t-9)', 'var1(t-8)',\n",
    "       'var1(t-7)', 'var1(t-6)', 'var1(t-5)', 'var1(t-4)', 'var1(t-3)',\n",
    "       'var1(t-2)', 'var1(t-1)']],train_cluster1[['var1(t)']])\n",
    "    \n",
    "    days=range(1,32)\n",
    "    n1=len(days)\n",
    "    k=0\n",
    "    day_n=0\n",
    "    while(k<n1):\n",
    "        day=days[day_n]\n",
    "        predicted_date=datetime.date(year=2018,day=day,month=1)\n",
    "        pred_date_before=predicted_date- timedelta(days=20)\n",
    "        delta = predicted_date - pred_date_before\n",
    "        bet_day=[]\n",
    "        for i in range(delta.days):\n",
    "            add=str(pred_date_before + timedelta(days=i))\n",
    "            bet_day.append(add)\n",
    "        pred_date_before=str(pred_date_before)\n",
    "        predicted_date=str(predicted_date)\n",
    "        value_predict=cluster1_frame[bet_day]\n",
    "        y_pred=model.predict(value_predict)\n",
    "        cluster1_frame[predicted_date]=y_pred\n",
    "        k=k+1\n",
    "        day_n=day_n+1\n",
    "    return(cluster1_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# performance Measurement\n",
    "import math\n",
    "def cluster_performance_measurement(test_data,cluster1_frame,cluster2_frame,cluster3_frame,cluster4_frame,cluster5_frame,cluster6_frame,cluster7_frame,data_cluster,date_ass):\n",
    "    test_data['pid']=test_data[\"pid\"].astype(int)\n",
    "    test_data['key']=test_data[\"pid\"].astype(str) +\" \"+ test_data[\"size\"].astype(str) \n",
    "    test_data=test_data[['key','stock','sold_out_date']]\n",
    "    d1 = date(2018, 1, 1)  \n",
    "    d2 = date(2018, 1, 31)  \n",
    "    delta = d2 - d1         \n",
    "    pre_day=[]\n",
    "    for i in range(delta.days + 1):\n",
    "        add=str(d1 + timedelta(days=i))\n",
    "        pre_day.append(add)\n",
    "\n",
    "    all_=test_data.key.unique()\n",
    "    n=len(test_data.key.unique())\n",
    "    i=0\n",
    "    k=0\n",
    "    n1=len(pre_day)\n",
    "    days=[]\n",
    "    ids=[]\n",
    "    while(i<n):\n",
    "        b=all_[i]\n",
    "        k=0\n",
    "        while(k<n1):\n",
    "            a=pre_day[k]\n",
    "            ids.append(b)\n",
    "            days.append(a)\n",
    "            k=k+1\n",
    "        i=i+1\n",
    "    test_days = pd.DataFrame({'date':days,'key':ids})\n",
    "    test_comb=test_days.merge(test_data,on=['key'],how='outer')\n",
    "    test_comb=test_comb.fillna(0)\n",
    "    c_test=test_comb.merge(data_cluster,on='key',how='left')\n",
    "    c_test['clust_7']=c_test['clust_7'].fillna(8)\n",
    "    check=c_test[['key','sold_out_date','clust_7']]\n",
    "    check=check.drop_duplicates()\n",
    "    check['sol_out_pred']=0\n",
    "    i=0\n",
    "    all_=c_test.key.unique()\n",
    "    sold_out=[]\n",
    "    n=len(all_)\n",
    "    while(i<n):\n",
    "        subset_w=c_test.loc[c_test['key']==all_[i]]\n",
    "        check_w=check.loc[check['key']==all_[i]]\n",
    "        stock=subset_w['stock'].values[0]\n",
    "        clust=subset_w['clust_7'].values[0]\n",
    "        n1=len(pre_day)\n",
    "        k=0\n",
    "        while(k<n1): #day based\n",
    "            day=pre_day[k]\n",
    "            that_day_sale=0\n",
    "            if(clust==1):\n",
    "                that_day_sale=cluster1_frame[day].values[0]\n",
    "            if(clust==2):\n",
    "                that_day_sale=cluster2_frame[day].values[0]  \n",
    "            if(clust==3):\n",
    "                that_day_sale=cluster3_frame[day].values[0]\n",
    "            if(clust==4):\n",
    "                that_day_sale=cluster4_frame[day].values[0]\n",
    "            if(clust==5):\n",
    "                that_day_sale=cluster5_frame[day].values[0]\n",
    "            if(clust==6):\n",
    "                that_day_sale=cluster6_frame[day].values[0]\n",
    "            if(clust==7):\n",
    "                that_day_sale=cluster7_frame[day].values[0]\n",
    "            if(clust==8):\n",
    "                sold_out.append(date_ass)\n",
    "                break\n",
    "            stock=stock-that_day_sale\n",
    "            if((stock==0) | (stock<0) ):\n",
    "                sold_out.append(day)\n",
    "                break\n",
    "            if(k==(n1-1)):\n",
    "                sold_out.append(date_ass)\n",
    "            k=k+1\n",
    "        i=i+1\n",
    "        \n",
    "    check['sol_out_pred']=sold_out\n",
    "    check[\"date_sold_out_date\"] = pd.to_datetime(check[\"sold_out_date\"],format=\"%Y/%m/%d\")\n",
    "    check[\"date_sold_out_pred\"] = pd.to_datetime(check[\"sol_out_pred\"],format=\"%Y/%m/%d\")\n",
    "    days_differ=check[\"date_sold_out_date\"]-check[\"date_sold_out_pred\"]\n",
    "    \n",
    "    check['days_differ']=days_differ\n",
    "    check['correct_differ']=abs(check['days_differ'].astype(dt.timedelta).map(lambda x: np.nan if pd.isnull(x) else x.days))\n",
    "\n",
    "    cluster_size=check.clust_7.value_counts()\n",
    "    by_group=check.groupby(['clust_7']).sum()\n",
    "    by_group['clust_size']=cluster_size\n",
    "    by_group['mean']=by_group['correct_differ']/by_group['clust_size']\n",
    "    print(by_group)\n",
    "    \n",
    "    \n",
    "    days=pd.DataFrame(days_differ,columns=['days_differ'])\n",
    "    days['correct_differ']=abs(days['days_differ'].astype(dt.timedelta).map(lambda x: np.nan if pd.isnull(x) else x.days))\n",
    "    summ=sum(days['correct_differ'])\n",
    "    return(math.sqrt(summ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# performance Measurement\n",
    "import math\n",
    "def cluster_performance_measurement_bin(test_data,cluster1_frame,cluster2_frame,cluster3_frame,cluster4_frame,cluster5_frame,data_cluster,date_ass):\n",
    "    test_data['pid']=test_data[\"pid\"].astype(int)\n",
    "    test_data['key']=test_data[\"pid\"].astype(str) +\" \"+ test_data[\"size\"].astype(str) \n",
    "    test_data=test_data[['key','stock','sold_out_date']]\n",
    "    d1 = date(2018, 1, 1)  \n",
    "    d2 = date(2018, 1, 31)  \n",
    "    delta = d2 - d1         \n",
    "    pre_day=[]\n",
    "    for i in range(delta.days + 1):\n",
    "        add=str(d1 + timedelta(days=i))\n",
    "        pre_day.append(add)\n",
    "\n",
    "    all_=test_data.key.unique()\n",
    "    n=len(test_data.key.unique())\n",
    "    i=0\n",
    "    k=0\n",
    "    n1=len(pre_day)\n",
    "    days=[]\n",
    "    ids=[]\n",
    "    while(i<n):\n",
    "        b=all_[i]\n",
    "        k=0\n",
    "        while(k<n1):\n",
    "            a=pre_day[k]\n",
    "            ids.append(b)\n",
    "            days.append(a)\n",
    "            k=k+1\n",
    "        i=i+1\n",
    "    test_days = pd.DataFrame({'date':days,'key':ids})\n",
    "    test_comb=test_days.merge(test_data,on=['key'],how='outer')\n",
    "    test_comb=test_comb.fillna(0)\n",
    "    c_test=test_comb.merge(data_cluster,on='key',how='left')\n",
    "    check=c_test[['key','sold_out_date','clust_7']]\n",
    "    check=check.drop_duplicates()\n",
    "    check['sol_out_pred']=0\n",
    "    i=0\n",
    "    all_=c_test.key.unique()\n",
    "    sold_out=[]\n",
    "    n=len(all_)\n",
    "    while(i<n):\n",
    "        subset_w=c_test.loc[c_test['key']==all_[i]]\n",
    "        check_w=check.loc[check['key']==all_[i]]\n",
    "        stock=subset_w['stock'].values[0]\n",
    "        clust=subset_w['clust_7'].values[0]\n",
    "        n1=len(pre_day)\n",
    "        k=0\n",
    "        while(k<n1): #day based\n",
    "            day=pre_day[k]\n",
    "            that_day_sale=0\n",
    "            if(clust==0):\n",
    "                that_day_sale=cluster1_frame[day].values[0]\n",
    "            if(clust==1):\n",
    "                that_day_sale=cluster2_frame[day].values[0]  \n",
    "            if(clust==2):\n",
    "                that_day_sale=cluster3_frame[day].values[0]\n",
    "            if(clust==3):\n",
    "                that_day_sale=cluster4_frame[day].values[0]\n",
    "            if(clust==4):\n",
    "                that_day_sale=cluster5_frame[day].values[0]\n",
    "            if(clust=='nan'):\n",
    "                sold_out.append(date_ass)\n",
    "                break\n",
    "            stock=stock-that_day_sale\n",
    "            if((stock==0) | (stock<0) ):\n",
    "                sold_out.append(day)\n",
    "                break\n",
    "            if(k==(n1-1)):\n",
    "                sold_out.append(date_ass)\n",
    "            k=k+1\n",
    "        i=i+1\n",
    "        \n",
    "    check['sol_out_pred']=sold_out\n",
    "    check[\"date_sold_out_date\"] = pd.to_datetime(check[\"sold_out_date\"],format=\"%Y/%m/%d\")\n",
    "    check[\"date_sold_out_pred\"] = pd.to_datetime(check[\"sol_out_pred\"],format=\"%Y/%m/%d\")\n",
    "    days_differ=check[\"date_sold_out_date\"]-check[\"date_sold_out_pred\"]\n",
    "    \n",
    "    check['days_differ']=days_differ\n",
    "    check['correct_differ']=abs(check['days_differ'].astype(dt.timedelta).map(lambda x: np.nan if pd.isnull(x) else x.days))\n",
    "\n",
    "    cluster_size=data_cluster.clust_7.value_counts()\n",
    "    by_group=check.groupby(['clust_7']).sum()\n",
    "    by_group['clust_size']=cluster_size\n",
    "    by_group['mean']=by_group['correct_differ']/by_group['clust_size']\n",
    "    print(by_group)\n",
    "    \n",
    "    \n",
    "    days=pd.DataFrame(days_differ,columns=['days_differ'])\n",
    "    days['correct_differ']=abs(days['days_differ'].astype(dt.timedelta).map(lambda x: np.nan if pd.isnull(x) else x.days))\n",
    "    summ=sum(days['correct_differ'])\n",
    "    return(math.sqrt(summ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Windowing Lag(20) Regreesion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far this our best cluster and model. So here we are, running again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(806078, 3)\n",
      "8858\n"
     ]
    }
   ],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "cluster1_frame=days_20_cluster(c_dataframe,c_train,1,regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156520, 3)\n",
      "1720\n"
     ]
    }
   ],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "cluster2_frame=days_20_cluster(c_dataframe,c_train,2,regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25844, 3)\n",
      "284\n"
     ]
    }
   ],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "cluster3_frame=days_20_cluster(c_dataframe,c_train,3,regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1001, 3)\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "cluster4_frame=days_20_cluster(c_dataframe,c_train,4,regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8463, 3)\n",
      "93\n"
     ]
    }
   ],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "cluster5_frame=days_20_cluster(c_dataframe,c_train,5,regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3640, 3)\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "cluster6_frame=days_20_cluster(c_dataframe,c_train,6,regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(364, 3)\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "cluster7_frame=days_20_cluster(c_dataframe,c_train,7,regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pre_total = pd.concat([cluster1_frame,cluster2_frame,cluster3_frame,cluster4_frame,cluster5_frame,cluster6_frame,cluster7_frame])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pre_total=pre_total.reset_index().drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pre_total=pre_total.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pre_total.to_csv('Windowing_lag30_best_cluster.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         correct_differ  clust_size      mean\n",
      "clust_7                                      \n",
      "1.0               37577        4668  8.049914\n",
      "2.0               12941        1575  8.216508\n",
      "3.0                1879         281  6.686833\n",
      "4.0                  63          11  5.727273\n",
      "5.0                 566          93  6.086022\n",
      "6.0                 230          40  5.750000\n",
      "7.0                   9           4  2.250000\n",
      "8.0               10322        1470  7.021769\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "252.16462876462273"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data=pd.read_csv('test_0.csv') \n",
    "cluster_performance_measurement(test_data,cluster1_frame,cluster2_frame,cluster3_frame,cluster4_frame,cluster5_frame,cluster6_frame,cluster7_frame,data_cluster,'2018-01-22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-4a2e3ec703e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_1.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcluster_performance_measurement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcluster1_frame\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcluster2_frame\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcluster3_frame\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcluster4_frame\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcluster5_frame\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcluster6_frame\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcluster7_frame\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_cluster\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2018-01-22'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-091e9e7e1a09>\u001b[0m in \u001b[0;36mcluster_performance_measurement\u001b[0;34m(test_data, cluster1_frame, cluster2_frame, cluster3_frame, cluster4_frame, cluster5_frame, cluster6_frame, cluster7_frame, data_cluster, date_ass)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0msubset_w\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'key'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mall_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mcheck_w\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'key'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mall_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mstock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubset_w\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stock'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mclust\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubset_w\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clust_7'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bengikoseoglu/anaconda2/lib/python2.7/site-packages/pandas/core/ops.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, other, axis)\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mna_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    862\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m                 raise TypeError('Could not compare %s type with Series' %\n",
      "\u001b[0;32m/Users/bengikoseoglu/anaconda2/lib/python2.7/site-packages/pandas/core/ops.pyc\u001b[0m in \u001b[0;36mna_op\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_comp_method_OBJECT_ARRAY\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bengikoseoglu/anaconda2/lib/python2.7/site-packages/pandas/core/ops.pyc\u001b[0m in \u001b[0;36m_comp_method_OBJECT_ARRAY\u001b[0;34m(op, x, y)\u001b[0m\n\u001b[1;32m    743\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvec_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_data=pd.read_csv('test_1.csv') \n",
    "cluster_performance_measurement(test_data,cluster1_frame,cluster2_frame,cluster3_frame,cluster4_frame,cluster5_frame,cluster6_frame,cluster7_frame,data_cluster,'2018-01-22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data=pd.read_csv('test_2.csv') \n",
    "cluster_performance_measurement(test_data,cluster1_frame,cluster2_frame,cluster3_frame,cluster4_frame,cluster5_frame,cluster6_frame,cluster7_frame,data_cluster,'2018-01-22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data=pd.read_csv('test_3.csv') \n",
    "cluster_performance_measurement(test_data,cluster1_frame,cluster2_frame,cluster3_frame,cluster4_frame,cluster5_frame,cluster6_frame,cluster7_frame,data_cluster,'2018-01-22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data=pd.read_csv('test_4.csv') \n",
    "cluster_performance_measurement(test_data,cluster1_frame,cluster2_frame,cluster3_frame,cluster4_frame,cluster5_frame,cluster6_frame,cluster7_frame,data_cluster,'2018-01-22')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Windowing Lag(20) Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(806078, 3)\n",
      "8858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bengikoseoglu/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:19: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    }
   ],
   "source": [
    "clf=RandomForestRegressor(max_features='sqrt', min_samples_split=2, n_estimators=10, min_samples_leaf= 4)\n",
    "cluster1_frame=days_20_cluster(c_dataframe,c_train,1,clf)#c_dataframe,c_train,6,regr,True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156520, 3)\n",
      "1720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bengikoseoglu/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:19: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    }
   ],
   "source": [
    "clf=RandomForestRegressor(max_features='sqrt', min_samples_split=2, n_estimators=10, min_samples_leaf= 4)\n",
    "cluster2_frame=days_20_cluster(c_dataframe,c_train,2,clf)#c_dataframe,c_train,6,regr,True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25844, 3)\n",
      "284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bengikoseoglu/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:19: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    }
   ],
   "source": [
    "clf=RandomForestRegressor(max_features='sqrt', min_samples_split=2, n_estimators=10, min_samples_leaf= 4)\n",
    "cluster3_frame=days_20_cluster(c_dataframe,c_train,3,clf)#c_dataframe,c_train,6,regr,True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1001, 3)\n",
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bengikoseoglu/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:19: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    }
   ],
   "source": [
    "clf=RandomForestRegressor(max_features='sqrt', min_samples_split=2, n_estimators=10, min_samples_leaf= 4)\n",
    "cluster4_frame=days_20_cluster(c_dataframe,c_train,4,clf)#c_dataframe,c_train,6,regr,True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8463, 3)\n",
      "93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bengikoseoglu/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:19: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    }
   ],
   "source": [
    "clf=RandomForestRegressor(max_features='sqrt', min_samples_split=2, n_estimators=10, min_samples_leaf= 4)\n",
    "cluster5_frame=days_20_cluster(c_dataframe,c_train,5,clf)#c_dataframe,c_train,6,regr,True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3640, 3)\n",
      "40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bengikoseoglu/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:19: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    }
   ],
   "source": [
    "clf=RandomForestRegressor(max_features='sqrt', min_samples_split=2, n_estimators=10, min_samples_leaf= 4)\n",
    "cluster6_frame=days_20_cluster(c_dataframe,c_train,6,clf)#c_dataframe,c_train,6,regr,True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(364, 3)\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bengikoseoglu/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:19: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    }
   ],
   "source": [
    "clf=RandomForestRegressor(max_features='sqrt', min_samples_split=2, n_estimators=10, min_samples_leaf= 4)\n",
    "cluster7_frame=days_20_cluster(c_dataframe,c_train,7,clf)#c_dataframe,c_train,6,regr,True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         correct_differ  clust_size      mean\n",
      "clust_7                                      \n",
      "1.0               38304        4668  8.205656\n",
      "2.0               12505        1575  7.939683\n",
      "3.0                1908         281  6.790036\n",
      "4.0                  66          11  6.000000\n",
      "5.0                 570          93  6.129032\n",
      "6.0                 240          40  6.000000\n",
      "7.0                   9           4  2.250000\n",
      "8.0               10322        1470  7.021769\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "252.8319600050595"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data=pd.read_csv('test_0.csv') \n",
    "cluster_performance_measurement(test_data,cluster1_frame,cluster2_frame,cluster3_frame,cluster4_frame,cluster5_frame,cluster6_frame,cluster7_frame,data_cluster,'2018-01-22')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Windowing Lag(20) ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(806078, 3)\n",
      "8858\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPRegressor()\n",
    "cluster1_frame=days_20_cluster(c_dataframe,c_train,1,mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156520, 3)\n",
      "1720\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPRegressor()\n",
    "cluster2_frame=days_20_cluster(c_dataframe,c_train,2,mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25844, 3)\n",
      "284\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPRegressor()\n",
    "cluster3_frame=days_20_cluster(c_dataframe,c_train,3,mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1001, 3)\n",
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bengikoseoglu/anaconda2/lib/python2.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPRegressor()\n",
    "cluster4_frame=days_20_cluster(c_dataframe,c_train,4,mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8463, 3)\n",
      "93\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPRegressor()\n",
    "cluster4_frame=days_20_cluster(c_dataframe,c_train,5,mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3640, 3)\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPRegressor()\n",
    "cluster6_frame=days_20_cluster(c_dataframe,c_train,6,mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         correct_differ  clust_size      mean\n",
      "clust_7                                      \n",
      "1.0               36895        4668  7.903813\n",
      "2.0               12985        1575  8.244444\n",
      "3.0                1771         281  6.302491\n",
      "4.0                  39          11  3.545455\n",
      "5.0                 566          93  6.086022\n",
      "6.0                 232          40  5.800000\n",
      "7.0                   9           4  2.250000\n",
      "8.0               10322        1470  7.021769\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "250.63718798294877"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data=pd.read_csv('test_0.csv') \n",
    "cluster_performance_measurement(test_data,cluster1_frame,cluster2_frame,cluster3_frame,cluster4_frame,cluster5_frame,cluster6_frame,cluster7_frame,data_cluster,'2018-01-22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         correct_differ  clust_size      mean\n",
      "clust_7                                      \n",
      "1.0               37080        4668  7.943445\n",
      "2.0               13147        1575  8.347302\n",
      "3.0                1717         281  6.110320\n",
      "4.0                  36          11  3.272727\n",
      "5.0                 497          93  5.344086\n",
      "6.0                 225          40  5.625000\n",
      "7.0                  17           4  4.250000\n",
      "8.0               10299        1470  7.006122\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "251.03386225766437"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data=pd.read_csv('test_1.csv') \n",
    "cluster_performance_measurement(test_data,cluster1_frame,cluster2_frame,cluster3_frame,cluster4_frame,cluster5_frame,cluster6_frame,cluster7_frame,data_cluster,'2018-01-22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         correct_differ  clust_size      mean\n",
      "clust_7                                      \n",
      "1.0               37093        4668  7.946230\n",
      "2.0               13152        1575  8.350476\n",
      "3.0                1782         281  6.341637\n",
      "4.0                  42          11  3.818182\n",
      "5.0                 520          93  5.591398\n",
      "6.0                 225          40  5.625000\n",
      "7.0                  19           4  4.750000\n",
      "8.0               10273        1470  6.988435\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "251.20907626915076"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data=pd.read_csv('test_2.csv') \n",
    "cluster_performance_measurement(test_data,cluster1_frame,cluster2_frame,cluster3_frame,cluster4_frame,cluster5_frame,cluster6_frame,cluster7_frame,data_cluster,'2018-01-22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         correct_differ  clust_size      mean\n",
      "clust_7                                      \n",
      "1.0               36686        4668  7.859040\n",
      "2.0               13220        1575  8.393651\n",
      "3.0                1751         281  6.231317\n",
      "4.0                  50          11  4.545455\n",
      "5.0                 481          93  5.172043\n",
      "6.0                 230          40  5.750000\n",
      "7.0                  14           4  3.500000\n",
      "8.0               10214        1470  6.948299\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "250.2918296708864"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data=pd.read_csv('test_3.csv') \n",
    "cluster_performance_measurement(test_data,cluster1_frame,cluster2_frame,cluster3_frame,cluster4_frame,cluster5_frame,cluster6_frame,cluster7_frame,data_cluster,'2018-01-22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         correct_differ  clust_size      mean\n",
      "clust_7                                      \n",
      "1.0               37015        4668  7.929520\n",
      "2.0               13287        1575  8.436190\n",
      "3.0                1719         281  6.117438\n",
      "4.0                  33          11  3.000000\n",
      "5.0                 554          93  5.956989\n",
      "6.0                 219          40  5.475000\n",
      "7.0                  21           4  5.250000\n",
      "8.0               10187        1470  6.929932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "251.06771994822432"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data=pd.read_csv('test_4.csv') \n",
    "cluster_performance_measurement(test_data,cluster1_frame,cluster2_frame,cluster3_frame,cluster4_frame,cluster5_frame,cluster6_frame,cluster7_frame,data_cluster,'2018-01-22')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Windowing Lag(20) Gradient Boosting Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "params = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 2,\n",
    "              'learning_rate': 0.01, 'loss': 'ls'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(806078, 3)\n",
      "8858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bengikoseoglu/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "clf = ensemble.GradientBoostingRegressor(**params)\n",
    "cluster1_frame=days_20_cluster(c_dataframe,c_train,1,clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156520, 3)\n",
      "1720\n"
     ]
    }
   ],
   "source": [
    "clf = ensemble.GradientBoostingRegressor(**params)\n",
    "cluster2_frame=days_20_cluster(c_dataframe,c_train,2,clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25844, 3)\n",
      "284\n"
     ]
    }
   ],
   "source": [
    "clf = ensemble.GradientBoostingRegressor(**params)\n",
    "cluster3_frame=days_20_cluster(c_dataframe,c_train,3,clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1001, 3)\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "clf = ensemble.GradientBoostingRegressor(**params)\n",
    "cluster4_frame=days_20_cluster(c_dataframe,c_train,4,clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8463, 3)\n",
      "93\n"
     ]
    }
   ],
   "source": [
    "clf = ensemble.GradientBoostingRegressor(**params)\n",
    "cluster5_frame=days_20_cluster(c_dataframe,c_train,5,clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3640, 3)\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "clf = ensemble.GradientBoostingRegressor(**params)\n",
    "cluster6_frame=days_20_cluster(c_dataframe,c_train,6,clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(364, 3)\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "clf = ensemble.GradientBoostingRegressor(**params)\n",
    "cluster7_frame=days_20_cluster(c_dataframe,c_train,7,clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         correct_differ  clust_size      mean\n",
      "clust_7                                      \n",
      "1.0               40188        4668  8.609254\n",
      "2.0               12706        1575  8.067302\n",
      "3.0                1844         281  6.562278\n",
      "4.0                  80          11  7.272727\n",
      "5.0                 591          93  6.354839\n",
      "6.0                 242          40  6.050000\n",
      "7.0                  12           4  3.000000\n",
      "8.0               10322        1470  7.021769\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "256.87545620397447"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data=pd.read_csv('test_0.csv') \n",
    "cluster_performance_measurement(test_data,cluster1_frame,cluster2_frame,cluster3_frame,cluster4_frame,cluster5_frame,cluster6_frame,cluster7_frame,data_cluster,'2018-01-22')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
