{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "data1=pd.read_csv('integrated_data.csv')\n",
    "cluster_time=pd.read_csv('time_cluster_bg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# assign training data with cluster label\n",
    "data=pd.merge(data1,cluster_time[['key','clust_7']],on='key',how='left')\n",
    "data=data.fillna(8) # assign cluster 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split train & test\n",
    "df_train_data=data[data.month !=1]\n",
    "df_test_data=data[data.month ==1 ] \n",
    "df_test_data=df_test_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cluster(train,test,label):\n",
    "    df_train_data_x=train[train.clust_7 ==label]\n",
    "    df_test_data_x=test[test.clust_7 ==label] \n",
    "    \n",
    "    target_train_x = df_train_data_x[['units']]\n",
    "    data_train_x = df_train_data_x.drop(['Unnamed: 0','units','key','date','rrp','weekday','month','day','clust_7','sum_unit','median_temp'], axis=1)\n",
    "    target_test_x = df_test_data_x[['units']]\n",
    "    data_test_x =df_test_data_x.drop(['Unnamed: 0','units','key','date','rrp','weekday','month','day','clust_7','sum_unit','median_temp'], axis=1)\n",
    "    \n",
    "    return target_train_x,data_train_x,target_test_x,data_test_x,df_test_data_x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X0=cluster(df_train_data,df_test_data,1)\n",
    "X1=cluster(df_train_data,df_test_data,2)\n",
    "X2=cluster(df_train_data,df_test_data,3)\n",
    "X3=cluster(df_train_data,df_test_data,4)\n",
    "X4=cluster(df_train_data,df_test_data,5)\n",
    "X5=cluster(df_train_data,df_test_data,6)\n",
    "X6=cluster(df_train_data,df_test_data,7)\n",
    "X7=cluster(df_train_data,df_test_data,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(165074, 1)\n",
      "(165074, 25)\n",
      "(56234, 1)\n",
      "(56234, 25)\n",
      "(56234, 36)\n"
     ]
    }
   ],
   "source": [
    "print(X7[0].shape)\n",
    "print(X7[1].shape)\n",
    "print(X7[2].shape)\n",
    "print(X7[3].shape)\n",
    "print(X7[4].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "\n",
    "params0 = {'n_estimators': 1000, 'max_depth': 3, 'min_samples_split': 2,\n",
    "              'learning_rate': 0.05, 'loss': 'ls'}\n",
    "clf0 = ensemble.GradientBoostingRegressor(**params0)\n",
    "clf0.fit(X0[1], X0[0]['units'].ravel()) \n",
    "prediction0= clf0.predict(X0[3])\n",
    "    \n",
    "df_prediction0=pd.DataFrame(prediction0, columns=['pre_units']).round(decimals=5)\n",
    "df_prediction_need0=X0[4][['key','day','units']]\n",
    "df_prediction_need0=df_prediction_need0.reset_index(drop=True)\n",
    "pre_cluster0=df_prediction_need0.merge(df_prediction0, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params1 = {'n_estimators': 1000, 'max_depth': 3, 'min_samples_split': 2,\n",
    "              'learning_rate': 0.05, 'loss': 'ls'}\n",
    "clf1 = ensemble.GradientBoostingRegressor(**params1)\n",
    "clf1.fit(X1[1], X1[0]['units'].ravel()) \n",
    "prediction1= clf1.predict(X1[3])\n",
    "    \n",
    "df_prediction1=pd.DataFrame(prediction1, columns=['pre_units']).round(decimals=5)\n",
    "df_prediction_need1=X1[4][['key','day','units']]\n",
    "df_prediction_need1=df_prediction_need1.reset_index(drop=True)\n",
    "pre_cluster1=df_prediction_need1.merge(df_prediction1, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params2 = {'learning_rate': 0.05, 'max_depth': 3, 'min_samples_split': 2, 'n_estimators': 500, 'loss': 'ls'}\n",
    "clf2 = ensemble.GradientBoostingRegressor(**params2)\n",
    "clf2.fit(X2[1], X2[0]['units'].ravel()) \n",
    "prediction2= clf2.predict(X2[3])\n",
    "    \n",
    "df_prediction2=pd.DataFrame(prediction2, columns=['pre_units']).round(decimals=5)\n",
    "df_prediction_need2=X2[4][['key','day','units']]\n",
    "df_prediction_need2=df_prediction_need2.reset_index(drop=True)\n",
    "pre_cluster2=df_prediction_need2.merge(df_prediction2, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params3 = {'learning_rate': 0.05, 'max_depth': 3, 'min_samples_split': 10, 'n_estimators': 1000, 'loss': 'ls'}\n",
    "clf3 = ensemble.GradientBoostingRegressor(**params3)\n",
    "clf3.fit(X3[1], X3[0]['units'].ravel()) \n",
    "prediction3= clf3.predict(X3[3])\n",
    "    \n",
    "df_prediction3=pd.DataFrame(prediction3, columns=['pre_units']).round(decimals=5)\n",
    "df_prediction_need3=X3[4][['key','day','units']]\n",
    "df_prediction_need3=df_prediction_need3.reset_index(drop=True)\n",
    "pre_cluster3=df_prediction_need3.merge(df_prediction3, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params4 = {'learning_rate': 0.01, 'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 100, 'loss': 'ls'}\n",
    "clf4 = ensemble.GradientBoostingRegressor(**params4)\n",
    "clf4.fit(X4[1], X4[0]['units'].ravel()) \n",
    "prediction4= clf4.predict(X4[3])\n",
    "    \n",
    "df_prediction4=pd.DataFrame(prediction4, columns=['pre_units']).round(decimals=5)\n",
    "df_prediction_need4=X4[4][['key','day','units']]\n",
    "df_prediction_need4=df_prediction_need4.reset_index(drop=True)\n",
    "pre_cluster4=df_prediction_need4.merge(df_prediction4, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params5 = {'learning_rate': 0.05, 'max_depth': 3, 'min_samples_split': 2, 'n_estimators': 100, 'loss': 'ls'}\n",
    "clf5 = ensemble.GradientBoostingRegressor(**params5)\n",
    "clf5.fit(X5[1], X5[0]['units'].ravel()) \n",
    "prediction5= clf5.predict(X5[3])\n",
    "    \n",
    "df_prediction5=pd.DataFrame(prediction5, columns=['pre_units']).round(decimals=5)\n",
    "df_prediction_need5=X5[4][['key','day','units']]\n",
    "df_prediction_need5=df_prediction_need5.reset_index(drop=True)\n",
    "pre_cluster5=df_prediction_need5.merge(df_prediction5, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params6 = {'learning_rate': 0.01, 'max_depth': 15, 'min_samples_split': 10, 'n_estimators': 100, 'loss': 'ls'}\n",
    "clf6 = ensemble.GradientBoostingRegressor(**params6)\n",
    "clf6.fit(X6[1], X6[0]['units'].ravel()) \n",
    "prediction6= clf6.predict(X6[3])\n",
    "    \n",
    "df_prediction6=pd.DataFrame(prediction6, columns=['pre_units']).round(decimals=5)\n",
    "df_prediction_need6=X6[4][['key','day','units']]\n",
    "df_prediction_need6=df_prediction_need6.reset_index(drop=True)\n",
    "pre_cluster6=df_prediction_need6.merge(df_prediction6, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params7 = {'n_estimators': 1000, 'max_depth': 3, 'min_samples_split': 2,\n",
    "              'learning_rate': 0.05, 'loss': 'ls'}\n",
    "clf7 = ensemble.GradientBoostingRegressor(**params7)\n",
    "clf7.fit(X7[1], X7[0]['units'].ravel()) \n",
    "prediction7= clf7.predict(X7[3])\n",
    "    \n",
    "df_prediction7=pd.DataFrame(prediction7, columns=['pre_units']).round(decimals=5)\n",
    "df_prediction_need7=X7[4][['key','day','units']]\n",
    "df_prediction_need7=df_prediction_need7.reset_index(drop=True)\n",
    "pre_cluster7=df_prediction_need7.merge(df_prediction7, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrate prediciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pre_total = pd.concat([pre_cluster0,pre_cluster1,pre_cluster2,pre_cluster3,pre_cluster4,pre_cluster5,pre_cluster6,pre_cluster7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def predict_date(pre_total):\n",
    "    date=[]\n",
    "    i=0\n",
    "    n=len(pre_total)\n",
    "    while i<n:\n",
    "        day_int=pre_total.iloc[i]['day']\n",
    "        date_time=datetime.datetime(2018, 1, day_int, 0,0).date()\n",
    "        date_string=str(date_time)\n",
    "        date.append(date_string)\n",
    "        i+=1\n",
    "    pre_total['date']=date\n",
    "\n",
    "    pre_total_pvt = pre_total.pivot_table('pre_units', ['key'], 'date')\n",
    "    pre_total_pvt['key'] = pre_total_pvt.index\n",
    "    pre_total_pvt=pre_total_pvt.reset_index(drop=True)\n",
    "\n",
    "    return pre_total_pvt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pre_total_pvt    = predict_date(pre_total)\n",
    "pre_cluster0_pvt = predict_date(pre_cluster0)\n",
    "pre_cluster1_pvt = predict_date(pre_cluster1)\n",
    "pre_cluster2_pvt = predict_date(pre_cluster2)\n",
    "pre_cluster3_pvt = predict_date(pre_cluster3)\n",
    "pre_cluster4_pvt = predict_date(pre_cluster4)\n",
    "pre_cluster5_pvt = predict_date(pre_cluster5)\n",
    "pre_cluster6_pvt = predict_date(pre_cluster6)\n",
    "pre_cluster7_pvt = predict_date(pre_cluster7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# measure sold_out_day deviation with actual\n",
    "from datetime import date, timedelta\n",
    "import math\n",
    "\n",
    "def measurement(prediction, test_data):\n",
    "\n",
    "    d1 = date(2018, 1, 1)  \n",
    "    d2 = date(2018, 1, 31)  \n",
    "    delta = d2 - d1         \n",
    "    pre_day=[]\n",
    "    for i in range(delta.days + 1):\n",
    "        add=str(d1 + timedelta(days=i))\n",
    "        pre_day.append(add)\n",
    "\n",
    "    sold_out_day=[]\n",
    "    key=[]\n",
    "    #lets innet join with testdataset\n",
    "    window_inner=test_data.merge(prediction,left_on='key', right_on='key', how='inner')\n",
    "    window_inner['sol_out_pred']=0\n",
    "\n",
    "    all_=window_inner.key.unique()\n",
    "    n=len(all_) \n",
    "    i=0\n",
    "    #print (n)\n",
    "\n",
    "    while(i<n): #dataset\n",
    "        subset_w=prediction.loc[prediction['key']==all_[i]]\n",
    "        stock=test_data.loc[test_data['key']==all_[i]]['stock'].values[0]\n",
    "        n1=len(pre_day)\n",
    "        k=0\n",
    "        while(k<n1): #day calculation\n",
    "            day=pre_day[k]\n",
    "            that_day_sale=subset_w[day].values[0]\n",
    "            stock=stock-that_day_sale\n",
    "            if((stock==0) | (stock<0) ):\n",
    "                window_inner.iloc[i,37]=day\n",
    "                break\n",
    "            if(k==(n1-1)): \n",
    "                window_inner.iloc[i,37]='2018-01-22'\n",
    "            k=k+1\n",
    "        i=i+1\n",
    "\n",
    "    test_data['key']=test_data[\"pid\"].astype(str) +\" \"+ test_data[\"size\"].astype(str)\n",
    "    window_inner[\"date_sold_out_date\"] = pd.to_datetime(window_inner[\"sold_out_date\"],format=\"%Y/%m/%d\")\n",
    "    window_inner[\"date_sold_out_pred\"] = pd.to_datetime(window_inner[\"sol_out_pred\"],format=\"%Y/%m/%d\")\n",
    "\n",
    "    days_differ=window_inner[\"date_sold_out_date\"]-window_inner[\"date_sold_out_pred\"]\n",
    "    days=pd.DataFrame(days_differ,columns=['days_differ'])\n",
    "    days['correct_differ']=abs(days['days_differ'].astype(datetime.timedelta).map(lambda x: np.nan if pd.isnull(x) else x.days))\n",
    "\n",
    "    error=math.sqrt(sum(days['correct_differ']))\n",
    "\n",
    "    return error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error(test_table,total,cluster0,cluster1,cluster2,cluster3,cluster4,cluster5,cluster6,cluster7):\n",
    "    \n",
    "    test_data_0=pd.read_csv(test_table)\n",
    "    test_data_0['pid']=test_data_0['pid'].astype(int)\n",
    "    test_data_0['key']=test_data_0[\"pid\"].astype(str) +\" \"+ test_data_0[\"size\"].astype(str)\n",
    "\n",
    "    error_0=measurement(cluster0, test_data_0)\n",
    "    error_1=measurement(cluster1, test_data_0)\n",
    "    error_2=measurement(cluster2, test_data_0)\n",
    "    error_3=measurement(cluster3, test_data_0)\n",
    "    error_4=measurement(cluster4, test_data_0)\n",
    "    error_5=measurement(cluster5, test_data_0)\n",
    "    error_6=measurement(cluster6, test_data_0)\n",
    "    error_7=measurement(cluster7, test_data_0)\n",
    "    error_total=measurement(total, test_data_0)\n",
    "\n",
    "    print('cluster0',error_0,'\\ncluster1',error_1, '\\ncluster2',error_2,'\\ncluster3',error_3, '\\ncluster4',error_4,\n",
    "          '\\ncluster5',error_5, '\\ncluster6',error_6,'\\ncluster7',error_7,'\\ntotoal',error_total)\n",
    "    \n",
    "    return error_0,error_1,error_2,error_3,error_4,error_5,error_6,error_7,error_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster0 200.0199990001 \n",
      "cluster1 106.99065379742288 \n",
      "cluster2 41.32795663954365 \n",
      "cluster3 7.0 \n",
      "cluster4 22.64950330581225 \n",
      "cluster5 15.588457268119896 \n",
      "cluster6 3.3166247903554 \n",
      "cluster7 101.59724405711013 \n",
      "totoal 253.57641846196975\n"
     ]
    }
   ],
   "source": [
    "error_0=error('test_0.csv',pre_total_pvt,pre_cluster0_pvt,pre_cluster1_pvt,pre_cluster2_pvt,pre_cluster3_pvt,pre_cluster4_pvt,pre_cluster5_pvt,pre_cluster6_pvt,pre_cluster7_pvt,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster0 201.47704583897394 \n",
      "cluster1 107.97221864905805 \n",
      "cluster2 41.13392760240627 \n",
      "cluster3 6.855654600401044 \n",
      "cluster4 21.236760581595302 \n",
      "cluster5 14.89966442575134 \n",
      "cluster6 4.123105625617661 \n",
      "cluster7 101.48398888494677 \n",
      "totoal 254.9097879642914\n"
     ]
    }
   ],
   "source": [
    "error_1=error('test_1.csv',pre_total_pvt,pre_cluster0_pvt,pre_cluster1_pvt,pre_cluster2_pvt,pre_cluster3_pvt,pre_cluster4_pvt,pre_cluster5_pvt,pre_cluster6_pvt,pre_cluster7_pvt,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Test_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster0 201.99009876724156 \n",
      "cluster1 108.21275340735028 \n",
      "cluster2 42.1070065428546 \n",
      "cluster3 7.0 \n",
      "cluster4 21.6794833886788 \n",
      "cluster5 14.352700094407323 \n",
      "cluster6 4.47213595499958 \n",
      "cluster7 101.35580891098448 \n",
      "totoal 255.54060342732228\n"
     ]
    }
   ],
   "source": [
    "error_2=error('test_2.csv',pre_total_pvt,pre_cluster0_pvt,pre_cluster1_pvt,pre_cluster2_pvt,pre_cluster3_pvt,pre_cluster4_pvt,pre_cluster5_pvt,pre_cluster6_pvt,pre_cluster7_pvt,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster0 201.14919835783587 \n",
      "cluster1 108.12955192730617 \n",
      "cluster2 41.557189510360296 \n",
      "cluster3 7.3484692283495345 \n",
      "cluster4 20.952326839756964 \n",
      "cluster5 15.329709716755891 \n",
      "cluster6 3.7416573867739413 \n",
      "cluster7 101.06433594498111 \n",
      "totoal 254.62914208707534\n"
     ]
    }
   ],
   "source": [
    "error_3=error('test_3.csv',pre_total_pvt,pre_cluster0_pvt,pre_cluster1_pvt,pre_cluster2_pvt,pre_cluster3_pvt,pre_cluster4_pvt,pre_cluster5_pvt,pre_cluster6_pvt,pre_cluster7_pvt,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster0 200.9651711118123 \n",
      "cluster1 108.12030336620407 \n",
      "cluster2 40.792156108742276 \n",
      "cluster3 6.244997998398398 \n",
      "cluster4 22.516660498395403 \n",
      "cluster5 15.066519173319364 \n",
      "cluster6 4.123105625617661 \n",
      "cluster7 100.93066927351666 \n",
      "totoal 254.39732702998276\n"
     ]
    }
   ],
   "source": [
    "error_4=error('test_4.csv',pre_total_pvt,pre_cluster0_pvt,pre_cluster1_pvt,pre_cluster2_pvt,pre_cluster3_pvt,pre_cluster4_pvt,pre_cluster5_pvt,pre_cluster6_pvt,pre_cluster7_pvt,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster0 201.120302615 \n",
      "cluster1 107.885096229 \n",
      "cluster2 41.3836472808 \n",
      "cluster3 6.88982436543 \n",
      "cluster4 21.8069469228 \n",
      "cluster5 15.0474101357 \n",
      "cluster6 3.95532587667 \n",
      "cluster7 101.286409414 \n",
      "totoal 254.610655794\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "c0=[error_0[0],error_1[0],error_2[0],error_3[0],error_4[0]]\n",
    "c1=[error_0[1],error_1[1],error_2[1],error_3[1],error_4[1]]\n",
    "c2=[error_0[2],error_1[2],error_2[2],error_3[2],error_4[2]]\n",
    "c3=[error_0[3],error_1[3],error_2[3],error_3[3],error_4[3]]\n",
    "c4=[error_0[4],error_1[4],error_2[4],error_3[4],error_4[4]]\n",
    "c5=[error_0[5],error_1[5],error_2[5],error_3[5],error_4[5]]\n",
    "c6=[error_0[6],error_1[6],error_2[6],error_3[6],error_4[6]]\n",
    "c7=[error_0[7],error_1[7],error_2[7],error_3[7],error_4[7]]\n",
    "total=[error_0[8],error_1[8],error_2[8],error_3[8],error_4[8]]\n",
    "\n",
    "print('cluster0',numpy.mean(c0),'\\ncluster1',numpy.mean(c1), '\\ncluster2',numpy.mean(c2),'\\ncluster3',numpy.mean(c3), '\\ncluster4',numpy.mean(c4),\n",
    "           '\\ncluster5',numpy.mean(c5), '\\ncluster6',numpy.mean(c6),'\\ncluster7',numpy.mean(c7),'\\ntotoal',numpy.mean(total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
