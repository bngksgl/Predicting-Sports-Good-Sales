{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "data1=pd.read_csv('integrated_data.csv')\n",
    "cluster_time=pd.read_csv('time_cluster_bg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assign training data with cluster label\n",
    "data=pd.merge(data1,cluster_time[['key','clust_7']],on='key',how='left')\n",
    "data=data.fillna(8) # assign cluster 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split train & test\n",
    "df_train_data=data[data.month !=1]\n",
    "df_test_data=data[data.month ==1 ] \n",
    "df_test_data=df_test_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cluster(train,test,label):\n",
    "    df_train_data_x=train[train.clust_7 ==label]\n",
    "    df_test_data_x=test[test.clust_7 ==label] \n",
    "    \n",
    "    target_train_x = df_train_data_x[['units']]\n",
    "    data_train_x = df_train_data_x.drop(['Unnamed: 0','units','key','date','rrp','weekday','month','day','clust_7','sum_unit'], axis=1)\n",
    "    target_test_x = df_test_data_x[['units']]\n",
    "    data_test_x =df_test_data_x.drop(['Unnamed: 0','units','key','date','rrp','weekday','month','day','clust_7','sum_unit'], axis=1)\n",
    "    \n",
    "    return target_train_x,data_train_x,target_test_x,data_test_x,df_test_data_x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X0=cluster(df_train_data,df_test_data,1)\n",
    "X1=cluster(df_train_data,df_test_data,2)\n",
    "X2=cluster(df_train_data,df_test_data,3)\n",
    "X3=cluster(df_train_data,df_test_data,4)\n",
    "X4=cluster(df_train_data,df_test_data,5)\n",
    "X5=cluster(df_train_data,df_test_data,6)\n",
    "X6=cluster(df_train_data,df_test_data,7)\n",
    "X7=cluster(df_train_data,df_test_data,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 0-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GONGNA\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "def XGBoostingRegressor0(data_train_X, target_train_y, data_test_X, df_test_data_X):\n",
    "    model = XGBRegressor(colsample_bytree=0.8,learning_rate=0.03,n_estimators=1000,max_depth=6,min_child_weight=7,objective='reg:linear',subsample=0.7)\n",
    "    model.fit(data_train_X, target_train_y['units'].ravel())\n",
    "    # make predictions for test data\n",
    "    prediction = model.predict(data_test_X)\n",
    "    #predictions = [round(value) for value in y_pred]\n",
    "    df_prediction=pd.DataFrame(prediction, columns=['pre_units']).round(decimals=5)\n",
    "    df_prediction_need=df_test_data_X[['key','day','units']]\n",
    "    df_prediction_need=df_prediction_need.reset_index(drop=True)\n",
    "    df_prediction2=df_prediction_need.merge(df_prediction, left_index=True, right_index=True)\n",
    "    return df_prediction2\n",
    "\n",
    "def XGBoostingRegressor1(data_train_X, target_train_y, data_test_X, df_test_data_X):\n",
    "    model = XGBRegressor(colsample_bytree=0.7,learning_rate=0.03,n_estimators=1000,max_depth=6,min_child_weight=7,objective='reg:linear',subsample=0.7)\n",
    "    model.fit(data_train_X, target_train_y['units'].ravel())\n",
    "    # make predictions for test data\n",
    "    prediction = model.predict(data_test_X)\n",
    "    #predictions = [round(value) for value in y_pred]\n",
    "    df_prediction=pd.DataFrame(prediction, columns=['pre_units']).round(decimals=5)\n",
    "    df_prediction_need=df_test_data_X[['key','day','units']]\n",
    "    df_prediction_need=df_prediction_need.reset_index(drop=True)\n",
    "    df_prediction2=df_prediction_need.merge(df_prediction, left_index=True, right_index=True)\n",
    "    return df_prediction2\n",
    "\n",
    "def XGBoostingRegressor2(data_train_X, target_train_y, data_test_X, df_test_data_X):\n",
    "    model = XGBRegressor(colsample_bytree=0.7,learning_rate=0.03,n_estimators=1000,max_depth=6,min_child_weight=7,objective='reg:linear',subsample=0.8)\n",
    "    model.fit(data_train_X, target_train_y['units'].ravel())\n",
    "    # make predictions for test data\n",
    "    prediction = model.predict(data_test_X)\n",
    "    #predictions = [round(value) for value in y_pred]\n",
    "    df_prediction=pd.DataFrame(prediction, columns=['pre_units']).round(decimals=5)\n",
    "    df_prediction_need=df_test_data_X[['key','day','units']]\n",
    "    df_prediction_need=df_prediction_need.reset_index(drop=True)\n",
    "    df_prediction2=df_prediction_need.merge(df_prediction, left_index=True, right_index=True)\n",
    "    return df_prediction2\n",
    "\n",
    "def XGBoostingRegressor3(data_train_X, target_train_y, data_test_X, df_test_data_X):\n",
    "    model = XGBRegressor(colsample_bytree=0.8,learning_rate=0.03,n_estimators=1000,max_depth=7,min_child_weight=6,objective='reg:linear',subsample=0.8)\n",
    "    model.fit(data_train_X, target_train_y['units'].ravel())\n",
    "    # make predictions for test data\n",
    "    prediction = model.predict(data_test_X)\n",
    "    #predictions = [round(value) for value in y_pred]\n",
    "    df_prediction=pd.DataFrame(prediction, columns=['pre_units']).round(decimals=5)\n",
    "    df_prediction_need=df_test_data_X[['key','day','units']]\n",
    "    df_prediction_need=df_prediction_need.reset_index(drop=True)\n",
    "    df_prediction2=df_prediction_need.merge(df_prediction, left_index=True, right_index=True)\n",
    "    return df_prediction2\n",
    "\n",
    "def XGBoostingRegressor4(data_train_X, target_train_y, data_test_X, df_test_data_X):\n",
    "    model = XGBRegressor(colsample_bytree=0.7,learning_rate=0.03,n_estimators=1000,max_depth=6,min_child_weight=7,objective='reg:linear',subsample=0.8)\n",
    "    model.fit(data_train_X, target_train_y['units'].ravel())\n",
    "    # make predictions for test data\n",
    "    prediction = model.predict(data_test_X)\n",
    "    #predictions = [round(value) for value in y_pred]\n",
    "    df_prediction=pd.DataFrame(prediction, columns=['pre_units']).round(decimals=5)\n",
    "    df_prediction_need=df_test_data_X[['key','day','units']]\n",
    "    df_prediction_need=df_prediction_need.reset_index(drop=True)\n",
    "    df_prediction2=df_prediction_need.merge(df_prediction, left_index=True, right_index=True)\n",
    "    return df_prediction2\n",
    "\n",
    "def XGBoostingRegressor5(data_train_X, target_train_y, data_test_X, df_test_data_X):\n",
    "    model = XGBRegressor(colsample_bytree=0.8,learning_rate=0.03,n_estimators=1000,max_depth=6,min_child_weight=7,objective='reg:linear',subsample=0.7)\n",
    "    model.fit(data_train_X, target_train_y['units'].ravel())\n",
    "    # make predictions for test data\n",
    "    prediction = model.predict(data_test_X)\n",
    "    #predictions = [round(value) for value in y_pred]\n",
    "    df_prediction=pd.DataFrame(prediction, columns=['pre_units']).round(decimals=5)\n",
    "    df_prediction_need=df_test_data_X[['key','day','units']]\n",
    "    df_prediction_need=df_prediction_need.reset_index(drop=True)\n",
    "    df_prediction2=df_prediction_need.merge(df_prediction, left_index=True, right_index=True)\n",
    "    return df_prediction2\n",
    "\n",
    "def XGBoostingRegressor6(data_train_X, target_train_y, data_test_X, df_test_data_X):\n",
    "    model = XGBRegressor(colsample_bytree=0.8,learning_rate=0.03,n_estimators=1000,max_depth=9,min_child_weight=6,objective='reg:linear',subsample=0.7)\n",
    "    model.fit(data_train_X, target_train_y['units'].ravel())\n",
    "    # make predictions for test data\n",
    "    prediction = model.predict(data_test_X)\n",
    "    #predictions = [round(value) for value in y_pred]\n",
    "    df_prediction=pd.DataFrame(prediction, columns=['pre_units']).round(decimals=5)\n",
    "    df_prediction_need=df_test_data_X[['key','day','units']]\n",
    "    df_prediction_need=df_prediction_need.reset_index(drop=True)\n",
    "    df_prediction2=df_prediction_need.merge(df_prediction, left_index=True, right_index=True)\n",
    "    return df_prediction2\n",
    "\n",
    "def XGBoostingRegressor7(data_train_X, target_train_y, data_test_X, df_test_data_X):\n",
    "    model = XGBRegressor(colsample_bytree=1.0,learning_rate=0.05,n_estimators=1000,max_depth=9,min_child_weight=5,objective='reg:linear',subsample=1.0)\n",
    "    model.fit(data_train_X, target_train_y['units'].ravel())\n",
    "    # make predictions for test data\n",
    "    prediction = model.predict(data_test_X)\n",
    "    #predictions = [round(value) for value in y_pred]\n",
    "    df_prediction=pd.DataFrame(prediction, columns=['pre_units']).round(decimals=5)\n",
    "    df_prediction_need=df_test_data_X[['key','day','units']]\n",
    "    df_prediction_need=df_prediction_need.reset_index(drop=True)\n",
    "    df_prediction2=df_prediction_need.merge(df_prediction, left_index=True, right_index=True)\n",
    "    return df_prediction2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pre_cluster0=XGBoostingRegressor0(X0[1], X0[0], X0[3],X0[4])\n",
    "pre_cluster1=XGBoostingRegressor1(X1[1], X1[0], X1[3],X1[4])\n",
    "pre_cluster2=XGBoostingRegressor2(X2[1], X2[0], X2[3],X2[4])\n",
    "pre_cluster3=XGBoostingRegressor3(X3[1], X3[0], X3[3],X3[4])\n",
    "pre_cluster4=XGBoostingRegressor4(X4[1], X4[0], X4[3],X4[4])\n",
    "pre_cluster5=XGBoostingRegressor5(X5[1], X5[0], X5[3],X5[4])\n",
    "pre_cluster6=XGBoostingRegressor6(X6[1], X6[0], X6[3],X6[4])\n",
    "pre_cluster7=XGBoostingRegressor7(X7[1], X7[0], X7[3],X7[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrate prediciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pre_total = pd.concat([pre_cluster0,pre_cluster1,pre_cluster2,pre_cluster3,pre_cluster4,pre_cluster5,pre_cluster6,pre_cluster7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pre_total.to_csv('XGBoostRegression_predict.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def predict_date(pre_total):\n",
    "    date=[]\n",
    "    i=0\n",
    "    n=len(pre_total)\n",
    "    while i<n:\n",
    "        day_int=pre_total.iloc[i]['day']\n",
    "        date_time=datetime.datetime(2018, 1, day_int, 0,0).date()\n",
    "        date_string=str(date_time)\n",
    "        date.append(date_string)\n",
    "        i+=1\n",
    "    pre_total['date']=date\n",
    "\n",
    "    pre_total_pvt = pre_total.pivot_table('pre_units', ['key'], 'date')\n",
    "    pre_total_pvt['key'] = pre_total_pvt.index\n",
    "    pre_total_pvt=pre_total_pvt.reset_index(drop=True)\n",
    "\n",
    "    return pre_total_pvt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pre_total_pvt    = predict_date(pre_total)\n",
    "pre_cluster0_pvt = predict_date(pre_cluster0)\n",
    "pre_cluster1_pvt = predict_date(pre_cluster1)\n",
    "pre_cluster2_pvt = predict_date(pre_cluster2)\n",
    "pre_cluster3_pvt = predict_date(pre_cluster3)\n",
    "pre_cluster4_pvt = predict_date(pre_cluster4)\n",
    "pre_cluster5_pvt = predict_date(pre_cluster5)\n",
    "pre_cluster6_pvt = predict_date(pre_cluster6)\n",
    "pre_cluster7_pvt = predict_date(pre_cluster7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# measure sold_out_day deviation with actual\n",
    "from datetime import date, timedelta\n",
    "import math\n",
    "\n",
    "def measurement(prediction, test_data):\n",
    "\n",
    "    d1 = date(2018, 1, 1)  \n",
    "    d2 = date(2018, 1, 31)  \n",
    "    delta = d2 - d1         \n",
    "    pre_day=[]\n",
    "    for i in range(delta.days + 1):\n",
    "        add=str(d1 + timedelta(days=i))\n",
    "        pre_day.append(add)\n",
    "\n",
    "    sold_out_day=[]\n",
    "    key=[]\n",
    "    #lets innet join with testdataset\n",
    "    window_inner=test_data.merge(prediction,left_on='key', right_on='key', how='inner')\n",
    "    window_inner['sol_out_pred']=0\n",
    "\n",
    "    all_=window_inner.key.unique()\n",
    "    n=len(all_) \n",
    "    i=0\n",
    "    #print (n)\n",
    "\n",
    "    while(i<n): #dataset\n",
    "        subset_w=prediction.loc[prediction['key']==all_[i]]\n",
    "        stock=test_data.loc[test_data['key']==all_[i]]['stock'].values[0]\n",
    "        n1=len(pre_day)\n",
    "        k=0\n",
    "        while(k<n1): #day calculation\n",
    "            day=pre_day[k]\n",
    "            that_day_sale=subset_w[day].values[0]\n",
    "            stock=stock-that_day_sale\n",
    "            if((stock==0) | (stock<0) ):\n",
    "                window_inner.iloc[i,37]=day\n",
    "                break\n",
    "            if(k==(n1-1)): \n",
    "                window_inner.iloc[i,37]='2018-01-22'\n",
    "            k=k+1\n",
    "        i=i+1\n",
    "\n",
    "    test_data['key']=test_data[\"pid\"].astype(str) +\" \"+ test_data[\"size\"].astype(str)\n",
    "    window_inner[\"date_sold_out_date\"] = pd.to_datetime(window_inner[\"sold_out_date\"],format=\"%Y/%m/%d\")\n",
    "    window_inner[\"date_sold_out_pred\"] = pd.to_datetime(window_inner[\"sol_out_pred\"],format=\"%Y/%m/%d\")\n",
    "\n",
    "    days_differ=window_inner[\"date_sold_out_date\"]-window_inner[\"date_sold_out_pred\"]\n",
    "    days=pd.DataFrame(days_differ,columns=['days_differ'])\n",
    "    days['correct_differ']=abs(days['days_differ'].astype(datetime.timedelta).map(lambda x: np.nan if pd.isnull(x) else x.days))\n",
    "\n",
    "    error=math.sqrt(sum(days['correct_differ']))\n",
    "\n",
    "    return error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error(test_table,total,cluster0,cluster1,cluster2,cluster3,cluster4,cluster5,cluster6,cluster7):\n",
    "    \n",
    "    test_data_0=pd.read_csv(test_table)\n",
    "    test_data_0['pid']=test_data_0['pid'].astype(int)\n",
    "    test_data_0['key']=test_data_0[\"pid\"].astype(str) +\" \"+ test_data_0[\"size\"].astype(str)\n",
    "\n",
    "    error_0=measurement(cluster0, test_data_0)\n",
    "    error_1=measurement(cluster1, test_data_0)\n",
    "    error_2=measurement(cluster2, test_data_0)\n",
    "    error_3=measurement(cluster3, test_data_0)\n",
    "    error_4=measurement(cluster4, test_data_0)\n",
    "    error_5=measurement(cluster5, test_data_0)\n",
    "    error_6=measurement(cluster6, test_data_0)\n",
    "    error_7=measurement(cluster7, test_data_0)\n",
    "    error_total=measurement(total, test_data_0)\n",
    "\n",
    "    print('cluster0',error_0,'\\ncluster1',error_1, '\\ncluster2',error_2,'\\ncluster3',error_3, '\\ncluster4',error_4,\n",
    "          '\\ncluster5',error_5, '\\ncluster6',error_6,'\\ncluster7',error_7,'\\ntotoal',error_total)\n",
    "    \n",
    "    return error_0,error_1,error_2,error_3,error_4,error_5,error_6,error_7,error_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster0 200.57916143009473 \n",
      "cluster1 106.2449998823474 \n",
      "cluster2 40.39801975344831 \n",
      "cluster3 7.0710678118654755 \n",
      "cluster4 22.15851980616034 \n",
      "cluster5 14.966629547095765 \n",
      "cluster6 3.3166247903554 \n",
      "cluster7 101.59724405711013 \n",
      "totoal 253.4758371127315\n"
     ]
    }
   ],
   "source": [
    "error_0=error('test_0.csv',pre_total_pvt,pre_cluster0_pvt,pre_cluster1_pvt,pre_cluster2_pvt,pre_cluster3_pvt,pre_cluster4_pvt,pre_cluster5_pvt,pre_cluster6_pvt,pre_cluster7_pvt,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster0 201.67300265528849 \n",
      "cluster1 107.13076122197583 \n",
      "cluster2 40.24922359499622 \n",
      "cluster3 6.782329983125268 \n",
      "cluster4 20.73644135332772 \n",
      "cluster5 14.696938456699069 \n",
      "cluster6 4.123105625617661 \n",
      "cluster7 101.48398888494677 \n",
      "totoal 254.51326095117324\n"
     ]
    }
   ],
   "source": [
    "error_1=error('test_1.csv',pre_total_pvt,pre_cluster0_pvt,pre_cluster1_pvt,pre_cluster2_pvt,pre_cluster3_pvt,pre_cluster4_pvt,pre_cluster5_pvt,pre_cluster6_pvt,pre_cluster7_pvt,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster0 202.2943400097986 \n",
      "cluster1 107.53139076567363 \n",
      "cluster2 41.400483088968905 \n",
      "cluster3 7.14142842854285 \n",
      "cluster4 21.166010488516726 \n",
      "cluster5 13.92838827718412 \n",
      "cluster6 4.123105625617661 \n",
      "cluster7 101.35580891098448 \n",
      "totoal 255.30961595678295\n"
     ]
    }
   ],
   "source": [
    "error_2=error('test_2.csv',pre_total_pvt,pre_cluster0_pvt,pre_cluster1_pvt,pre_cluster2_pvt,pre_cluster3_pvt,pre_cluster4_pvt,pre_cluster5_pvt,pre_cluster6_pvt,pre_cluster7_pvt,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster0 201.27344583923633 \n",
      "cluster1 107.71722239270747 \n",
      "cluster2 40.85339643163099 \n",
      "cluster3 7.54983443527075 \n",
      "cluster4 20.199009876724155 \n",
      "cluster5 14.7648230602334 \n",
      "cluster6 3.3166247903554 \n",
      "cluster7 101.06433594498111 \n",
      "totoal 254.3442548987494\n"
     ]
    }
   ],
   "source": [
    "error_3=error('test_3.csv',pre_total_pvt,pre_cluster0_pvt,pre_cluster1_pvt,pre_cluster2_pvt,pre_cluster3_pvt,pre_cluster4_pvt,pre_cluster5_pvt,pre_cluster6_pvt,pre_cluster7_pvt,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster0 201.2486024796197 \n",
      "cluster1 107.33126291998991 \n",
      "cluster2 40.38564101261734 \n",
      "cluster3 6.244997998398398 \n",
      "cluster4 21.863211109075447 \n",
      "cluster5 14.52583904633395 \n",
      "cluster6 3.605551275463989 \n",
      "cluster7 100.93066927351666 \n",
      "totoal 254.12595302329905\n"
     ]
    }
   ],
   "source": [
    "error_4=error('test_4.csv',pre_total_pvt,pre_cluster0_pvt,pre_cluster1_pvt,pre_cluster2_pvt,pre_cluster3_pvt,pre_cluster4_pvt,pre_cluster5_pvt,pre_cluster6_pvt,pre_cluster7_pvt,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster0 201.413710483 \n",
      "cluster1 107.191127437 \n",
      "cluster2 40.6573527763 \n",
      "cluster3 6.95793173144 \n",
      "cluster4 21.2246385268 \n",
      "cluster5 14.5765236775 \n",
      "cluster6 3.69700242148 \n",
      "cluster7 101.286409414 \n",
      "totoal 254.353784389\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "c0=[error_0[0],error_1[0],error_2[0],error_3[0],error_4[0]]\n",
    "c1=[error_0[1],error_1[1],error_2[1],error_3[1],error_4[1]]\n",
    "c2=[error_0[2],error_1[2],error_2[2],error_3[2],error_4[2]]\n",
    "c3=[error_0[3],error_1[3],error_2[3],error_3[3],error_4[3]]\n",
    "c4=[error_0[4],error_1[4],error_2[4],error_3[4],error_4[4]]\n",
    "c5=[error_0[5],error_1[5],error_2[5],error_3[5],error_4[5]]\n",
    "c6=[error_0[6],error_1[6],error_2[6],error_3[6],error_4[6]]\n",
    "c7=[error_0[7],error_1[7],error_2[7],error_3[7],error_4[7]]\n",
    "total=[error_0[8],error_1[8],error_2[8],error_3[8],error_4[8]]\n",
    "\n",
    "print('cluster0',numpy.mean(c0),'\\ncluster1',numpy.mean(c1), '\\ncluster2',numpy.mean(c2),'\\ncluster3',numpy.mean(c3), '\\ncluster4',numpy.mean(c4),\n",
    "           '\\ncluster5',numpy.mean(c5), '\\ncluster6',numpy.mean(c6),'\\ncluster7',numpy.mean(c7),'\\ntotoal',numpy.mean(total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
