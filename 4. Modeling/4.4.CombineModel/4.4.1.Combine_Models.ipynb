{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According our analysis, we found out that Time Series Clustering Results with the best model.\n",
    "\n",
    "\n",
    "Cluster 1: Windowing Time Series Clustering (Lag=20)- ANN\n",
    "\n",
    "Cluster 2: XGBoostRegression w/ 1 temprature\n",
    "\n",
    "Cluster 3: XGBoostRegression w/ 2 temprature\n",
    "\n",
    "Cluster 4: Windowing Time Series Clustering (Lag=20)- ANN\n",
    "\n",
    "Cluster 5: Arima Time Clustering\n",
    "\n",
    "Cluster 6: Arima Time Clustering\n",
    "\n",
    "Cluster 7: Windowing Individually Lag(30)-  Linear Reg\n",
    "\n",
    "Cluster 8: GBoostRegression w/ 2 temprature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '/Users/bengikoseoglu/anaconda2/lib/python27.zip',\n",
       " '/Users/bengikoseoglu/anaconda2/lib/python2.7',\n",
       " '/Users/bengikoseoglu/anaconda2/lib/python2.7/plat-darwin',\n",
       " '/Users/bengikoseoglu/anaconda2/lib/python2.7/plat-mac',\n",
       " '/Users/bengikoseoglu/anaconda2/lib/python2.7/plat-mac/lib-scriptpackages',\n",
       " '/Users/bengikoseoglu/anaconda2/lib/python2.7/lib-tk',\n",
       " '/Users/bengikoseoglu/anaconda2/lib/python2.7/lib-old',\n",
       " '/Users/bengikoseoglu/anaconda2/lib/python2.7/lib-dynload',\n",
       " '/Users/bengikoseoglu/anaconda2/lib/python2.7/site-packages',\n",
       " '/Users/bengikoseoglu/anaconda2/lib/python2.7/site-packages/aeosa',\n",
       " '/Users/bengikoseoglu/anaconda2/lib/python2.7/site-packages/IPython/extensions',\n",
       " '/Users/bengikoseoglu/.ipython',\n",
       " '/Users/bengikoseoglu/anaconda2/pkgs',\n",
       " '/Library/Python/2.7/site-packages']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/bengikoseoglu/anaconda2/pkgs')\n",
    "sys.path.append('/Library/Python/2.7/site-packages')\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from datetime import date, timedelta\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv('integrated_data.csv')\n",
    "cluster_time=pd.read_csv('clusters_key_7.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_cluster=cluster_time[['key','clust_7']]\n",
    "data=pd.merge(data,data_cluster,on='key',how='left')\n",
    "data=data.fillna(8) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df=data[data.month !=1] \n",
    "test_df=data[data.month ==1 ] \n",
    "test_df=test_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cluster_2temp(train,test,label):\n",
    "    df_train_data_x=train[train.clust_7 ==label]\n",
    "    df_test_data_x=test[test.clust_7 ==label] \n",
    "    \n",
    "    target_train_x = df_train_data_x[['units']]\n",
    "    data_train_x = df_train_data_x.drop(['Unnamed: 0','units','key','date','rrp','weekday','month','day','clust_7','sum_unit'], axis=1)\n",
    "    target_test_x = df_test_data_x[['units']]\n",
    "    data_test_x =df_test_data_x.drop(['Unnamed: 0','units','key','date','rrp','weekday','month','day','clust_7','sum_unit'], axis=1)\n",
    "    \n",
    "    return target_train_x,data_train_x,target_test_x,data_test_x,df_test_data_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X0=cluster_2temp(train_df,test_df,1)\n",
    "X1=cluster_2temp(train_df,test_df,2)\n",
    "X2=cluster_2temp(train_df,test_df,3)\n",
    "X3=cluster_2temp(train_df,test_df,4)\n",
    "X4=cluster_2temp(train_df,test_df,5)\n",
    "X5=cluster_2temp(train_df,test_df,6)\n",
    "X6=cluster_2temp(train_df,test_df,7)\n",
    "X7=cluster_2temp(train_df,test_df,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cluster_1temp(train,test,label):\n",
    "    df_train_data_x=train[train.clust_7 ==label]\n",
    "    df_test_data_x=test[test.clust_7 ==label] \n",
    "    \n",
    "    target_train_x = df_train_data_x[['units']]\n",
    "    data_train_x = df_train_data_x.drop(['Unnamed: 0','units','key','date','rrp','weekday','month','day','clust_7','sum_unit','median_temp'], axis=1)\n",
    "    target_test_x = df_test_data_x[['units']]\n",
    "    data_test_x =df_test_data_x.drop(['Unnamed: 0','units','key','date','rrp','weekday','month','day','clust_7','sum_unit','median_temp'], axis=1)\n",
    "    \n",
    "    return target_train_x,data_train_x,target_test_x,data_test_x,df_test_data_x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X1=cluster_1temp(train_df,test_df,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster 1 (Windowing Time Seies Clustering (Lag:20) Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\t\"\"\"\n",
    "\tFrame a time series as a supervised learning dataset.\n",
    "\tArguments:\n",
    "\t\tdata: Sequence of observations as a list or NumPy array.\n",
    "\t\tn_in: Number of lag observations as input (X).\n",
    "\t\tn_out: Number of observations as output (y).\n",
    "\t\tdropnan: Boolean whether or not to drop rows with NaN values.\n",
    "\tReturns:\n",
    "\t\tPandas DataFrame of series framed for supervised learning.\n",
    "\t\"\"\"\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d1 = date(2017, 10, 1)  # start date\n",
    "d2 = date(2017, 12, 31)  # end date\n",
    "d3=date(2018, 1, 31)\n",
    "\n",
    "delta = d2 - d1         # timedelta\n",
    "\n",
    "pre_day=[]\n",
    "for i in range(delta.days + 1):\n",
    "    add=str(d1 + timedelta(days=i))\n",
    "    pre_day.append(add)\n",
    "\n",
    "post_day=[] \n",
    "delta_2 = d3 - d2\n",
    "\n",
    "for i in range(delta_2.days + 1):\n",
    "    add=str(d2 + timedelta(days=i))\n",
    "    post_day.append(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train=train=train_df[['date','key','units']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col=post_day[1:]\n",
    "col.append('key')\n",
    "all_=train.key.unique()\n",
    "empty_matrix=np.zeros((len(all_),len(col)))\n",
    "data_frame=pd.DataFrame(data=empty_matrix, columns=col)\n",
    "data_frame['key']=all_\n",
    "days=range(1,32)\n",
    "n1=len(days)\n",
    "empty_matrix_2=np.zeros((all_.shape[0],1))\n",
    "data_frame_2=pd.DataFrame(data=empty_matrix_2, columns=['key'])\n",
    "data_frame_2['key']=all_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales_prev=train.loc[train['date']=='2017-12-31'].sort_values('date')[['key','units']]\n",
    "sales_prev1=train.loc[train['date']=='2017-12-30'].sort_values('date')[['key','units']]\n",
    "sales_prev2=train.loc[train['date']=='2017-12-29'].sort_values('date')[['key','units']]\n",
    "sales_prev3=train.loc[train['date']=='2017-12-28'].sort_values('date')[['key','units']]\n",
    "sales_prev4=train.loc[train['date']=='2017-12-27'].sort_values('date')[['key','units']]\n",
    "sales_prev5=train.loc[train['date']=='2017-12-26'].sort_values('date')[['key','units']]\n",
    "sales_prev6=train.loc[train['date']=='2017-12-25'].sort_values('date')[['key','units']]\n",
    "sales_prev7=train.loc[train['date']=='2017-12-24'].sort_values('date')[['key','units']]\n",
    "sales_prev8=train.loc[train['date']=='2017-12-23'].sort_values('date')[['key','units']] \n",
    "sales_prev9=train.loc[train['date']=='2017-12-22'].sort_values('date')[['key','units']]\n",
    "sales_prev10=train.loc[train['date']=='2017-12-21'].sort_values('date')[['key','units']]\n",
    "sales_prev11=train.loc[train['date']=='2017-12-20'].sort_values('date')[['key','units']]\n",
    "sales_prev12=train.loc[train['date']=='2017-12-19'].sort_values('date')[['key','units']]\n",
    "sales_prev13=train.loc[train['date']=='2017-12-18'].sort_values('date')[['key','units']]\n",
    "sales_prev14=train.loc[train['date']=='2017-12-17'].sort_values('date')[['key','units']]\n",
    "sales_prev15=train.loc[train['date']=='2017-12-16'].sort_values('date')[['key','units']]\n",
    "sales_prev16=train.loc[train['date']=='2017-12-15'].sort_values('date')[['key','units']]\n",
    "sales_prev17=train.loc[train['date']=='2017-12-14'].sort_values('date')[['key','units']]\n",
    "sales_prev18=train.loc[train['date']=='2017-12-13'].sort_values('date')[['key','units']]\n",
    "sales_prev19=train.loc[train['date']=='2017-12-12'].sort_values('date')[['key','units']]\n",
    "sales_prev20=train.loc[train['date']=='2017-12-11'].sort_values('date')[['key','units']]\n",
    "sales_prev21=train.loc[train['date']=='2017-12-10'].sort_values('date')[['key','units']]\n",
    "sales_prev22=train.loc[train['date']=='2017-12-09'].sort_values('date')[['key','units']]\n",
    "data_frame_2=data_frame_2.merge(sales_prev, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-31'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev1, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-30'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev2, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-29'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev3, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-28'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev4, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-27'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev5, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-26'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev6, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-25'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev7, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-24'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev8, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-23'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev9, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-22'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev10, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-21'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev11, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-20'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev12, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-19'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev13, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-18'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev14, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-17'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev15, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-16'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev16, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-15'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev17, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-14'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev18, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-13'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev19, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-12'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev20, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-11'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev21, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-10'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev22, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-09'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_frame_2=data_frame_2.fillna(0)\n",
    "data_frame=pd.merge(data_frame,data_frame_2,on='key')\n",
    "c_train=train.merge(data_cluster,on='key',how='inner')\n",
    "c_dataframe=data_frame.merge(data_cluster,on='key',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Running the clusters\n",
    "def days_20_cluster(c_dataframe,c_train,clusterno,model):\n",
    "    c_train_c1=c_train[c_train['clust_7']==clusterno].drop('clust_7',axis=1)\n",
    "    print(c_train_c1.shape)\n",
    "    print(c_train_c1.key.nunique())\n",
    "    train_comb_pivot=c_train_c1.pivot(index='key', columns='date', values='units')\n",
    "    train_comb_pivot=train_comb_pivot.mean(axis=0) #assign zero sales\n",
    "    train_comb_pivot=pd.DataFrame(train_comb_pivot,columns=['mean_sales'])\n",
    "    train_cluster1=series_to_supervised(train_comb_pivot, n_in=20, n_out=1, dropnan=True)\n",
    "    cluster1=c_dataframe[c_dataframe['clust_7']==clusterno]\n",
    "    cluster1_n=cluster1.drop('clust_7',axis=1).set_index('key').transpose()\n",
    "    cluster1.mean=cluster1_n.mean(axis=1)\n",
    "    cluster1_frame=pd.DataFrame(cluster1.mean, columns=['mean_sales'])\n",
    "    cluster1_frame=cluster1_frame.transpose()\n",
    "    model.fit(train_cluster1[['var1(t-20)', 'var1(t-19)', 'var1(t-18)', 'var1(t-17)',\n",
    "       'var1(t-16)', 'var1(t-15)', 'var1(t-14)', 'var1(t-13)',\n",
    "       'var1(t-12)', 'var1(t-11)', 'var1(t-10)', 'var1(t-9)', 'var1(t-8)',\n",
    "       'var1(t-7)', 'var1(t-6)', 'var1(t-5)', 'var1(t-4)', 'var1(t-3)',\n",
    "       'var1(t-2)', 'var1(t-1)']],train_cluster1[['var1(t)']])\n",
    "    \n",
    "    days=range(1,32)\n",
    "    n1=len(days)\n",
    "    k=0\n",
    "    day_n=0\n",
    "    while(k<n1):\n",
    "        day=days[day_n]\n",
    "        predicted_date=datetime.date(year=2018,day=day,month=1)\n",
    "        pred_date_before=predicted_date- timedelta(days=20)\n",
    "        delta = predicted_date - pred_date_before\n",
    "        bet_day=[]\n",
    "        for i in range(delta.days):\n",
    "            add=str(pred_date_before + timedelta(days=i))\n",
    "            bet_day.append(add)\n",
    "        pred_date_before=str(pred_date_before)\n",
    "        predicted_date=str(predicted_date)\n",
    "        value_predict=cluster1_frame[bet_day]\n",
    "        y_pred=model.predict(value_predict)\n",
    "        cluster1_frame[predicted_date]=y_pred\n",
    "        k=k+1\n",
    "        day_n=day_n+1\n",
    "    return(cluster1_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(806078, 3)\n",
      "8858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bengikoseoglu/anaconda2/lib/python2.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:1306: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "mlp = MLPRegressor()\n",
    "cluster1_frame=days_20_cluster(c_dataframe,c_train,1,mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bengikoseoglu/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/bengikoseoglu/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "cluster1_reults=cluster1_frame.transpose()[0:31].values\n",
    "test=test_df[['key','date','units']].merge(data_cluster, on='key',how='inner')\n",
    "test_clust1=test[test['clust_7']==1]\n",
    "i=0\n",
    "n=test_clust1.key.nunique()\n",
    "days=range(1,32)\n",
    "day_all=[]\n",
    "pre_unit=[]\n",
    "while(i<n):\n",
    "    k=0\n",
    "    while(k<31):\n",
    "        day_all.append(days[k])\n",
    "        uni=cluster1_reults[k][0]\n",
    "        pre_unit.append(uni)\n",
    "        k=k+1\n",
    "    i=i+1\n",
    "\n",
    "test_clust1['day']=day_all\n",
    "test_clust1['pre_units']=pre_unit\n",
    "test_clust1=test_clust1.drop(['clust_7'],axis=1)\n",
    "\n",
    "pre_cluster1=test_clust1[['key','day','units','pre_units']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>day</th>\n",
       "      <th>units</th>\n",
       "      <th>pre_units</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19671 39 1/3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.058323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19671 39 1/3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.057411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19671 39 1/3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19671 39 1/3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.054305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19671 39 1/3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.054117</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            key  day  units  pre_units\n",
       "0  19671 39 1/3    1    0.0   0.058323\n",
       "1  19671 39 1/3    2    0.0   0.057411\n",
       "2  19671 39 1/3    3    0.0   0.060539\n",
       "3  19671 39 1/3    4    0.0   0.054305\n",
       "4  19671 39 1/3    5    0.0   0.054117"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_cluster1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster 2 (XGBoostRegression w/ 1 temprature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "def XGBoostingRegressor1(data_train_X, target_train_y, data_test_X, df_test_data_X):\n",
    "    model = XGBRegressor(colsample_bytree=0.7,learning_rate=0.03,n_estimators=1000,max_depth=6,min_child_weight=7,objective='reg:linear',subsample=0.7)\n",
    "    model.fit(data_train_X, target_train_y['units'].ravel())\n",
    "    # make predictions for test data\n",
    "    prediction = model.predict(data_test_X)\n",
    "    #predictions = [round(value) for value in y_pred]\n",
    "    df_prediction=pd.DataFrame(prediction, columns=['pre_units']).round(decimals=5)\n",
    "    df_prediction_need=df_test_data_X[['key','day','units']]\n",
    "    df_prediction_need=df_prediction_need.reset_index(drop=True)\n",
    "    df_prediction2=df_prediction_need.merge(df_prediction, left_index=True, right_index=True)\n",
    "    return df_prediction2\n",
    "\n",
    "pre_cluster2=XGBoostingRegressor1(X1[1], X1[0], X1[3],X1[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster 3 (XGBoostRegression w/ 2 temprature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def XGBoostingRegressor2(data_train_X, target_train_y, data_test_X, df_test_data_X):\n",
    "    model = XGBRegressor(colsample_bytree=0.7,learning_rate=0.03,n_estimators=1000,max_depth=6,min_child_weight=7,objective='reg:linear',subsample=0.8)\n",
    "    model.fit(data_train_X, target_train_y['units'].ravel())\n",
    "    # make predictions for test data\n",
    "    prediction = model.predict(data_test_X)\n",
    "    #predictions = [round(value) for value in y_pred]\n",
    "    df_prediction=pd.DataFrame(prediction, columns=['pre_units']).round(decimals=5)\n",
    "    df_prediction_need=df_test_data_X[['key','day','units']]\n",
    "    df_prediction_need=df_prediction_need.reset_index(drop=True)\n",
    "    df_prediction2=df_prediction_need.merge(df_prediction, left_index=True, right_index=True)\n",
    "    return df_prediction2\n",
    "\n",
    "pre_cluster3=XGBoostingRegressor2(X2[1], X2[0], X2[3],X2[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster 4 (ANN Windowing TimeSeries Clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1001, 3)\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "mlp = MLPRegressor()\n",
    "cluster4_frame=days_20_cluster(c_dataframe,c_train,4,mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bengikoseoglu/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/bengikoseoglu/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "cluster4_reults=cluster4_frame.transpose()[0:31].values\n",
    "cluster4_reults\n",
    "test=test_df[['key','date','units']].merge(data_cluster, on='key',how='inner')\n",
    "test_clust4=test[test['clust_7']==4]\n",
    "i=0\n",
    "n=test_clust4.key.nunique()\n",
    "days=range(1,32)\n",
    "day_all=[]\n",
    "pre_unit=[]\n",
    "while(i<n):\n",
    "    k=0\n",
    "    while(k<31):\n",
    "        day_all.append(days[k])\n",
    "        uni=cluster4_reults[k][0]\n",
    "        pre_unit.append(uni)\n",
    "        k=k+1\n",
    "    i=i+1\n",
    "test_clust4['day']=day_all\n",
    "test_clust4['pre_units']=pre_unit\n",
    "test_clust4=test_clust4.drop(['clust_7'],axis=1)\n",
    "\n",
    "pre_cluster4=test_clust4[['key','day','units','pre_units']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster 5 (Arima Time Clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clust5=pd.read_csv('ARIMA_dataset4_pred_units.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_clust5=X4[4][['key','day','units']]\n",
    "test_clust5=test_clust5.reset_index(drop=True)\n",
    "clust5_full=clust5.append([clust5]*(test_clust5.key.nunique()-1), ignore_index=True)\n",
    "pre_cluster5=pd.merge(test_clust5,clust5_full, left_index=True, right_index=True)\n",
    "\n",
    "pre_cluster5=pre_cluster5.rename(index=str, columns={'forecast of units': \"pre_units\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_cluster5.key.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster 6 (Arima Time Clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clust6=pd.read_csv('ARIMA_dataset5_pred_units.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_clust6=X5[4][['key','day','units']]\n",
    "test_clust6=test_clust6.reset_index(drop=True)\n",
    "clust6_full=clust6.append([clust6]*(test_clust6.key.nunique()-1), ignore_index=True)\n",
    "pre_cluster6=pd.merge(test_clust6,clust6_full, left_index=True, right_index=True)\n",
    "\n",
    "pre_cluster6=pre_cluster6.rename(index=str, columns={'forecast of units': \"pre_units\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster 7 (Windowing Individually Lag(30)-  Linear Reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "windowing_lag30_all=pd.read_csv('windowing_lag30_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "windowing_lag30_all=windowing_lag30_all.merge(data_cluster,on='key',how='inner')\n",
    "clust_7_frame=windowing_lag30_all[windowing_lag30_all['clust_7']==7] #takte the 7 clusters ones\n",
    "clust_7_frame=clust_7_frame.drop('Unnamed: 0',axis=1)\n",
    "clust_7_frame=clust_7_frame.set_index('key').transpose()[0:31]\n",
    "\n",
    "days=range(1,32)\n",
    "a=clust_7_frame[['16818 L']]\n",
    "a=a.rename(columns={'16818 L':'pre_units'}).reset_index()\n",
    "a['day']=days\n",
    "a['key']='16818 L'\n",
    "b=clust_7_frame[['12985 L']]\n",
    "b=b.rename(columns={'12985 L':'pre_units'}).reset_index()\n",
    "b['day']=days\n",
    "b['key']='12985 L'\n",
    "c=clust_7_frame[['15845 L']]\n",
    "c=c.rename(columns={'15845 L':'pre_units'}).reset_index()\n",
    "c['day']=days\n",
    "c['key']='15845 L'\n",
    "d=clust_7_frame[['15845 M']]\n",
    "d=d.rename(columns={'15845 M':'pre_units'}).reset_index()\n",
    "d['day']=days\n",
    "d['key']='15845 M'\n",
    "\n",
    "all_=pd.concat([a,b,c,d])\n",
    "all_=all_.rename(columns={'index':'date'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster7_results=all_.merge(test_df[['key','date','units']],on=['key','date'],how='inner')\n",
    "cluster7_results=cluster7_results.drop(['date'],axis=1)\n",
    "\n",
    "i=0\n",
    "n=cluster7_results.key.nunique()\n",
    "days=range(1,32)\n",
    "day_all=[]\n",
    "while(i<n):\n",
    "    k=0\n",
    "    while(k<31):\n",
    "        day_all.append(days[k])\n",
    "        k=k+1\n",
    "    i=i+1\n",
    "    \n",
    "cluster7_results['day']=day_all\n",
    "pre_cluster7=cluster7_results[['key','day','units','pre_units']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster 8 (GBoostRegression w/ 2 temprature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "\n",
    "params7 = {'n_estimators': 1000, 'max_depth': 3, 'min_samples_split': 2,\n",
    "              'learning_rate': 0.05, 'loss': 'ls'}\n",
    "clf7 = ensemble.GradientBoostingRegressor(**params7)\n",
    "clf7.fit(X7[1], X7[0]['units'].ravel()) \n",
    "prediction7= clf7.predict(X7[3])\n",
    "    \n",
    "df_prediction7=pd.DataFrame(prediction7, columns=['pre_units']).round(decimals=5)\n",
    "df_prediction_need7=X7[4][['key','day','units']]\n",
    "df_prediction_need7=df_prediction_need7.reset_index(drop=True)\n",
    "\n",
    "pre_cluster8=df_prediction_need7.merge(df_prediction7, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Integrate Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pre_total = pd.concat([pre_cluster1,pre_cluster2,pre_cluster3,pre_cluster4,pre_cluster5,pre_cluster6,pre_cluster7,pre_cluster8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pre_total.to_csv('CombineModel_predict.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def predict_date(pre_total):\n",
    "    date=[]\n",
    "    i=0\n",
    "    n=len(pre_total)\n",
    "    while i<n:\n",
    "        day_int=pre_total.iloc[i]['day']\n",
    "        date_time=datetime.datetime(2018, 1, day_int, 0,0).date()\n",
    "        date_string=str(date_time)\n",
    "        date.append(date_string)\n",
    "        i+=1\n",
    "    pre_total['date']=date\n",
    "\n",
    "    pre_total_pvt = pre_total.pivot_table('pre_units', ['key'], 'date')\n",
    "    pre_total_pvt['key'] = pre_total_pvt.index\n",
    "    pre_total_pvt=pre_total_pvt.reset_index(drop=True)\n",
    "\n",
    "    return pre_total_pvt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bengikoseoglu/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "pre_total_pvt    = predict_date(pre_total)\n",
    "pre_cluster0_pvt = predict_date(pre_cluster1)\n",
    "pre_cluster1_pvt = predict_date(pre_cluster2)\n",
    "pre_cluster2_pvt = predict_date(pre_cluster3)\n",
    "pre_cluster3_pvt = predict_date(pre_cluster4)\n",
    "pre_cluster4_pvt = predict_date(pre_cluster5)\n",
    "pre_cluster5_pvt = predict_date(pre_cluster6)\n",
    "pre_cluster6_pvt = predict_date(pre_cluster7)\n",
    "pre_cluster7_pvt = predict_date(pre_cluster8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# measure sold_out_day deviation with actual\n",
    "from datetime import date, timedelta\n",
    "import math\n",
    "\n",
    "def measurement(prediction, test_data):\n",
    "\n",
    "    d1 = date(2018, 1, 1)  \n",
    "    d2 = date(2018, 1, 31)  \n",
    "    delta = d2 - d1         \n",
    "    pre_day=[]\n",
    "    for i in range(delta.days + 1):\n",
    "        add=str(d1 + timedelta(days=i))\n",
    "        pre_day.append(add)\n",
    "\n",
    "    sold_out_day=[]\n",
    "    key=[]\n",
    "    #lets innet join with testdataset\n",
    "    window_inner=test_data.merge(prediction,left_on='key', right_on='key', how='inner')\n",
    "    window_inner['sol_out_pred']=0\n",
    "\n",
    "    all_=window_inner.key.unique()\n",
    "    n=len(all_) \n",
    "    i=0\n",
    "    #print (n)\n",
    "\n",
    "    while(i<n): #dataset\n",
    "        subset_w=prediction.loc[prediction['key']==all_[i]]\n",
    "        stock=test_data.loc[test_data['key']==all_[i]]['stock'].values[0]\n",
    "        n1=len(pre_day)\n",
    "        k=0\n",
    "        while(k<n1): #day calculation\n",
    "            day=pre_day[k]\n",
    "            that_day_sale=subset_w[day].values[0]\n",
    "            stock=stock-that_day_sale\n",
    "            if((stock==0) | (stock<0) ):\n",
    "                window_inner.iloc[i,37]=day\n",
    "                break\n",
    "            if(k==(n1-1)): \n",
    "                window_inner.iloc[i,37]='2018-01-22'\n",
    "            k=k+1\n",
    "        i=i+1\n",
    "\n",
    "    test_data['key']=test_data[\"pid\"].astype(str) +\" \"+ test_data[\"size\"].astype(str)\n",
    "    window_inner[\"date_sold_out_date\"] = pd.to_datetime(window_inner[\"sold_out_date\"],format=\"%Y/%m/%d\")\n",
    "    window_inner[\"date_sold_out_pred\"] = pd.to_datetime(window_inner[\"sol_out_pred\"],format=\"%Y/%m/%d\")\n",
    "\n",
    "    days_differ=window_inner[\"date_sold_out_date\"]-window_inner[\"date_sold_out_pred\"]\n",
    "    days=pd.DataFrame(days_differ,columns=['days_differ'])\n",
    "    days['correct_differ']=abs(days['days_differ'].astype(datetime.timedelta).map(lambda x: np.nan if pd.isnull(x) else x.days))\n",
    "\n",
    "    error=math.sqrt(sum(days['correct_differ']))\n",
    "\n",
    "    return error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error(test_table,total,cluster0,cluster1,cluster2,cluster3,cluster4,cluster5,cluster6,cluster7):\n",
    "    \n",
    "    test_data_0=pd.read_csv(test_table)\n",
    "    test_data_0['pid']=test_data_0['pid'].astype(int)\n",
    "    test_data_0['key']=test_data_0[\"pid\"].astype(str) +\" \"+ test_data_0[\"size\"].astype(str)\n",
    "\n",
    "    error_0=measurement(cluster0, test_data_0)\n",
    "    error_1=measurement(cluster1, test_data_0)\n",
    "    error_2=measurement(cluster2, test_data_0)\n",
    "    error_3=measurement(cluster3, test_data_0)\n",
    "    error_4=measurement(cluster4, test_data_0)\n",
    "    error_5=measurement(cluster5, test_data_0)\n",
    "    error_6=measurement(cluster6, test_data_0)\n",
    "    error_7=measurement(cluster7, test_data_0)\n",
    "    error_total=measurement(total, test_data_0)\n",
    "\n",
    "    print('cluster0',error_0,'\\ncluster1',error_1, '\\ncluster2',error_2,'\\ncluster3',error_3, '\\ncluster4',error_4,\n",
    "          '\\ncluster5',error_5, '\\ncluster6',error_6,'\\ncluster7',error_7,'\\ntotoal',error_total)\n",
    "    \n",
    "    return error_0,error_1,error_2,error_3,error_4,error_5,error_6,error_7,error_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('cluster0', 191.86974748511034, '\\ncluster1', 105.73551910309043, '\\ncluster2', 40.50925820105819, '\\ncluster3', 7.3484692283495345, '\\ncluster4', 23.15167380558045, '\\ncluster5', 15.491933384829668, '\\ncluster6', 2.23606797749979, '\\ncluster7', 101.59724405711013, '\\ntotoal', 246.5603374429878)\n"
     ]
    }
   ],
   "source": [
    "error_0=error('test_0.csv',pre_total_pvt,pre_cluster0_pvt,pre_cluster1_pvt,pre_cluster2_pvt,pre_cluster3_pvt,pre_cluster4_pvt,pre_cluster5_pvt,pre_cluster6_pvt,pre_cluster7_pvt,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('cluster0', 192.52272593125207, '\\ncluster1', 106.66770832824712, '\\ncluster2', 40.47221268969612, '\\ncluster3', 7.54983443527075, '\\ncluster4', 21.633307652783937, '\\ncluster5', 14.52583904633395, '\\ncluster6', 3.3166247903554, '\\ncluster7', 101.48398888494677, '\\ntotoal', 247.23875100800845)\n"
     ]
    }
   ],
   "source": [
    "error_1=error('test_1.csv',pre_total_pvt,pre_cluster0_pvt,pre_cluster1_pvt,pre_cluster2_pvt,pre_cluster3_pvt,pre_cluster4_pvt,pre_cluster5_pvt,pre_cluster6_pvt,pre_cluster7_pvt,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('cluster0', 192.4811679100062, '\\ncluster1', 107.12142642814275, '\\ncluster2', 41.42463035441596, '\\ncluster3', 7.3484692283495345, '\\ncluster4', 22.38302928559939, '\\ncluster5', 14.142135623730951, '\\ncluster6', 3.605551275463989, '\\ncluster7', 101.35580891098448, '\\ntotoal', 247.54999495051499)\n"
     ]
    }
   ],
   "source": [
    "error_2=error('test_2.csv',pre_total_pvt,pre_cluster0_pvt,pre_cluster1_pvt,pre_cluster2_pvt,pre_cluster3_pvt,pre_cluster4_pvt,pre_cluster5_pvt,pre_cluster6_pvt,pre_cluster7_pvt,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('cluster0', 191.65594172892213, '\\ncluster1', 107.4337004854622, '\\ncluster2', 40.90232267243512, '\\ncluster3', 7.681145747868608, '\\ncluster4', 21.330729007701542, '\\ncluster5', 15.0, '\\ncluster6', 2.449489742783178, '\\ncluster7', 101.06433594498111, '\\ntotoal', 246.79140989912918)\n"
     ]
    }
   ],
   "source": [
    "error_3=error('test_3.csv',pre_total_pvt,pre_cluster0_pvt,pre_cluster1_pvt,pre_cluster2_pvt,pre_cluster3_pvt,pre_cluster4_pvt,pre_cluster5_pvt,pre_cluster6_pvt,pre_cluster7_pvt,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('cluster0', 192.2888452302941, '\\ncluster1', 107.060730429042, '\\ncluster2', 40.50925820105819, '\\ncluster3', 7.3484692283495345, '\\ncluster4', 22.781571499789035, '\\ncluster5', 14.560219778561036, '\\ncluster6', 3.7416573867739413, '\\ncluster7', 100.93066927351666, '\\ntotoal', 247.1113109511582)\n"
     ]
    }
   ],
   "source": [
    "error_4=error('test_4.csv',pre_total_pvt,pre_cluster0_pvt,pre_cluster1_pvt,pre_cluster2_pvt,pre_cluster3_pvt,pre_cluster4_pvt,pre_cluster5_pvt,pre_cluster6_pvt,pre_cluster7_pvt,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('cluster0', 192.16368565711699, '\\ncluster1', 106.80381695479689, '\\ncluster2', 40.763536423732717, '\\ncluster3', 7.4552775736375922, '\\ncluster4', 22.256062250290871, '\\ncluster5', 14.744025566691121, '\\ncluster6', 3.0698782345752598, '\\ncluster7', 101.28640941430783, '\\ntotoal', 247.05036085035971)\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "c0=[error_0[0],error_1[0],error_2[0],error_3[0],error_4[0]]\n",
    "c1=[error_0[1],error_1[1],error_2[1],error_3[1],error_4[1]]\n",
    "c2=[error_0[2],error_1[2],error_2[2],error_3[2],error_4[2]]\n",
    "c3=[error_0[3],error_1[3],error_2[3],error_3[3],error_4[3]]\n",
    "c4=[error_0[4],error_1[4],error_2[4],error_3[4],error_4[4]]\n",
    "c5=[error_0[5],error_1[5],error_2[5],error_3[5],error_4[5]]\n",
    "c6=[error_0[6],error_1[6],error_2[6],error_3[6],error_4[6]]\n",
    "c7=[error_0[7],error_1[7],error_2[7],error_3[7],error_4[7]]\n",
    "total=[error_0[8],error_1[8],error_2[8],error_3[8],error_4[8]]\n",
    "\n",
    "print('cluster0',numpy.mean(c0),'\\ncluster1',numpy.mean(c1), '\\ncluster2',numpy.mean(c2),'\\ncluster3',numpy.mean(c3), '\\ncluster4',numpy.mean(c4),\n",
    "           '\\ncluster5',numpy.mean(c5), '\\ncluster6',numpy.mean(c6),'\\ncluster7',numpy.mean(c7),'\\ntotoal',numpy.mean(total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
