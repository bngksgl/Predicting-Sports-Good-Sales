{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According our analysis, we found out that Time Series Clustering Results with the best model.\n",
    "\n",
    "\n",
    "Cluster 1: Arima Time Clustering\n",
    "\n",
    "Cluster 2: GBoostRegression- sum unit\n",
    "\n",
    "Cluster 3: XGBoostRegression w/ 2 temprature\n",
    "\n",
    "Cluster 4: Windowing Time Series Clustering (Lag=20)- ANN\n",
    "\n",
    "Cluster 5: XGBoostRegression w/ 2 temprature\n",
    "\n",
    "Cluster 6: GBoostRegression- sum unit\n",
    "\n",
    "Cluster 7: Windowing Individually Lag(30)- Linear Reg\n",
    "\n",
    "Cluster 8: GBoostRegression w/ 2 temprature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '/Users/bengikoseoglu/anaconda2/lib/python27.zip',\n",
       " '/Users/bengikoseoglu/anaconda2/lib/python2.7',\n",
       " '/Users/bengikoseoglu/anaconda2/lib/python2.7/plat-darwin',\n",
       " '/Users/bengikoseoglu/anaconda2/lib/python2.7/plat-mac',\n",
       " '/Users/bengikoseoglu/anaconda2/lib/python2.7/plat-mac/lib-scriptpackages',\n",
       " '/Users/bengikoseoglu/anaconda2/lib/python2.7/lib-tk',\n",
       " '/Users/bengikoseoglu/anaconda2/lib/python2.7/lib-old',\n",
       " '/Users/bengikoseoglu/anaconda2/lib/python2.7/lib-dynload',\n",
       " '/Users/bengikoseoglu/anaconda2/lib/python2.7/site-packages',\n",
       " '/Users/bengikoseoglu/anaconda2/lib/python2.7/site-packages/aeosa',\n",
       " '/Users/bengikoseoglu/anaconda2/lib/python2.7/site-packages/IPython/extensions',\n",
       " '/Users/bengikoseoglu/.ipython',\n",
       " '/Users/bengikoseoglu/anaconda2/pkgs',\n",
       " '/Library/Python/2.7/site-packages']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/bengikoseoglu/anaconda2/pkgs')\n",
    "sys.path.append('/Library/Python/2.7/site-packages')\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from datetime import date, timedelta\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv('integrated_data.csv')\n",
    "cluster_time=pd.read_csv('clusters_key_7.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_cluster=cluster_time[['key','clust_7']]\n",
    "data=pd.merge(data,data_cluster,on='key',how='left')\n",
    "data=data.fillna(8) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df=data[data.month !=1] \n",
    "test_df=data[data.month ==1 ] \n",
    "test_df=test_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cluster_2temp(train,test,label):\n",
    "    df_train_data_x=train[train.clust_7 ==label]\n",
    "    df_test_data_x=test[test.clust_7 ==label] \n",
    "    \n",
    "    target_train_x = df_train_data_x[['units']]\n",
    "    data_train_x = df_train_data_x.drop(['Unnamed: 0','units','key','date','rrp','weekday','month','day','clust_7','sum_unit'], axis=1)\n",
    "    target_test_x = df_test_data_x[['units']]\n",
    "    data_test_x =df_test_data_x.drop(['Unnamed: 0','units','key','date','rrp','weekday','month','day','clust_7','sum_unit'], axis=1)\n",
    "    \n",
    "    return target_train_x,data_train_x,target_test_x,data_test_x,df_test_data_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X0=cluster_2temp(train_df,test_df,1)\n",
    "X1=cluster_2temp(train_df,test_df,2)\n",
    "X2=cluster_2temp(train_df,test_df,3)\n",
    "X3=cluster_2temp(train_df,test_df,4)\n",
    "X4=cluster_2temp(train_df,test_df,5)\n",
    "X5=cluster_2temp(train_df,test_df,6)\n",
    "X6=cluster_2temp(train_df,test_df,7)\n",
    "X7=cluster_2temp(train_df,test_df,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cluster_1temp(train,test,label):\n",
    "    df_train_data_x=train[train.clust_7 ==label]\n",
    "    df_test_data_x=test[test.clust_7 ==label] \n",
    "    \n",
    "    target_train_x = df_train_data_x[['units']]\n",
    "    data_train_x = df_train_data_x.drop(['Unnamed: 0','units','key','date','rrp','weekday','month','day','clust_7','sum_unit','median_temp'], axis=1)\n",
    "    target_test_x = df_test_data_x[['units']]\n",
    "    data_test_x =df_test_data_x.drop(['Unnamed: 0','units','key','date','rrp','weekday','month','day','clust_7','sum_unit','median_temp'], axis=1)\n",
    "    \n",
    "    return target_train_x,data_train_x,target_test_x,data_test_x,df_test_data_x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X1=cluster_1temp(train_df,test_df,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster 1 (Arima Time Clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clust0=pd.read_csv('ARIMA_dataset0_pred_units.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clust0['forecast_of_units']=clust0['forecast of units;'].str.replace(';','')\n",
    "clust0=clust0.drop('forecast of units;',axis=1)\n",
    "clust0['forecast_of_units']=clust0['forecast_of_units'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>forecast_of_units</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.082668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.079272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.076319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.073751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.071518</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   forecast_of_units\n",
       "0           0.082668\n",
       "1           0.079272\n",
       "2           0.076319\n",
       "3           0.073751\n",
       "4           0.071518"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clust0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_clust0=X0[4][['key','day','units']]\n",
    "test_clust0=test_clust0.reset_index(drop=True)\n",
    "clust0_full=clust0.append([clust0]*(test_clust0.key.nunique()-1), ignore_index=True)\n",
    "pre_cluster0=pd.merge(test_clust0,clust0_full, left_index=True, right_index=True)\n",
    "\n",
    "pre_cluster1=pre_cluster0.rename(index=str, columns={'forecast_of_units': \"pre_units\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster 2 (GBoostRegression sum unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_g=pd.read_csv('integrated_data.csv') #uploading the dataset again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "da_g=data_g[['key','month','units']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oct_=da_g[da_g['month']==10]\n",
    "nov_=da_g[da_g['month']==11]\n",
    "dec_=da_g[da_g['month']==12]\n",
    "octt=oct_.groupby(['key', 'month'])['units'].sum()\n",
    "novv=nov_.groupby(['key', 'month'])['units'].sum()\n",
    "decc=dec_.groupby(['key', 'month'])['units'].sum()\n",
    "octt=(pd.DataFrame(octt)).reset_index()\n",
    "novv=(pd.DataFrame(novv)).reset_index()\n",
    "decc=(pd.DataFrame(decc)).reset_index()\n",
    "octt=octt.rename(columns={'units':'prev_month_sales'})\n",
    "octt[['month']]=11\n",
    "novv=novv.rename(columns={'units':'prev_month_sales'})\n",
    "novv[['month']]=12\n",
    "decc=decc.rename(columns={'units':'prev_month_sales'})\n",
    "decc[['month']]=1\n",
    "nov_data=data_g[data_g.month ==11] \n",
    "dec_data=data_g[data_g.month ==12]\n",
    "jan_data=data_g[data_g.month ==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "november=nov_data.merge(octt[['key','prev_month_sales']],on=['key'],how='left')\n",
    "december=dec_data.merge(novv[['key','prev_month_sales']],on=['key'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data1=december.append(november)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test_data_g=jan_data.merge(decc[['key','prev_month_sales']],on=['key'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster_time=pd.read_csv('clusters_key_7')\n",
    "data_gg=pd.merge(data1,cluster_time[['key','clust_7']],on='key',how='left')\n",
    "data_gg=data_gg.fillna(8) # assign cluster 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_gg.date.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    531480\n",
       "8.0    108840\n",
       "2.0    103200\n",
       "3.0     17040\n",
       "5.0      5580\n",
       "6.0      2400\n",
       "4.0       660\n",
       "7.0       240\n",
       "Name: clust_7, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_gg.clust_7.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test_data_gg=pd.merge(df_test_data_g,cluster_time[['key','clust_7']],on='key',how='left')\n",
    "df_test_data_gg=df_test_data_gg.fillna(8) # assign cluster 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_data_gg.date.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train_data_gg=data_gg.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    531480\n",
       "8.0    108840\n",
       "2.0    103200\n",
       "3.0     17040\n",
       "5.0      5580\n",
       "6.0      2400\n",
       "4.0       660\n",
       "7.0       240\n",
       "Name: clust_7, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_data_gg.clust_7.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 7 train clusters\n",
    "df_train_data0=df_train_data_gg[df_train_data_gg.clust_7 ==1] \n",
    "df_train_data1=df_train_data_gg[df_train_data_gg.clust_7 ==2] \n",
    "df_train_data2=df_train_data_gg[df_train_data_gg.clust_7 ==3] \n",
    "df_train_data3=df_train_data_gg[df_train_data_gg.clust_7 ==4] \n",
    "df_train_data4=df_train_data_gg[df_train_data_gg.clust_7 ==5] \n",
    "df_train_data5=df_train_data_gg[df_train_data_gg.clust_7 ==6] \n",
    "df_train_data6=df_train_data_gg[df_train_data_gg.clust_7 ==7] \n",
    "df_train_data7=df_train_data_gg[df_train_data_gg.clust_7 ==8 ] \n",
    "\n",
    "#7 test clusters\n",
    "df_test_data0=df_test_data_gg[df_test_data_gg.clust_7 ==1] \n",
    "df_test_data1=df_test_data_gg[df_test_data_gg.clust_7 ==2]\n",
    "df_test_data2=df_test_data_gg[df_test_data_gg.clust_7 ==3] \n",
    "df_test_data3=df_test_data_gg[df_test_data_gg.clust_7 ==4]\n",
    "df_test_data4=df_test_data_gg[df_test_data_gg.clust_7 ==5]\n",
    "df_test_data5=df_test_data_gg[df_test_data_gg.clust_7 ==6]\n",
    "df_test_data6=df_test_data_gg[df_test_data_gg.clust_7 ==7]\n",
    "df_test_data7=df_test_data_gg[df_test_data_gg.clust_7 ==8] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    274598\n",
       "8.0     56234\n",
       "2.0     53320\n",
       "3.0      8804\n",
       "5.0      2883\n",
       "6.0      1240\n",
       "4.0       341\n",
       "7.0       124\n",
       "Name: clust_7, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_data_gg.clust_7.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(274598, 37)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_data0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53320, 37)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_train0 = df_train_data0[['units']]\n",
    "data_train0 = df_train_data0.drop(['Unnamed: 0','units','key','date','rrp','weekday','month','day','clust_7','sum_unit'], axis=1)\n",
    "target_test0 = df_test_data0[['units']]\n",
    "data_test0 =df_test_data0.drop(['Unnamed: 0','units','key','date','rrp','weekday','month','day','clust_7','sum_unit'], axis=1)\n",
    "\n",
    "target_train1 = df_train_data1[['units']]\n",
    "data_train1 = df_train_data1.drop(['Unnamed: 0','units','key','date','rrp','weekday','month','day','clust_7','sum_unit'], axis=1)\n",
    "target_test1 = df_test_data1[['units']]\n",
    "data_test1 =df_test_data1.drop(['Unnamed: 0','units','key','date','rrp','weekday','month','day','clust_7','sum_unit'], axis=1)\n",
    "\n",
    "target_train2 = df_train_data2[['units']]\n",
    "data_train2 = df_train_data2.drop(['Unnamed: 0','units','key','date','rrp','weekday','month','day','clust_7','sum_unit'], axis=1)\n",
    "target_test2 = df_test_data2[['units']]\n",
    "data_test2 =df_test_data2.drop(['Unnamed: 0','units','key','date','rrp','weekday','month','day','clust_7','sum_unit'], axis=1)\n",
    "\n",
    "target_train3 = df_train_data3[['units']]\n",
    "data_train3 = df_train_data3.drop(['Unnamed: 0','units','key','date','rrp','weekday','month','day','clust_7','sum_unit'], axis=1)\n",
    "target_test3 = df_test_data3[['units']]\n",
    "data_test3 =df_test_data3.drop(['Unnamed: 0','units','key','date','rrp','weekday','month','day','clust_7','sum_unit'], axis=1)\n",
    "\n",
    "target_train4 = df_train_data4[['units']]\n",
    "data_train4 = df_train_data4.drop(['Unnamed: 0','units','key','date','rrp','weekday','month','day','clust_7','sum_unit'], axis=1)\n",
    "target_test4 = df_test_data4[['units']]\n",
    "data_test4 =df_test_data4.drop(['Unnamed: 0','units','key','date','rrp','weekday','month','day','clust_7','sum_unit'], axis=1)\n",
    "\n",
    "target_train5 = df_train_data5[['units']]\n",
    "data_train5 = df_train_data5.drop(['Unnamed: 0','units','key','date','rrp','weekday','month','day','clust_7','sum_unit'], axis=1)\n",
    "target_test5 = df_test_data5[['units']]\n",
    "data_test5 =df_test_data5.drop(['Unnamed: 0','units','key','date','rrp','weekday','month','day','clust_7','sum_unit'], axis=1)\n",
    "\n",
    "target_train6 = df_train_data6[['units']]\n",
    "data_train6 = df_train_data6.drop(['Unnamed: 0','units','key','date','rrp','weekday','month','day','clust_7','sum_unit'], axis=1)\n",
    "target_test6 = df_test_data6[['units']]\n",
    "data_test6=df_test_data6.drop(['Unnamed: 0','units','key','date','rrp','weekday','month','day','clust_7','sum_unit'], axis=1)\n",
    "\n",
    "target_train6 = df_train_data6[['units']]\n",
    "data_train6 = df_train_data6.drop(['Unnamed: 0','units','key','date','rrp','weekday','month','day','clust_7','sum_unit'], axis=1)\n",
    "target_test6 = df_test_data6[['units']]\n",
    "data_test6=df_test_data6.drop(['Unnamed: 0','units','key','date','rrp','weekday','month','day','clust_7','sum_unit'], axis=1)\n",
    "\n",
    "target_train7 = df_train_data7[['units']]\n",
    "data_train7 = df_train_data7.drop(['Unnamed: 0','units','key','date','rrp','weekday','month','day','clust_7','sum_unit'], axis=1)\n",
    "target_test7 = df_test_data7[['units']]\n",
    "data_test7=df_test_data7.drop(['Unnamed: 0','units','key','date','rrp','weekday','month','day','clust_7','sum_unit'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103200, 27)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53320, 27)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "params5 = {'n_estimators': 1000, 'max_depth': 3, 'min_samples_split': 2,\n",
    "              'learning_rate': 0.05, 'loss': 'ls'}\n",
    "clf5 = ensemble.GradientBoostingRegressor(**params5)\n",
    "clf5.fit(data_train1, target_train1['units'].ravel()) \n",
    "prediction5= clf5.predict(data_test1)\n",
    "    \n",
    "df_prediction5=pd.DataFrame(prediction5, columns=['pre_units']).round(decimals=5)\n",
    "df_prediction_need5=df_test_data1[['key','day','units']]\n",
    "df_prediction_need5=df_prediction_need5.reset_index(drop=True)\n",
    "pre_cluster2=df_prediction_need5.merge(df_prediction5, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>day</th>\n",
       "      <th>units</th>\n",
       "      <th>pre_units</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19671 42 2/3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.04115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19671 42 2/3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.08911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19671 42 2/3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.05495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19671 42 2/3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19671 42 2/3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            key  day  units  pre_units\n",
       "0  19671 42 2/3    1    0.0    0.04115\n",
       "1  19671 42 2/3    2    0.0    0.08911\n",
       "2  19671 42 2/3    3    0.0   -0.05495\n",
       "3  19671 42 2/3    4    0.0    0.14930\n",
       "4  19671 42 2/3    5    0.0    0.02330"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_cluster2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53320, 4)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_cluster2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster 3 (XGBoostRegression w/ 2 temprature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "def XGBoostingRegressor2(data_train_X, target_train_y, data_test_X, df_test_data_X):\n",
    "    model = XGBRegressor(colsample_bytree=0.7,learning_rate=0.03,n_estimators=1000,max_depth=6,min_child_weight=7,objective='reg:linear',subsample=0.8)\n",
    "    model.fit(data_train_X, target_train_y['units'].ravel())\n",
    "    # make predictions for test data\n",
    "    prediction = model.predict(data_test_X)\n",
    "    #predictions = [round(value) for value in y_pred]\n",
    "    df_prediction=pd.DataFrame(prediction, columns=['pre_units']).round(decimals=5)\n",
    "    df_prediction_need=df_test_data_X[['key','day','units']]\n",
    "    df_prediction_need=df_prediction_need.reset_index(drop=True)\n",
    "    df_prediction2=df_prediction_need.merge(df_prediction, left_index=True, right_index=True)\n",
    "    return df_prediction2\n",
    "\n",
    "pre_cluster3=XGBoostingRegressor2(X2[1], X2[0], X2[3],X2[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster 4 (Windowing Time Seies Clustering (Lag:20) ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\t\"\"\"\n",
    "\tFrame a time series as a supervised learning dataset.\n",
    "\tArguments:\n",
    "\t\tdata: Sequence of observations as a list or NumPy array.\n",
    "\t\tn_in: Number of lag observations as input (X).\n",
    "\t\tn_out: Number of observations as output (y).\n",
    "\t\tdropnan: Boolean whether or not to drop rows with NaN values.\n",
    "\tReturns:\n",
    "\t\tPandas DataFrame of series framed for supervised learning.\n",
    "\t\"\"\"\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d1 = date(2017, 10, 1)  # start date\n",
    "d2 = date(2017, 12, 31)  # end date\n",
    "d3=date(2018, 1, 31)\n",
    "\n",
    "delta = d2 - d1         # timedelta\n",
    "\n",
    "pre_day=[]\n",
    "for i in range(delta.days + 1):\n",
    "    add=str(d1 + timedelta(days=i))\n",
    "    pre_day.append(add)\n",
    "\n",
    "post_day=[] \n",
    "delta_2 = d3 - d2\n",
    "\n",
    "for i in range(delta_2.days + 1):\n",
    "    add=str(d2 + timedelta(days=i))\n",
    "    post_day.append(add)\n",
    "train=train=train_df[['date','key','units']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col=post_day[1:]\n",
    "col.append('key')\n",
    "all_=train.key.unique()\n",
    "empty_matrix=np.zeros((len(all_),len(col)))\n",
    "data_frame=pd.DataFrame(data=empty_matrix, columns=col)\n",
    "data_frame['key']=all_\n",
    "days=range(1,32)\n",
    "n1=len(days)\n",
    "empty_matrix_2=np.zeros((all_.shape[0],1))\n",
    "data_frame_2=pd.DataFrame(data=empty_matrix_2, columns=['key'])\n",
    "data_frame_2['key']=all_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales_prev=train.loc[train['date']=='2017-12-31'].sort_values('date')[['key','units']]\n",
    "sales_prev1=train.loc[train['date']=='2017-12-30'].sort_values('date')[['key','units']]\n",
    "sales_prev2=train.loc[train['date']=='2017-12-29'].sort_values('date')[['key','units']]\n",
    "sales_prev3=train.loc[train['date']=='2017-12-28'].sort_values('date')[['key','units']]\n",
    "sales_prev4=train.loc[train['date']=='2017-12-27'].sort_values('date')[['key','units']]\n",
    "sales_prev5=train.loc[train['date']=='2017-12-26'].sort_values('date')[['key','units']]\n",
    "sales_prev6=train.loc[train['date']=='2017-12-25'].sort_values('date')[['key','units']]\n",
    "sales_prev7=train.loc[train['date']=='2017-12-24'].sort_values('date')[['key','units']]\n",
    "sales_prev8=train.loc[train['date']=='2017-12-23'].sort_values('date')[['key','units']] \n",
    "sales_prev9=train.loc[train['date']=='2017-12-22'].sort_values('date')[['key','units']]\n",
    "sales_prev10=train.loc[train['date']=='2017-12-21'].sort_values('date')[['key','units']]\n",
    "sales_prev11=train.loc[train['date']=='2017-12-20'].sort_values('date')[['key','units']]\n",
    "sales_prev12=train.loc[train['date']=='2017-12-19'].sort_values('date')[['key','units']]\n",
    "sales_prev13=train.loc[train['date']=='2017-12-18'].sort_values('date')[['key','units']]\n",
    "sales_prev14=train.loc[train['date']=='2017-12-17'].sort_values('date')[['key','units']]\n",
    "sales_prev15=train.loc[train['date']=='2017-12-16'].sort_values('date')[['key','units']]\n",
    "sales_prev16=train.loc[train['date']=='2017-12-15'].sort_values('date')[['key','units']]\n",
    "sales_prev17=train.loc[train['date']=='2017-12-14'].sort_values('date')[['key','units']]\n",
    "sales_prev18=train.loc[train['date']=='2017-12-13'].sort_values('date')[['key','units']]\n",
    "sales_prev19=train.loc[train['date']=='2017-12-12'].sort_values('date')[['key','units']]\n",
    "sales_prev20=train.loc[train['date']=='2017-12-11'].sort_values('date')[['key','units']]\n",
    "sales_prev21=train.loc[train['date']=='2017-12-10'].sort_values('date')[['key','units']]\n",
    "sales_prev22=train.loc[train['date']=='2017-12-09'].sort_values('date')[['key','units']]\n",
    "data_frame_2=data_frame_2.merge(sales_prev, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-31'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev1, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-30'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev2, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-29'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev3, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-28'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev4, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-27'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev5, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-26'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev6, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-25'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev7, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-24'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev8, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-23'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev9, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-22'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev10, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-21'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev11, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-20'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev12, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-19'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev13, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-18'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev14, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-17'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev15, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-16'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev16, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-15'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev17, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-14'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev18, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-13'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev19, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-12'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev20, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-11'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev21, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-10'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev22, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2017-12-09'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_frame_2=data_frame_2.fillna(0)\n",
    "data_frame=pd.merge(data_frame,data_frame_2,on='key')\n",
    "c_train=train.merge(data_cluster,on='key',how='inner')\n",
    "c_dataframe=data_frame.merge(data_cluster,on='key',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def days_20_cluster(c_dataframe,c_train,clusterno,model):\n",
    "    c_train_c1=c_train[c_train['clust_7']==clusterno].drop('clust_7',axis=1)\n",
    "    print(c_train_c1.shape)\n",
    "    print(c_train_c1.key.nunique())\n",
    "    train_comb_pivot=c_train_c1.pivot(index='key', columns='date', values='units')\n",
    "    train_comb_pivot=train_comb_pivot.mean(axis=0) #assign zero sales\n",
    "    train_comb_pivot=pd.DataFrame(train_comb_pivot,columns=['mean_sales'])\n",
    "    train_cluster1=series_to_supervised(train_comb_pivot, n_in=20, n_out=1, dropnan=True)\n",
    "    cluster1=c_dataframe[c_dataframe['clust_7']==clusterno]\n",
    "    cluster1_n=cluster1.drop('clust_7',axis=1).set_index('key').transpose()\n",
    "    cluster1.mean=cluster1_n.mean(axis=1)\n",
    "    cluster1_frame=pd.DataFrame(cluster1.mean, columns=['mean_sales'])\n",
    "    cluster1_frame=cluster1_frame.transpose()\n",
    "    model.fit(train_cluster1[['var1(t-20)', 'var1(t-19)', 'var1(t-18)', 'var1(t-17)',\n",
    "       'var1(t-16)', 'var1(t-15)', 'var1(t-14)', 'var1(t-13)',\n",
    "       'var1(t-12)', 'var1(t-11)', 'var1(t-10)', 'var1(t-9)', 'var1(t-8)',\n",
    "       'var1(t-7)', 'var1(t-6)', 'var1(t-5)', 'var1(t-4)', 'var1(t-3)',\n",
    "       'var1(t-2)', 'var1(t-1)']],train_cluster1[['var1(t)']])\n",
    "    \n",
    "    days=range(1,32)\n",
    "    n1=len(days)\n",
    "    k=0\n",
    "    day_n=0\n",
    "    while(k<n1):\n",
    "        day=days[day_n]\n",
    "        predicted_date=datetime.date(year=2018,day=day,month=1)\n",
    "        pred_date_before=predicted_date- timedelta(days=20)\n",
    "        delta = predicted_date - pred_date_before\n",
    "        bet_day=[]\n",
    "        for i in range(delta.days):\n",
    "            add=str(pred_date_before + timedelta(days=i))\n",
    "            bet_day.append(add)\n",
    "        pred_date_before=str(pred_date_before)\n",
    "        predicted_date=str(predicted_date)\n",
    "        value_predict=cluster1_frame[bet_day]\n",
    "        y_pred=model.predict(value_predict)\n",
    "        cluster1_frame[predicted_date]=y_pred\n",
    "        k=k+1\n",
    "        day_n=day_n+1\n",
    "    return(cluster1_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1001, 3)\n",
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bengikoseoglu/anaconda2/lib/python2.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:1306: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "mlp = MLPRegressor()\n",
    "cluster4_frame=days_20_cluster(c_dataframe,c_train,4,mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bengikoseoglu/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/bengikoseoglu/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "cluster4_reults=cluster4_frame.transpose()[0:31].values\n",
    "cluster4_reults\n",
    "test=test_df[['key','date','units']].merge(data_cluster, on='key',how='inner')\n",
    "test_clust4=test[test['clust_7']==4]\n",
    "i=0\n",
    "n=test_clust4.key.nunique()\n",
    "days=range(1,32)\n",
    "day_all=[]\n",
    "pre_unit=[]\n",
    "while(i<n):\n",
    "    k=0\n",
    "    while(k<31):\n",
    "        day_all.append(days[k])\n",
    "        uni=cluster4_reults[k][0]\n",
    "        pre_unit.append(uni)\n",
    "        k=k+1\n",
    "    i=i+1\n",
    "test_clust4['day']=day_all\n",
    "test_clust4['pre_units']=pre_unit\n",
    "test_clust4=test_clust4.drop(['clust_7'],axis=1)\n",
    "\n",
    "pre_cluster4=test_clust4[['key','day','units','pre_units']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster 5 (XGBoostRegression w/ 2 temprature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "def XGBoostingRegressor2(data_train_X, target_train_y, data_test_X, df_test_data_X):\n",
    "    model = XGBRegressor(colsample_bytree=0.7,learning_rate=0.03,n_estimators=1000,max_depth=6,min_child_weight=7,objective='reg:linear',subsample=0.8)\n",
    "    model.fit(data_train_X, target_train_y['units'].ravel())\n",
    "    # make predictions for test data\n",
    "    prediction = model.predict(data_test_X)\n",
    "    #predictions = [round(value) for value in y_pred]\n",
    "    df_prediction=pd.DataFrame(prediction, columns=['pre_units']).round(decimals=5)\n",
    "    df_prediction_need=df_test_data_X[['key','day','units']]\n",
    "    df_prediction_need=df_prediction_need.reset_index(drop=True)\n",
    "    df_prediction2=df_prediction_need.merge(df_prediction, left_index=True, right_index=True)\n",
    "    return df_prediction2\n",
    "\n",
    "pre_cluster5=XGBoostingRegressor2(X4[1], X4[0], X4[3],X4[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster 6 (GBoost Sum Unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_g=pd.read_csv('integrated_data.csv') #uploading the dataset again\n",
    "da_g=data_g[['key','month','units']]\n",
    "oct_=da_g[da_g['month']==10]\n",
    "nov_=da_g[da_g['month']==11]\n",
    "dec_=da_g[da_g['month']==12]\n",
    "octt=oct_.groupby(['key', 'month'])['units'].sum()\n",
    "novv=nov_.groupby(['key', 'month'])['units'].sum()\n",
    "decc=dec_.groupby(['key', 'month'])['units'].sum()\n",
    "octt=(pd.DataFrame(octt)).reset_index()\n",
    "novv=(pd.DataFrame(novv)).reset_index()\n",
    "decc=(pd.DataFrame(decc)).reset_index()\n",
    "octt=octt.rename(columns={'units':'prev_month_sales'})\n",
    "octt[['month']]=11\n",
    "novv=novv.rename(columns={'units':'prev_month_sales'})\n",
    "novv[['month']]=12\n",
    "decc=decc.rename(columns={'units':'prev_month_sales'})\n",
    "decc[['month']]=1\n",
    "nov_data=data_g[data_g.month ==11] \n",
    "dec_data=data_g[data_g.month ==12]\n",
    "jan_data=data_g[data_g.month ==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "november=nov_data.merge(octt[['key','prev_month_sales']],on=['key'],how='left')\n",
    "december=dec_data.merge(novv[['key','prev_month_sales']],on=['key'],how='left')\n",
    "data1=december.append(november)\n",
    "df_test_data_g=jan_data.merge(decc[['key','prev_month_sales']],on=['key'],how='left')\n",
    "\n",
    "cluster_time=pd.read_csv('clusters_key_7')\n",
    "data_gg=pd.merge(data1,cluster_time[['key','clust_7']],on='key',how='left')\n",
    "data_gg=data_gg.fillna(8) # assign cluster 1\n",
    "\n",
    "df_test_data_gg=pd.merge(df_test_data_g,cluster_time[['key','clust_7']],on='key',how='left')\n",
    "df_test_data_gg=df_test_data_gg.fillna(8) # assign cluster 1\n",
    "\n",
    "df_train_data_gg=data_gg.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train_data0=df_train_data_gg[df_train_data_gg.clust_7 ==1] \n",
    "df_train_data1=df_train_data_gg[df_train_data_gg.clust_7 ==2] \n",
    "df_train_data2=df_train_data_gg[df_train_data_gg.clust_7 ==3] \n",
    "df_train_data3=df_train_data_gg[df_train_data_gg.clust_7 ==4] \n",
    "df_train_data4=df_train_data_gg[df_train_data_gg.clust_7 ==5] \n",
    "df_train_data5=df_train_data_gg[df_train_data_gg.clust_7 ==6] \n",
    "df_train_data6=df_train_data_gg[df_train_data_gg.clust_7 ==7] \n",
    "df_train_data7=df_train_data_gg[df_train_data_gg.clust_7 ==8 ] \n",
    "\n",
    "#7 test clusters\n",
    "df_test_data0=df_test_data_gg[df_test_data_gg.clust_7 ==1] \n",
    "df_test_data1=df_test_data_gg[df_test_data_gg.clust_7 ==2]\n",
    "df_test_data2=df_test_data_gg[df_test_data_gg.clust_7 ==3] \n",
    "df_test_data3=df_test_data_gg[df_test_data_gg.clust_7 ==4]\n",
    "df_test_data4=df_test_data_gg[df_test_data_gg.clust_7 ==5]\n",
    "df_test_data5=df_test_data_gg[df_test_data_gg.clust_7 ==6]\n",
    "df_test_data6=df_test_data_gg[df_test_data_gg.clust_7 ==7]\n",
    "df_test_data7=df_test_data_gg[df_test_data_gg.clust_7 ==8] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_train0 = df_train_data0[['units']]\n",
    "data_train0 = df_train_data0.drop(['Unnamed: 0','units','key','date','rrp','weekday','month','day','clust_7','sum_unit'], axis=1)\n",
    "target_test0 = df_test_data0[['units']]\n",
    "data_test0 =df_test_data0.drop(['Unnamed: 0','units','key','date','rrp','weekday','month','day','clust_7','sum_unit'], axis=1)\n",
    "\n",
    "target_train1 = df_train_data1[['units']]\n",
    "data_train1 = df_train_data1.drop(['Unnamed: 0','units','key','date','rrp','weekday','month','day','clust_7','sum_unit'], axis=1)\n",
    "target_test1 = df_test_data1[['units']]\n",
    "data_test1 =df_test_data1.drop(['Unnamed: 0','units','key','date','rrp','weekday','month','day','clust_7','sum_unit'], axis=1)\n",
    "\n",
    "target_train2 = df_train_data2[['units']]\n",
    "data_train2 = df_train_data2.drop(['Unnamed: 0','units','key','date','rrp','weekday','month','day','clust_7','sum_unit'], axis=1)\n",
    "target_test2 = df_test_data2[['units']]\n",
    "data_test2 =df_test_data2.drop(['Unnamed: 0','units','key','date','rrp','weekday','month','day','clust_7','sum_unit'], axis=1)\n",
    "\n",
    "target_train3 = df_train_data3[['units']]\n",
    "data_train3 = df_train_data3.drop(['Unnamed: 0','units','key','date','rrp','weekday','month','day','clust_7','sum_unit'], axis=1)\n",
    "target_test3 = df_test_data3[['units']]\n",
    "data_test3 =df_test_data3.drop(['Unnamed: 0','units','key','date','rrp','weekday','month','day','clust_7','sum_unit'], axis=1)\n",
    "\n",
    "target_train4 = df_train_data4[['units']]\n",
    "data_train4 = df_train_data4.drop(['Unnamed: 0','units','key','date','rrp','weekday','month','day','clust_7','sum_unit'], axis=1)\n",
    "target_test4 = df_test_data4[['units']]\n",
    "data_test4 =df_test_data4.drop(['Unnamed: 0','units','key','date','rrp','weekday','month','day','clust_7','sum_unit'], axis=1)\n",
    "\n",
    "target_train5 = df_train_data5[['units']]\n",
    "data_train5 = df_train_data5.drop(['Unnamed: 0','units','key','date','rrp','weekday','month','day','clust_7','sum_unit'], axis=1)\n",
    "target_test5 = df_test_data5[['units']]\n",
    "data_test5 =df_test_data5.drop(['Unnamed: 0','units','key','date','rrp','weekday','month','day','clust_7','sum_unit'], axis=1)\n",
    "\n",
    "target_train6 = df_train_data6[['units']]\n",
    "data_train6 = df_train_data6.drop(['Unnamed: 0','units','key','date','rrp','weekday','month','day','clust_7','sum_unit'], axis=1)\n",
    "target_test6 = df_test_data6[['units']]\n",
    "data_test6=df_test_data6.drop(['Unnamed: 0','units','key','date','rrp','weekday','month','day','clust_7','sum_unit'], axis=1)\n",
    "\n",
    "target_train6 = df_train_data6[['units']]\n",
    "data_train6 = df_train_data6.drop(['Unnamed: 0','units','key','date','rrp','weekday','month','day','clust_7','sum_unit'], axis=1)\n",
    "target_test6 = df_test_data6[['units']]\n",
    "data_test6=df_test_data6.drop(['Unnamed: 0','units','key','date','rrp','weekday','month','day','clust_7','sum_unit'], axis=1)\n",
    "\n",
    "target_train7 = df_train_data7[['units']]\n",
    "data_train7 = df_train_data7.drop(['Unnamed: 0','units','key','date','rrp','weekday','month','day','clust_7','sum_unit'], axis=1)\n",
    "target_test7 = df_test_data7[['units']]\n",
    "data_test7=df_test_data7.drop(['Unnamed: 0','units','key','date','rrp','weekday','month','day','clust_7','sum_unit'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params5 = {'n_estimators': 1000, 'max_depth': 3, 'min_samples_split': 2,\n",
    "              'learning_rate': 0.05, 'loss': 'ls'}\n",
    "clf5 = ensemble.GradientBoostingRegressor(**params5)\n",
    "clf5.fit(data_train5, target_train5['units'].ravel()) \n",
    "prediction5= clf5.predict(data_test5)\n",
    "    \n",
    "df_prediction5=pd.DataFrame(prediction5, columns=['pre_units']).round(decimals=5)\n",
    "df_prediction_need5=df_test_data5[['key','day','units']]\n",
    "df_prediction_need5=df_prediction_need5.reset_index(drop=True)\n",
    "pre_cluster6=df_prediction_need5.merge(df_prediction5, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>day</th>\n",
       "      <th>units</th>\n",
       "      <th>pre_units</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15122 4 ( 43-45 )</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.67247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15122 4 ( 43-45 )</td>\n",
       "      <td>2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.56780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15122 4 ( 43-45 )</td>\n",
       "      <td>3</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1.74597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15122 4 ( 43-45 )</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.16683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15122 4 ( 43-45 )</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.85763</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 key  day  units  pre_units\n",
       "0  15122 4 ( 43-45 )    1    5.0    2.67247\n",
       "1  15122 4 ( 43-45 )    2    7.0    3.56780\n",
       "2  15122 4 ( 43-45 )    3   31.0    1.74597\n",
       "3  15122 4 ( 43-45 )    4    0.0    5.16683\n",
       "4  15122 4 ( 43-45 )    5    2.0    2.85763"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_cluster6.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster 7 (Windowing Individually Lag(30)-  Linear Reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "windowing_lag30_all=pd.read_csv('windowing_lag30_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "windowing_lag30_all=windowing_lag30_all.merge(data_cluster,on='key',how='inner')\n",
    "clust_7_frame=windowing_lag30_all[windowing_lag30_all['clust_7']==7] #takte the 7 clusters ones\n",
    "clust_7_frame=clust_7_frame.drop('Unnamed: 0',axis=1)\n",
    "clust_7_frame=clust_7_frame.set_index('key').transpose()[0:31]\n",
    "\n",
    "days=range(1,32)\n",
    "a=clust_7_frame[['16818 L']]\n",
    "a=a.rename(columns={'16818 L':'pre_units'}).reset_index()\n",
    "a['day']=days\n",
    "a['key']='16818 L'\n",
    "b=clust_7_frame[['12985 L']]\n",
    "b=b.rename(columns={'12985 L':'pre_units'}).reset_index()\n",
    "b['day']=days\n",
    "b['key']='12985 L'\n",
    "c=clust_7_frame[['15845 L']]\n",
    "c=c.rename(columns={'15845 L':'pre_units'}).reset_index()\n",
    "c['day']=days\n",
    "c['key']='15845 L'\n",
    "d=clust_7_frame[['15845 M']]\n",
    "d=d.rename(columns={'15845 M':'pre_units'}).reset_index()\n",
    "d['day']=days\n",
    "d['key']='15845 M'\n",
    "\n",
    "all_=pd.concat([a,b,c,d])\n",
    "all_=all_.rename(columns={'index':'date'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster7_results=all_.merge(test_df[['key','date','units']],on=['key','date'],how='inner')\n",
    "cluster7_results=cluster7_results.drop(['date'],axis=1)\n",
    "\n",
    "i=0\n",
    "n=cluster7_results.key.nunique()\n",
    "days=range(1,32)\n",
    "day_all=[]\n",
    "while(i<n):\n",
    "    k=0\n",
    "    while(k<31):\n",
    "        day_all.append(days[k])\n",
    "        k=k+1\n",
    "    i=i+1\n",
    "    \n",
    "cluster7_results['day']=day_all\n",
    "pre_cluster7=cluster7_results[['key','day','units','pre_units']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster 8 (GBoostRegression w/ 2 temprature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "\n",
    "params7 = {'n_estimators': 1000, 'max_depth': 3, 'min_samples_split': 2,\n",
    "              'learning_rate': 0.05, 'loss': 'ls'}\n",
    "clf7 = ensemble.GradientBoostingRegressor(**params7)\n",
    "clf7.fit(X7[1], X7[0]['units'].ravel()) \n",
    "prediction7= clf7.predict(X7[3])\n",
    "    \n",
    "df_prediction7=pd.DataFrame(prediction7, columns=['pre_units']).round(decimals=5)\n",
    "df_prediction_need7=X7[4][['key','day','units']]\n",
    "df_prediction_need7=df_prediction_need7.reset_index(drop=True)\n",
    "\n",
    "pre_cluster8=df_prediction_need7.merge(df_prediction7, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Integrate Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pre_total = pd.concat([pre_cluster1,pre_cluster2,pre_cluster3,pre_cluster4,pre_cluster5,pre_cluster6,pre_cluster7,pre_cluster8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8858"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_cluster1.key.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1720"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_cluster2.key.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "284"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_cluster3.key.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_cluster4.key.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_cluster5.key.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_cluster6.key.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_cluster7.key.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1814"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_cluster8.key.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>day</th>\n",
       "      <th>units</th>\n",
       "      <th>pre_units</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19671 39 1/3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.082668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19671 39 1/3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.079272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19671 39 1/3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19671 39 1/3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.073751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19671 39 1/3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.071518</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            key  day  units  pre_units\n",
       "0  19671 39 1/3    1    0.0   0.082668\n",
       "1  19671 39 1/3    2    0.0   0.079272\n",
       "2  19671 39 1/3    3    0.0   0.076319\n",
       "3  19671 39 1/3    4    0.0   0.073751\n",
       "4  19671 39 1/3    5    0.0   0.071518"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(397544, 4)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12824"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_total.key.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pre_total.to_csv('CombineModel_predict.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def predict_date(pre_total):\n",
    "    date=[]\n",
    "    i=0\n",
    "    n=len(pre_total)\n",
    "    while i<n:\n",
    "        day_int=pre_total.iloc[i]['day']\n",
    "        date_time=datetime.datetime(2018, 1, day_int, 0,0).date()\n",
    "        date_string=str(date_time)\n",
    "        date.append(date_string)\n",
    "        i+=1\n",
    "    pre_total['date']=date\n",
    "\n",
    "    pre_total_pvt = pre_total.pivot_table('pre_units', ['key'], 'date')\n",
    "    pre_total_pvt['key'] = pre_total_pvt.index\n",
    "    pre_total_pvt=pre_total_pvt.reset_index(drop=True)\n",
    "\n",
    "    return pre_total_pvt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>day</th>\n",
       "      <th>units</th>\n",
       "      <th>pre_units</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19671 39 1/3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.082668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19671 39 1/3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.079272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19671 39 1/3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19671 39 1/3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.073751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19671 39 1/3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.071518</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            key  day  units  pre_units\n",
       "0  19671 39 1/3    1    0.0   0.082668\n",
       "1  19671 39 1/3    2    0.0   0.079272\n",
       "2  19671 39 1/3    3    0.0   0.076319\n",
       "3  19671 39 1/3    4    0.0   0.073751\n",
       "4  19671 39 1/3    5    0.0   0.071518"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_cluster1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bengikoseoglu/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "pre_total_pvt    = predict_date(pre_total)\n",
    "pre_cluster0_pvt = predict_date(pre_cluster1)\n",
    "pre_cluster1_pvt = predict_date(pre_cluster2)\n",
    "pre_cluster2_pvt = predict_date(pre_cluster3)\n",
    "pre_cluster3_pvt = predict_date(pre_cluster4)\n",
    "pre_cluster4_pvt = predict_date(pre_cluster5)\n",
    "pre_cluster5_pvt = predict_date(pre_cluster6)\n",
    "pre_cluster6_pvt = predict_date(pre_cluster7)\n",
    "pre_cluster7_pvt = predict_date(pre_cluster8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>day</th>\n",
       "      <th>units</th>\n",
       "      <th>pre_units</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19671 39 1/3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.082668</td>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19671 39 1/3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.079272</td>\n",
       "      <td>2018-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19671 39 1/3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076319</td>\n",
       "      <td>2018-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19671 39 1/3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.073751</td>\n",
       "      <td>2018-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19671 39 1/3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.071518</td>\n",
       "      <td>2018-01-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            key  day  units  pre_units        date\n",
       "0  19671 39 1/3    1    0.0   0.082668  2018-01-01\n",
       "1  19671 39 1/3    2    0.0   0.079272  2018-01-02\n",
       "2  19671 39 1/3    3    0.0   0.076319  2018-01-03\n",
       "3  19671 39 1/3    4    0.0   0.073751  2018-01-04\n",
       "4  19671 39 1/3    5    0.0   0.071518  2018-01-05"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_cluster1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# measure sold_out_day deviation with actual\n",
    "from datetime import date, timedelta\n",
    "import math\n",
    "\n",
    "def measurement(prediction, test_data):\n",
    "\n",
    "    d1 = date(2018, 1, 1)  \n",
    "    d2 = date(2018, 1, 31)  \n",
    "    delta = d2 - d1         \n",
    "    pre_day=[]\n",
    "    for i in range(delta.days + 1):\n",
    "        add=str(d1 + timedelta(days=i))\n",
    "        pre_day.append(add)\n",
    "\n",
    "    sold_out_day=[]\n",
    "    key=[]\n",
    "    #lets innet join with testdataset\n",
    "    window_inner=test_data.merge(prediction,left_on='key', right_on='key', how='inner')\n",
    "    window_inner['sol_out_pred']=0\n",
    "\n",
    "    all_=window_inner.key.unique()\n",
    "    n=len(all_) \n",
    "    i=0\n",
    "    #print (n)\n",
    "\n",
    "    while(i<n): #dataset\n",
    "        subset_w=prediction.loc[prediction['key']==all_[i]]\n",
    "        stock=test_data.loc[test_data['key']==all_[i]]['stock'].values[0]\n",
    "        n1=len(pre_day)\n",
    "        k=0\n",
    "        while(k<n1): #day calculation\n",
    "            day=pre_day[k]\n",
    "            that_day_sale=subset_w[day].values[0]\n",
    "            stock=stock-that_day_sale\n",
    "            if((stock==0) | (stock<0) ):\n",
    "                window_inner.iloc[i,37]=day\n",
    "                break\n",
    "            if(k==(n1-1)): \n",
    "                window_inner.iloc[i,37]='2018-01-22'\n",
    "            k=k+1\n",
    "        i=i+1\n",
    "\n",
    "    test_data['key']=test_data[\"pid\"].astype(str) +\" \"+ test_data[\"size\"].astype(str)\n",
    "    window_inner[\"date_sold_out_date\"] = pd.to_datetime(window_inner[\"sold_out_date\"],format=\"%Y/%m/%d\")\n",
    "    window_inner[\"date_sold_out_pred\"] = pd.to_datetime(window_inner[\"sol_out_pred\"],format=\"%Y/%m/%d\")\n",
    "\n",
    "    days_differ=window_inner[\"date_sold_out_date\"]-window_inner[\"date_sold_out_pred\"]\n",
    "    days=pd.DataFrame(days_differ,columns=['days_differ'])\n",
    "    days['correct_differ']=abs(days['days_differ'].astype(datetime.timedelta).map(lambda x: np.nan if pd.isnull(x) else x.days))\n",
    "\n",
    "    error=math.sqrt(sum(days['correct_differ']))\n",
    "\n",
    "    return error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error(test_table,total,cluster0,cluster1,cluster2,cluster3,cluster4,cluster5,cluster6,cluster7):\n",
    "    \n",
    "    test_data_0=pd.read_csv(test_table)\n",
    "    test_data_0['pid']=test_data_0['pid'].astype(int)\n",
    "    test_data_0['key']=test_data_0[\"pid\"].astype(str) +\" \"+ test_data_0[\"size\"].astype(str)\n",
    "\n",
    "    error_0=measurement(cluster0, test_data_0)\n",
    "    error_1=measurement(cluster1, test_data_0)\n",
    "    error_2=measurement(cluster2, test_data_0)\n",
    "    error_3=measurement(cluster3, test_data_0)\n",
    "    error_4=measurement(cluster4, test_data_0)\n",
    "    error_5=measurement(cluster5, test_data_0)\n",
    "    error_6=measurement(cluster6, test_data_0)\n",
    "    error_7=measurement(cluster7, test_data_0)\n",
    "    error_total=measurement(total, test_data_0)\n",
    "\n",
    "    print('cluster0',error_0,'\\ncluster1',error_1, '\\ncluster2',error_2,'\\ncluster3',error_3, '\\ncluster4',error_4,\n",
    "          '\\ncluster5',error_5, '\\ncluster6',error_6,'\\ncluster7',error_7,'\\ntotoal',error_total)\n",
    "    \n",
    "    return error_0,error_1,error_2,error_3,error_4,error_5,error_6,error_7,error_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('cluster0', 191.4654015742792, '\\ncluster1', 104.48444860360799, '\\ncluster2', 40.50925820105819, '\\ncluster3', 7.211102550927978, '\\ncluster4', 22.24859546128699, '\\ncluster5', 12.767145334803704, '\\ncluster6', 2.23606797749979, '\\ncluster7', 101.59724405711013, '\\ntotoal', 245.46690204587665)\n"
     ]
    }
   ],
   "source": [
    "error_0=error('test_0.csv',pre_total_pvt,pre_cluster0_pvt,pre_cluster1_pvt,pre_cluster2_pvt,pre_cluster3_pvt,pre_cluster4_pvt,pre_cluster5_pvt,pre_cluster6_pvt,pre_cluster7_pvt,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('cluster0', 192.3018460649819, '\\ncluster1', 105.09519494249011, '\\ncluster2', 40.47221268969612, '\\ncluster3', 6.928203230275509, '\\ncluster4', 20.518284528683193, '\\ncluster5', 12.806248474865697, '\\ncluster6', 3.3166247903554, '\\ncluster7', 101.48398888494677, '\\ntotoal', 246.18285886714372)\n"
     ]
    }
   ],
   "source": [
    "error_1=error('test_1.csv',pre_total_pvt,pre_cluster0_pvt,pre_cluster1_pvt,pre_cluster2_pvt,pre_cluster3_pvt,pre_cluster4_pvt,pre_cluster5_pvt,pre_cluster6_pvt,pre_cluster7_pvt,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('cluster0', 192.13536894595956, '\\ncluster1', 105.08092119885512, '\\ncluster2', 41.42463035441596, '\\ncluster3', 7.14142842854285, '\\ncluster4', 21.166010488516726, '\\ncluster5', 12.489995996796797, '\\ncluster6', 3.605551275463989, '\\ncluster7', 101.35580891098448, '\\ntotoal', 246.201137284132)\n"
     ]
    }
   ],
   "source": [
    "error_2=error('test_2.csv',pre_total_pvt,pre_cluster0_pvt,pre_cluster1_pvt,pre_cluster2_pvt,pre_cluster3_pvt,pre_cluster4_pvt,pre_cluster5_pvt,pre_cluster6_pvt,pre_cluster7_pvt,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('cluster0', 191.57505056765612, '\\ncluster1', 105.29007550571896, '\\ncluster2', 40.90232267243512, '\\ncluster3', 7.3484692283495345, '\\ncluster4', 20.074859899884732, '\\ncluster5', 12.328828005937952, '\\ncluster6', 2.449489742783178, '\\ncluster7', 101.06433594498111, '\\ntotoal', 245.5381844031596)\n"
     ]
    }
   ],
   "source": [
    "error_3=error('test_3.csv',pre_total_pvt,pre_cluster0_pvt,pre_cluster1_pvt,pre_cluster2_pvt,pre_cluster3_pvt,pre_cluster4_pvt,pre_cluster5_pvt,pre_cluster6_pvt,pre_cluster7_pvt,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('cluster0', 192.17179813906097, '\\ncluster1', 105.42770034483347, '\\ncluster2', 40.50925820105819, '\\ncluster3', 7.280109889280518, '\\ncluster4', 21.633307652783937, '\\ncluster5', 12.609520212918492, '\\ncluster6', 3.7416573867739413, '\\ncluster7', 100.93066927351666, '\\ntotoal', 246.1036367061649)\n"
     ]
    }
   ],
   "source": [
    "error_4=error('test_4.csv',pre_total_pvt,pre_cluster0_pvt,pre_cluster1_pvt,pre_cluster2_pvt,pre_cluster3_pvt,pre_cluster4_pvt,pre_cluster5_pvt,pre_cluster6_pvt,pre_cluster7_pvt,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('cluster0', 191.92989305838756, '\\ncluster1', 105.07566811910115, '\\ncluster2', 40.763536423732717, '\\ncluster3', 7.1818626654752773, '\\ncluster4', 21.128211606231115, '\\ncluster5', 12.600347605064528, '\\ncluster6', 3.0698782345752598, '\\ncluster7', 101.28640941430783, '\\ntotoal', 245.89854386129537)\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "c0=[error_0[0],error_1[0],error_2[0],error_3[0],error_4[0]]\n",
    "c1=[error_0[1],error_1[1],error_2[1],error_3[1],error_4[1]]\n",
    "c2=[error_0[2],error_1[2],error_2[2],error_3[2],error_4[2]]\n",
    "c3=[error_0[3],error_1[3],error_2[3],error_3[3],error_4[3]]\n",
    "c4=[error_0[4],error_1[4],error_2[4],error_3[4],error_4[4]]\n",
    "c5=[error_0[5],error_1[5],error_2[5],error_3[5],error_4[5]]\n",
    "c6=[error_0[6],error_1[6],error_2[6],error_3[6],error_4[6]]\n",
    "c7=[error_0[7],error_1[7],error_2[7],error_3[7],error_4[7]]\n",
    "total=[error_0[8],error_1[8],error_2[8],error_3[8],error_4[8]]\n",
    "\n",
    "print('cluster0',numpy.mean(c0),'\\ncluster1',numpy.mean(c1), '\\ncluster2',numpy.mean(c2),'\\ncluster3',numpy.mean(c3), '\\ncluster4',numpy.mean(c4),\n",
    "           '\\ncluster5',numpy.mean(c5), '\\ncluster6',numpy.mean(c6),'\\ncluster7',numpy.mean(c7),'\\ntotoal',numpy.mean(total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
