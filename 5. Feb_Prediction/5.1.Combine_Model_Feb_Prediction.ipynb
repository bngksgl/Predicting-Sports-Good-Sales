{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According our analysis, we found out that Time Series Clustering Results with the best model.\n",
    "\n",
    "\n",
    "Cluster 1: Arima Time Clustering\n",
    "\n",
    "Cluster 2: GBoostRegression- sum unit\n",
    "\n",
    "Cluster 3: XGBoostRegression w/ 2 temprature\n",
    "\n",
    "Cluster 4: Windowing Time Series Clustering (Lag=20)- ANN\n",
    "\n",
    "Cluster 5: XGBoostRegression w/ 2 temprature\n",
    "\n",
    "Cluster 6: GBoostRegression- sum unit\n",
    "\n",
    "Cluster 7: Windowing Individually Lag(30)- Linear Reg\n",
    "\n",
    "Cluster 8: GBoostRegression w/ 2 temprature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from datetime import date, timedelta\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv('integrated_data_Feb.csv')\n",
    "cluster_time=pd.read_csv('time_cluster_bg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_cluster=cluster_time[['key','clust_7']]\n",
    "data=pd.merge(data,data_cluster,on='key',how='left')\n",
    "data=data.fillna(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df=data[data.month !=2] \n",
    "test_df=data[data.month ==2] \n",
    "test_df=test_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cluster_2temp(train,test,label):\n",
    "    df_train_data_x=train[train.clust_7 ==label]\n",
    "    df_test_data_x=test[test.clust_7 ==label] \n",
    "    \n",
    "    target_train_x = df_train_data_x[['units']]\n",
    "    data_train_x = df_train_data_x.drop(['Unnamed: 0','units','key','date','rrp','weekday','month','day','clust_7','sum_unit'], axis=1)\n",
    "    target_test_x = df_test_data_x[['units']]\n",
    "    data_test_x =df_test_data_x.drop(['Unnamed: 0','units','key','date','rrp','weekday','month','day','clust_7','sum_unit'], axis=1)\n",
    "    \n",
    "    return target_train_x,data_train_x,target_test_x,data_test_x,df_test_data_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X0=cluster_2temp(train_df,test_df,1)\n",
    "X1=cluster_2temp(train_df,test_df,2)\n",
    "X2=cluster_2temp(train_df,test_df,3)\n",
    "X3=cluster_2temp(train_df,test_df,4)\n",
    "X4=cluster_2temp(train_df,test_df,5)\n",
    "X5=cluster_2temp(train_df,test_df,6)\n",
    "X6=cluster_2temp(train_df,test_df,7)\n",
    "X7=cluster_2temp(train_df,test_df,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cluster_1temp(train,test,label):\n",
    "    df_train_data_x=train[train.clust_7 ==label]\n",
    "    df_test_data_x=test[test.clust_7 ==label] \n",
    "    \n",
    "    target_train_x = df_train_data_x[['units']]\n",
    "    data_train_x = df_train_data_x.drop(['Unnamed: 0','units','key','date','rrp','weekday','month','day','clust_7','sum_unit','median_temp'], axis=1)\n",
    "    target_test_x = df_test_data_x[['units']]\n",
    "    data_test_x =df_test_data_x.drop(['Unnamed: 0','units','key','date','rrp','weekday','month','day','clust_7','sum_unit','median_temp'], axis=1)\n",
    "    \n",
    "    return target_train_x,data_train_x,target_test_x,data_test_x,df_test_data_x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X1=cluster_1temp(train_df,test_df,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster 1 (Arima Time Clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clust0=pd.read_excel('ARIMA_cluster_1_pre_units_Feb.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_clust0=X0[4][['key','day','units']]\n",
    "test_clust0=test_clust0.reset_index(drop=True)\n",
    "clust0_full=clust0.append([clust0]*(test_clust0.key.nunique()-1), ignore_index=True)\n",
    "pre_cluster0=pd.merge(test_clust0,clust0_full, left_index=True, right_index=True)\n",
    "\n",
    "pre_cluster1=pre_cluster0.rename(index=str, columns={'forecast of units': \"pre_units\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster 2 (GBoostRegression sum unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "da_g=data[['key','month','units']]\n",
    "\n",
    "oct_=da_g[da_g['month']==10]\n",
    "nov_=da_g[da_g['month']==11]\n",
    "dec_=da_g[da_g['month']==12]\n",
    "Jan_=da_g[da_g['month']==1]\n",
    "\n",
    "octt=oct_.groupby(['key', 'month'])['units'].sum()\n",
    "novv=nov_.groupby(['key', 'month'])['units'].sum()\n",
    "decc=dec_.groupby(['key', 'month'])['units'].sum()\n",
    "jann=Jan_.groupby(['key', 'month'])['units'].sum()\n",
    "\n",
    "octt=(pd.DataFrame(octt)).reset_index()\n",
    "novv=(pd.DataFrame(novv)).reset_index()\n",
    "decc=(pd.DataFrame(decc)).reset_index()\n",
    "jann=(pd.DataFrame(decc)).reset_index()\n",
    "\n",
    "octt=octt.rename(columns={'units':'prev_month_sales'})\n",
    "octt[['month']]=11\n",
    "novv=novv.rename(columns={'units':'prev_month_sales'})\n",
    "novv[['month']]=12\n",
    "decc=decc.rename(columns={'units':'prev_month_sales'})\n",
    "decc[['month']]=1\n",
    "jann=jann.rename(columns={'units':'prev_month_sales'})\n",
    "jann[['month']]=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nov_data=data[data.month ==11] \n",
    "dec_data=data[data.month ==12]\n",
    "jan_data=data[data.month ==1]\n",
    "feb_data=data[data.month ==2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# trainning data\n",
    "november=nov_data.merge(octt[['key','prev_month_sales']],on=['key'],how='left')\n",
    "december=dec_data.merge(novv[['key','prev_month_sales']],on=['key'],how='left')\n",
    "january=jan_data.merge(decc[['key','prev_month_sales']],on=['key'],how='left')\n",
    "\n",
    "df_train_data_g=pd.concat([november,december,january])\n",
    "#df_train_data_gg=pd.merge(df_train_data_g,cluster_time[['key','clust_7']],on='key',how='left')\n",
    "#df_train_data_gg=df_train_data_gg.fillna(8) \n",
    "\n",
    "# testing data\n",
    "df_test_data_g=feb_data.merge(jann[['key','prev_month_sales']],on=['key'],how='left')\n",
    "#df_test_data_gg=pd.merge(df_test_data_g,cluster_time[['key','clust_7']],on='key',how='left')  \n",
    "#df_test_data_gg=df_test_data_gg.fillna(8) \n",
    "\n",
    "# pick out cluster 2\n",
    "X1_g=cluster_2temp(df_train_data_g,df_test_data_g,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "params2 = {'n_estimators': 1000, 'max_depth': 3, 'min_samples_split': 2,\n",
    "              'learning_rate': 0.05, 'loss': 'ls'}\n",
    "clf2 = ensemble.GradientBoostingRegressor(**params2)\n",
    "clf2.fit(X1_g[1], X1_g[0]['units'].ravel()) \n",
    "prediction2= clf2.predict(X1_g[3])\n",
    "    \n",
    "df_prediction2=pd.DataFrame(prediction2, columns=['pre_units']).round(decimals=5)\n",
    "df_prediction_need2=X1_g[4][['key','day','units']]\n",
    "df_prediction_need2=df_prediction_need2.reset_index(drop=True)\n",
    "pre_cluster2=df_prediction_need2.merge(df_prediction2, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster 3 (XGBoostRegression w/ 2 temprature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GONGNA\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "def XGBoostingRegressor2(data_train_X, target_train_y, data_test_X, df_test_data_X):\n",
    "    model = XGBRegressor(colsample_bytree=0.7,learning_rate=0.03,n_estimators=1000,max_depth=6,min_child_weight=7,objective='reg:linear',subsample=0.8)\n",
    "    model.fit(data_train_X, target_train_y['units'].ravel())\n",
    "    # make predictions for test data\n",
    "    prediction = model.predict(data_test_X)\n",
    "    #predictions = [round(value) for value in y_pred]\n",
    "    df_prediction=pd.DataFrame(prediction, columns=['pre_units']).round(decimals=5)\n",
    "    df_prediction_need=df_test_data_X[['key','day','units']]\n",
    "    df_prediction_need=df_prediction_need.reset_index(drop=True)\n",
    "    df_prediction2=df_prediction_need.merge(df_prediction, left_index=True, right_index=True)\n",
    "    return df_prediction2\n",
    "\n",
    "pre_cluster3=XGBoostingRegressor2(X2[1], X2[0], X2[3],X2[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster 4 (Windowing Time Seies Clustering (Lag:20) ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    \"\"\"\n",
    "    Frame a time series as a supervised learning dataset.\n",
    "    Arguments:\n",
    "        data: Sequence of observations as a list or NumPy array.\n",
    "        n_in: Number of lag observations as input (X).\n",
    "        n_out: Number of observations as output (y).\n",
    "        dropnan: Boolean whether or not to drop rows with NaN values.\n",
    "        Returns:\n",
    "        Pandas DataFrame of series framed for supervised learning.\n",
    "    \"\"\"\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d1 = date(2017, 10, 1)  # start date\n",
    "d2 = date(2018, 1, 31)  # end date\n",
    "d3=date(2018, 2, 28)\n",
    "\n",
    "delta = d2 - d1         # timedelta\n",
    "\n",
    "pre_day=[]\n",
    "for i in range(delta.days + 1):\n",
    "    add=str(d1 + timedelta(days=i))\n",
    "    pre_day.append(add)\n",
    "\n",
    "post_day=[] \n",
    "delta_2 = d3 - d2\n",
    "\n",
    "for i in range(delta_2.days + 1):\n",
    "    add=str(d2 + timedelta(days=i))\n",
    "    post_day.append(add)\n",
    "train=train=train_df[['date','key','units']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_days_pre = pd.DataFrame({'date':pre_day})\n",
    "df_days_post = pd.DataFrame({'date':post_day})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col=post_day[1:]\n",
    "col.append('key')\n",
    "all_=train.key.unique()\n",
    "empty_matrix=np.zeros((len(all_),len(col)))\n",
    "data_frame=pd.DataFrame(data=empty_matrix, columns=col)\n",
    "data_frame['key']=all_\n",
    "days=range(1,32)\n",
    "n1=len(days)\n",
    "empty_matrix_2=np.zeros((all_.shape[0],1))\n",
    "data_frame_2=pd.DataFrame(data=empty_matrix_2, columns=['key'])\n",
    "data_frame_2['key']=all_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales_prev=train.loc[train['date']=='2018-01-31'].sort_values('date')[['key','units']]\n",
    "sales_prev1=train.loc[train['date']=='2018-01-30'].sort_values('date')[['key','units']]\n",
    "sales_prev2=train.loc[train['date']=='2018-01-29'].sort_values('date')[['key','units']]\n",
    "sales_prev3=train.loc[train['date']=='2018-01-28'].sort_values('date')[['key','units']]\n",
    "sales_prev4=train.loc[train['date']=='2018-01-27'].sort_values('date')[['key','units']]\n",
    "sales_prev5=train.loc[train['date']=='2018-01-26'].sort_values('date')[['key','units']]\n",
    "sales_prev6=train.loc[train['date']=='2018-01-25'].sort_values('date')[['key','units']]\n",
    "sales_prev7=train.loc[train['date']=='2018-01-24'].sort_values('date')[['key','units']]\n",
    "sales_prev8=train.loc[train['date']=='2018-01-23'].sort_values('date')[['key','units']] \n",
    "sales_prev9=train.loc[train['date']=='2018-01-22'].sort_values('date')[['key','units']]\n",
    "sales_prev10=train.loc[train['date']=='2018-01-21'].sort_values('date')[['key','units']]\n",
    "sales_prev11=train.loc[train['date']=='2018-01-20'].sort_values('date')[['key','units']]\n",
    "sales_prev12=train.loc[train['date']=='2018-01-19'].sort_values('date')[['key','units']]\n",
    "sales_prev13=train.loc[train['date']=='2018-01-18'].sort_values('date')[['key','units']]\n",
    "sales_prev14=train.loc[train['date']=='2018-01-17'].sort_values('date')[['key','units']]\n",
    "sales_prev15=train.loc[train['date']=='2018-01-16'].sort_values('date')[['key','units']]\n",
    "sales_prev16=train.loc[train['date']=='2018-01-15'].sort_values('date')[['key','units']]\n",
    "sales_prev17=train.loc[train['date']=='2018-01-14'].sort_values('date')[['key','units']]\n",
    "sales_prev18=train.loc[train['date']=='2018-01-13'].sort_values('date')[['key','units']]\n",
    "sales_prev19=train.loc[train['date']=='2018-01-12'].sort_values('date')[['key','units']]\n",
    "sales_prev20=train.loc[train['date']=='2018-01-11'].sort_values('date')[['key','units']]\n",
    "sales_prev21=train.loc[train['date']=='2018-01-10'].sort_values('date')[['key','units']]\n",
    "sales_prev22=train.loc[train['date']=='2018-01-09'].sort_values('date')[['key','units']]\n",
    "data_frame_2=data_frame_2.merge(sales_prev, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2018-01-31'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev1, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2018-01-30'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev2, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2018-01-29'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev3, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2018-01-28'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev4, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2018-01-27'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev5, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2018-01-26'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev6, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2018-01-25'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev7, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2018-01-24'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev8, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2018-01-23'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev9, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2018-01-22'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev10, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2018-01-21'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev11, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2018-01-20'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev12, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2018-01-19'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev13, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2018-01-18'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev14, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2018-01-17'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev15, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2018-01-16'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev16, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2018-01-15'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev17, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2018-01-14'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev18, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2018-01-13'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev19, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2018-01-12'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev20, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2018-01-11'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev21, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2018-01-10'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev22, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2018-01-09'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_frame_2=data_frame_2.fillna(0)\n",
    "data_frame=pd.merge(data_frame,data_frame_2,on='key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c_train=train.merge(data_cluster,on='key',how='inner') #get cluster for train\n",
    "c_dataframe=data_frame.merge(data_cluster,on='key',how='inner') #get clusers for test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def days_20_cluster(c_dataframe,c_train,clusterno,model):\n",
    "    c_train_c1=c_train[c_train['clust_7']==clusterno].drop('clust_7',axis=1)\n",
    "    print(c_train_c1.shape)\n",
    "    print(c_train_c1.key.nunique())\n",
    "    train_comb_pivot=c_train_c1.pivot(index='key', columns='date', values='units')\n",
    "    train_comb_pivot=train_comb_pivot.mean(axis=0) #assign zero sales\n",
    "    train_comb_pivot=pd.DataFrame(train_comb_pivot,columns=['mean_sales'])\n",
    "    train_cluster1=series_to_supervised(train_comb_pivot, n_in=20, n_out=1, dropnan=True)\n",
    "    cluster1=c_dataframe[c_dataframe['clust_7']==clusterno]\n",
    "    cluster1_n=cluster1.drop('clust_7',axis=1).set_index('key').transpose()\n",
    "    cluster1.mean=cluster1_n.mean(axis=1)\n",
    "    cluster1_frame=pd.DataFrame(cluster1.mean, columns=['mean_sales'])\n",
    "    cluster1_frame=cluster1_frame.transpose()\n",
    "    model.fit(train_cluster1[['var1(t-20)', 'var1(t-19)', 'var1(t-18)', 'var1(t-17)',\n",
    "       'var1(t-16)', 'var1(t-15)', 'var1(t-14)', 'var1(t-13)',\n",
    "       'var1(t-12)', 'var1(t-11)', 'var1(t-10)', 'var1(t-9)', 'var1(t-8)',\n",
    "       'var1(t-7)', 'var1(t-6)', 'var1(t-5)', 'var1(t-4)', 'var1(t-3)',\n",
    "       'var1(t-2)', 'var1(t-1)']],train_cluster1[['var1(t)']])\n",
    "    \n",
    "    days=range(1,29)\n",
    "    n1=len(days)\n",
    "    k=0\n",
    "    day_n=0\n",
    "    while(k<n1):\n",
    "        day=days[day_n]\n",
    "        predicted_date=datetime.date(year=2018,day=day,month=2)\n",
    "        pred_date_before=predicted_date- timedelta(days=20)\n",
    "        delta = predicted_date - pred_date_before\n",
    "        bet_day=[]\n",
    "        for i in range(delta.days):\n",
    "            add=str(pred_date_before + timedelta(days=i))\n",
    "            bet_day.append(add)\n",
    "        pred_date_before=str(pred_date_before)\n",
    "        predicted_date=str(predicted_date)\n",
    "        value_predict=cluster1_frame[bet_day]\n",
    "        y_pred=model.predict(value_predict)\n",
    "        cluster1_frame[predicted_date]=y_pred\n",
    "        k=k+1\n",
    "        day_n=day_n+1\n",
    "    return(cluster1_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1342, 3)\n",
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GONGNA\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1306: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "mlp = MLPRegressor()\n",
    "cluster4_frame=days_20_cluster(c_dataframe,c_train,4,mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GONGNA\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\GONGNA\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "cluster4_reults=cluster4_frame.transpose()[0:28].values\n",
    "cluster4_reults\n",
    "test=test_df[['key','date','units']].merge(data_cluster, on='key',how='inner')\n",
    "test_clust4=test[test['clust_7']==4]\n",
    "i=0\n",
    "n=test_clust4.key.nunique()\n",
    "days=range(1,29)\n",
    "day_all=[]\n",
    "pre_unit=[]\n",
    "while(i<n):\n",
    "    k=0\n",
    "    while(k<28):\n",
    "        day_all.append(days[k])\n",
    "        uni=cluster4_reults[k][0]\n",
    "        pre_unit.append(uni)\n",
    "        k=k+1\n",
    "    i=i+1\n",
    "test_clust4['day']=day_all\n",
    "test_clust4['pre_units']=pre_unit\n",
    "test_clust4=test_clust4.drop(['clust_7'],axis=1)\n",
    "\n",
    "pre_cluster4=test_clust4[['key','day','units','pre_units']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster 5 (XGBoostRegression w/ 2 temprature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "def XGBoostingRegressor2(data_train_X, target_train_y, data_test_X, df_test_data_X):\n",
    "    model = XGBRegressor(colsample_bytree=0.7,learning_rate=0.03,n_estimators=1000,max_depth=6,min_child_weight=7,objective='reg:linear',subsample=0.8)\n",
    "    model.fit(data_train_X, target_train_y['units'].ravel())\n",
    "    # make predictions for test data\n",
    "    prediction = model.predict(data_test_X)\n",
    "    #predictions = [round(value) for value in y_pred]\n",
    "    df_prediction=pd.DataFrame(prediction, columns=['pre_units']).round(decimals=5)\n",
    "    df_prediction_need=df_test_data_X[['key','day','units']]\n",
    "    df_prediction_need=df_prediction_need.reset_index(drop=True)\n",
    "    df_prediction2=df_prediction_need.merge(df_prediction, left_index=True, right_index=True)\n",
    "    return df_prediction2\n",
    "\n",
    "pre_cluster5=XGBoostingRegressor2(X4[1], X4[0], X4[3],X4[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster 6 (GBoost Sum Unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X5_g=cluster_2temp(df_train_data_g,df_test_data_g,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params5 = {'n_estimators': 1000, 'max_depth': 3, 'min_samples_split': 2,\n",
    "              'learning_rate': 0.05, 'loss': 'ls'}\n",
    "clf5 = ensemble.GradientBoostingRegressor(**params5)\n",
    "clf5.fit(X5_g[1], X5_g[0]['units'].ravel()) \n",
    "prediction5= clf5.predict(X5_g[3])\n",
    "    \n",
    "df_prediction5=pd.DataFrame(prediction5, columns=['pre_units']).round(decimals=5)\n",
    "df_prediction_need5=X5_g[4][['key','day','units']]\n",
    "df_prediction_need5=df_prediction_need5.reset_index(drop=True)\n",
    "pre_cluster6=df_prediction_need5.merge(df_prediction5, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster 7 (Windowing Individually Lag(30)-  Linear Reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allcomb=pd.concat([test_df,train_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allcomb=allcomb[allcomb['clust_7']==7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "year=2018\n",
    "month=1\n",
    "days=range(1, 29)\n",
    "lag=2\n",
    "col=post_day[1:] #column names for the df\n",
    "col.append('key')\n",
    "all_=allcomb.key.unique()\n",
    "empty_matrix=np.zeros((all_.shape[0],len(col))) #column according to the \n",
    "data_frame=pd.DataFrame(data=empty_matrix, columns=col)\n",
    "data_frame['key']=all_\n",
    "empty_matrix_2=np.zeros((all_.shape[0],1))\n",
    "data_frame_2=pd.DataFrame(data=empty_matrix_2, columns=['key'])\n",
    "data_frame_2['key']=all_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales_prev=allcomb.loc[allcomb['date']=='2018-01-31'].sort_values('date')[['key','units']]\n",
    "sales_prev1=allcomb.loc[allcomb['date']=='2018-01-30'].sort_values('date')[['key','units']]\n",
    "sales_prev2=allcomb.loc[allcomb['date']=='2018-01-29'].sort_values('date')[['key','units']]\n",
    "sales_prev3=allcomb.loc[allcomb['date']=='2018-01-28'].sort_values('date')[['key','units']]\n",
    "sales_prev4=allcomb.loc[allcomb['date']=='2018-01-27'].sort_values('date')[['key','units']]\n",
    "sales_prev5=allcomb.loc[allcomb['date']=='2018-01-26'].sort_values('date')[['key','units']]\n",
    "sales_prev6=allcomb.loc[allcomb['date']=='2018-01-25'].sort_values('date')[['key','units']]\n",
    "sales_prev7=allcomb.loc[allcomb['date']=='2018-01-24'].sort_values('date')[['key','units']]\n",
    "sales_prev8=allcomb.loc[allcomb['date']=='2018-01-23'].sort_values('date')[['key','units']] \n",
    "sales_prev9=allcomb.loc[allcomb['date']=='2018-01-22'].sort_values('date')[['key','units']]\n",
    "sales_prev10=allcomb.loc[allcomb['date']=='2018-01-21'].sort_values('date')[['key','units']]\n",
    "sales_prev11=allcomb.loc[allcomb['date']=='2018-01-20'].sort_values('date')[['key','units']]\n",
    "sales_prev12=allcomb.loc[allcomb['date']=='2018-01-19'].sort_values('date')[['key','units']]\n",
    "sales_prev13=allcomb.loc[allcomb['date']=='2018-01-18'].sort_values('date')[['key','units']]\n",
    "sales_prev14=allcomb.loc[allcomb['date']=='2018-01-17'].sort_values('date')[['key','units']]\n",
    "sales_prev15=allcomb.loc[allcomb['date']=='2018-01-16'].sort_values('date')[['key','units']]\n",
    "sales_prev16=allcomb.loc[allcomb['date']=='2018-01-15'].sort_values('date')[['key','units']]\n",
    "sales_prev17=allcomb.loc[allcomb['date']=='2018-01-14'].sort_values('date')[['key','units']]\n",
    "sales_prev18=allcomb.loc[allcomb['date']=='2018-01-13'].sort_values('date')[['key','units']]\n",
    "sales_prev19=allcomb.loc[allcomb['date']=='2018-01-12'].sort_values('date')[['key','units']]\n",
    "sales_prev20=allcomb.loc[allcomb['date']=='2018-01-11'].sort_values('date')[['key','units']]\n",
    "sales_prev21=allcomb.loc[allcomb['date']=='2018-01-10'].sort_values('date')[['key','units']]\n",
    "sales_prev22=allcomb.loc[allcomb['date']=='2018-01-09'].sort_values('date')[['key','units']]\n",
    "sales_prev23=allcomb.loc[allcomb['date']=='2018-01-08'].sort_values('date')[['key','units']]\n",
    "sales_prev24=allcomb.loc[allcomb['date']=='2018-01-07'].sort_values('date')[['key','units']]\n",
    "sales_prev25=allcomb.loc[allcomb['date']=='2018-01-06'].sort_values('date')[['key','units']]\n",
    "sales_prev26=allcomb.loc[allcomb['date']=='2018-01-05'].sort_values('date')[['key','units']]\n",
    "sales_prev27=allcomb.loc[allcomb['date']=='2018-01-04'].sort_values('date')[['key','units']]\n",
    "sales_prev28=allcomb.loc[allcomb['date']=='2018-01-03'].sort_values('date')[['key','units']]\n",
    "sales_prev29=allcomb.loc[allcomb['date']=='2018-01-02'].sort_values('date')[['key','units']]\n",
    "sales_prev30=allcomb.loc[allcomb['date']=='2018-01-01'].sort_values('date')[['key','units']]\n",
    "data_frame_2=data_frame_2.merge(sales_prev, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2018-01-31'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev1, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2018-01-30'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev2, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2018-01-29'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev3, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2018-01-28'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev4, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2018-01-27'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev5, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2018-01-26'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev6, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2018-01-25'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev7, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2018-01-24'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev8, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2018-01-23'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev9, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2018-01-22'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev10, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2018-01-21'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev11, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2018-01-20'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev12, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2018-01-19'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev13, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2018-01-18'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev14, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2018-01-17'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev15, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2018-01-16'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev16, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2018-01-15'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev17, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2018-01-14'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev18, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2018-01-13'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev19, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2018-01-12'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev20, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2018-01-11'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev21, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2018-01-10'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev22, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2018-01-09'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev23, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2018-01-08'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev24, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2018-01-07'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev25, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2018-01-06'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev26, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2018-01-05'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev27, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2018-01-04'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev28, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2018-01-03'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev29, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2018-01-02'})\n",
    "data_frame_2=data_frame_2.merge(sales_prev30, left_on='key', right_on='key', how='left')\n",
    "data_frame_2=data_frame_2.rename(columns={'units':'2018-01-01'})\n",
    "data_frame_2=data_frame_2.fillna(0)\n",
    "data_frame=pd.merge(data_frame,data_frame_2,on='key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df_c7=train_df[train_df['clust_7']==7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GONGNA\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:194: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "n=train_df_c7.key.nunique()\n",
    "all_=train_df_c7.key.unique()\n",
    "p=0\n",
    "while(p<n): #looping over datasets\n",
    "    #print(p)\n",
    "    subset_tr=train_df_c7.loc[train_df_c7['key']==all_[p]].sort_values('date')\n",
    "    df_model=series_to_supervised(subset_tr[['units']], n_in=30, n_out=1, dropnan=False)\n",
    "    df_model=df_model.dropna(axis=0)\n",
    "    days=range(1,29)\n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit( df_model[['var1(t-30)', 'var1(t-29)', 'var1(t-28)', 'var1(t-27)',\n",
    "           'var1(t-26)', 'var1(t-25)', 'var1(t-24)', 'var1(t-23)',\n",
    "           'var1(t-22)', 'var1(t-21)', 'var1(t-20)', 'var1(t-19)',\n",
    "           'var1(t-18)', 'var1(t-17)', 'var1(t-16)', 'var1(t-15)',\n",
    "           'var1(t-14)', 'var1(t-13)', 'var1(t-12)', 'var1(t-11)',\n",
    "           'var1(t-10)', 'var1(t-9)', 'var1(t-8)', 'var1(t-7)', 'var1(t-6)',\n",
    "           'var1(t-5)', 'var1(t-4)', 'var1(t-3)', 'var1(t-2)', 'var1(t-1)']],df_model[['var1(t)']])\n",
    "    k=0\n",
    "    day_n=0\n",
    "    while(k<28): #looping over days\n",
    "        day=days[day_n]\n",
    "        predicted_date=datetime.date(year=2018,day=day,month=2)#this is the day i am trying to predict\n",
    "        pred_date_before=predicted_date- timedelta(days=30)\n",
    "        delta = predicted_date - pred_date_before\n",
    "        bet_day=[]\n",
    "        for i in range(delta.days):\n",
    "            add=str(pred_date_before + timedelta(days=i))\n",
    "            bet_day.append(add)\n",
    "        pred_date_before=str(pred_date_before)\n",
    "        predicted_date=str(predicted_date)\n",
    "        value_predict=data_frame[bet_day].values[p].reshape(1, -1)\n",
    "        y_pred = regr.predict(value_predict)\n",
    "        data_frame[predicted_date].loc[p]=y_pred\n",
    "        k=k+1\n",
    "        day_n=day_n+1\n",
    "    p=p+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clust_7_frame=data_frame.set_index('key').transpose()[0:28]\n",
    "\n",
    "#days=range(1,32)\n",
    "a=clust_7_frame[['16818 L']]\n",
    "a=a.rename(columns={'16818 L':'pre_units'}).reset_index()\n",
    "a['day']=days\n",
    "a['key']='16818 L'\n",
    "b=clust_7_frame[['12985 L']]\n",
    "b=b.rename(columns={'12985 L':'pre_units'}).reset_index()\n",
    "b['day']=days\n",
    "b['key']='12985 L'\n",
    "c=clust_7_frame[['15845 L']]\n",
    "c=c.rename(columns={'15845 L':'pre_units'}).reset_index()\n",
    "c['day']=days\n",
    "c['key']='15845 L'\n",
    "d=clust_7_frame[['15845 M']]\n",
    "d=d.rename(columns={'15845 M':'pre_units'}).reset_index()\n",
    "d['day']=days\n",
    "d['key']='15845 M'\n",
    "\n",
    "all_=pd.concat([a,b,c,d])\n",
    "all_=all_.rename(columns={'index':'date'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster7_results=all_.merge(test_df[['key','date','units']],on=['key','date'],how='inner')\n",
    "cluster7_results=cluster7_results.drop(['date'],axis=1)\n",
    "\n",
    "i=0\n",
    "n=cluster7_results.key.nunique()\n",
    "days=range(1,29)\n",
    "day_all=[]\n",
    "while(i<n):\n",
    "    k=0\n",
    "    while(k<28):\n",
    "        day_all.append(days[k])\n",
    "        k=k+1\n",
    "    i=i+1\n",
    "    \n",
    "cluster7_results['day']=day_all\n",
    "pre_cluster7=cluster7_results[['key','day','units','pre_units']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster 8 (GBoostRegression w/ 2 temprature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "\n",
    "params7 = {'n_estimators': 1000, 'max_depth': 3, 'min_samples_split': 2,\n",
    "              'learning_rate': 0.05, 'loss': 'ls'}\n",
    "clf7 = ensemble.GradientBoostingRegressor(**params7)\n",
    "clf7.fit(X7[1], X7[0]['units'].ravel()) \n",
    "prediction7= clf7.predict(X7[3])\n",
    "    \n",
    "df_prediction7=pd.DataFrame(prediction7, columns=['pre_units']).round(decimals=5)\n",
    "df_prediction_need7=X7[4][['key','day','units']]\n",
    "df_prediction_need7=df_prediction_need7.reset_index(drop=True)\n",
    "\n",
    "pre_cluster8=df_prediction_need7.merge(df_prediction7, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Integrate Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pre_total = pd.concat([pre_cluster1,pre_cluster2,pre_cluster3,pre_cluster4,pre_cluster5,pre_cluster6,pre_cluster7,pre_cluster8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def predict_date(pre_total):\n",
    "    date=[]\n",
    "    i=0\n",
    "    n=len(pre_total)\n",
    "    while i<n:\n",
    "        day_int=pre_total.iloc[i]['day']\n",
    "        date_time=datetime.datetime(2018, 2, day_int, 0,0).date()\n",
    "        date_string=str(date_time)\n",
    "        date.append(date_string)\n",
    "        i+=1\n",
    "    pre_total['date']=date\n",
    "\n",
    "    pre_total_pvt = pre_total.pivot_table('pre_units', ['key'], 'date')\n",
    "    pre_total_pvt['key'] = pre_total_pvt.index\n",
    "    pre_total_pvt=pre_total_pvt.reset_index(drop=True)\n",
    "\n",
    "    return pre_total_pvt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pre_total_pvt    = predict_date(pre_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict sold_out_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GONGNA\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "items=pd.read_csv('items.csv',delimiter='|')\n",
    "item_stock=items[['pid','size','stock']]\n",
    "item_stock['key']=item_stock[\"pid\"].astype(str) +\" \"+ item_stock[\"size\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import date, timedelta\n",
    "\n",
    "d1 = date(2018, 2, 1)  \n",
    "d2 = date(2018, 2, 28)  \n",
    "delta = d2 - d1         \n",
    "pre_day=[]\n",
    "for i in range(delta.days + 1):\n",
    "    add=str(d1 + timedelta(days=i))\n",
    "    pre_day.append(add)\n",
    "\n",
    "sold_out_day=[]\n",
    "key=[]\n",
    "   \n",
    "window_inner=item_stock.merge(pre_total_pvt,left_on='key', right_on='key', how='inner')\n",
    "window_inner['soldOutDate']=0\n",
    "\n",
    "all_=window_inner.key.unique()\n",
    "n=len(all_) \n",
    "i=0\n",
    "\n",
    "while(i<n): #dataset\n",
    "    subset_w=pre_total_pvt.loc[pre_total_pvt['key']==all_[i]]\n",
    "    stock=item_stock.loc[item_stock['key']==all_[i]]['stock'].values[0]\n",
    "    n1=len(pre_day)\n",
    "    k=0\n",
    "    while(k<n1): #day calculation\n",
    "        day=pre_day[k]\n",
    "        that_day_sale=subset_w[day].values[0]\n",
    "        stock=stock-that_day_sale\n",
    "        if((stock==0) | (stock<0) ):\n",
    "            window_inner.iloc[i,32]=day\n",
    "            break\n",
    "        if(k==(n1-1)): \n",
    "            window_inner.iloc[i,32]='2018-02-22'\n",
    "        k=k+1\n",
    "    i=i+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "window_inner['soldOutDate']=pd.to_datetime(window_inner['soldOutDate'])\n",
    "SoldOutDay_predict=window_inner[['pid','size','soldOutDate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>size</th>\n",
       "      <th>soldOutDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000</td>\n",
       "      <td>XL ( 158-170 )</td>\n",
       "      <td>2018-02-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001</td>\n",
       "      <td>L</td>\n",
       "      <td>2018-02-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10003</td>\n",
       "      <td>3 (35-38 )</td>\n",
       "      <td>2018-02-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10003</td>\n",
       "      <td>4 ( 39-42 )</td>\n",
       "      <td>2018-02-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10003</td>\n",
       "      <td>5 ( 43-46 )</td>\n",
       "      <td>2018-02-18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     pid            size soldOutDate\n",
       "0  10000  XL ( 158-170 )  2018-02-18\n",
       "1  10001               L  2018-02-18\n",
       "2  10003      3 (35-38 )  2018-02-11\n",
       "3  10003     4 ( 39-42 )  2018-02-18\n",
       "4  10003     5 ( 43-46 )  2018-02-18"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SoldOutDay_predict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pid                     int64\n",
       "size                   object\n",
       "soldOutDate    datetime64[ns]\n",
       "dtype: object"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SoldOutDay_predict.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SoldOutDay_predict.to_csv('SoldOutDay_predict.csv',sep='|',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
